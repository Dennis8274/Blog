<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[QMQ源码分析之delay-server篇【三】]]></title>
    <url>%2F4c5184d3.html</url>
    <content type="text"><![CDATA[前言 上篇我们分析了QMQ delay-server关于存储的部分，这一篇我们会对投递的源码进行分析。 投递投递的相关内容在WheelTickManager这个类。提前加载schedule_log、wheel根据延迟时间到时进行投递等相关工作都在这里完成。而关于真正进行投递的相关类是在sender这个包里。 wheelwheel包里一共就三个类文件，HashWheelTimer、WheelLoadCursor、WheelTickManager，WheelTickManager就应该是wheel加载文件，wheel中的消息到时投递的管理器；WheelLoadCursor应该就是上一篇中提到的schedule_log文件加载到哪里的cursor标识；那么HashWheelTimer就是一个辅助工具类，简单理解成Java中的ScheduledExecutorService，可理解成是根据延迟消息的延迟时间进行投递的timer，所以这里不对这个工具类做更多解读，我们更关心MQ逻辑。首先来看提前一定时间加载schedule_log，这里的提前一定时间是多长时间呢？这个是根据需要配置的，比如3schedule_log的刻度自定义配置为1h，提前加载时间配置为30min，那么在2019-02-10 17:30就应该加载2019021018这个schedule_log。 1234567891011121314@Override public void start() &#123; if (!isStarted()) &#123; sender.init(); // hash wheel timer,内存中的wheel timer.start(); started.set(true); // 根据dispatch log,从上次投递结束的地方恢复开始投递 recover(); // 加载线程，用于加载schedule_log loadScheduler.scheduleWithFixedDelay(this::load, 0, config.getLoadSegmentDelayMinutes(), TimeUnit.MINUTES); LOGGER.info("wheel started."); &#125; &#125; recover这个方法，会根据dispatch log中的投递记录，找到上一次最后投递的位置，在delay-server重启的时候，wheel会根据这个位置恢复投递。 1234567891011121314151617181920212223242526272829303132333435private void recover() &#123; LOGGER.info("wheel recover..."); // 最新的dispatch log segment DispatchLogSegment currentDispatchedSegment = facade.latestDispatchSegment(); if (currentDispatchedSegment == null) &#123; LOGGER.warn("load latest dispatch segment null"); return; &#125; int latestOffset = currentDispatchedSegment.getSegmentBaseOffset(); DispatchLogSegment lastSegment = facade.lowerDispatchSegment(latestOffset); if (null != lastSegment) doRecover(lastSegment); // 根据最新的dispatch log segment进行恢复投递 doRecover(currentDispatchedSegment); LOGGER.info("wheel recover done. currentOffset:&#123;&#125;", latestOffset); &#125;private void doRecover(DispatchLogSegment dispatchLogSegment) &#123; int segmentBaseOffset = dispatchLogSegment.getSegmentBaseOffset(); ScheduleSetSegment setSegment = facade.loadScheduleLogSegment(segmentBaseOffset); if (setSegment == null) &#123; LOGGER.error("load schedule index error,dispatch segment:&#123;&#125;", segmentBaseOffset); return; &#125; // 得到一个关于已投递记录的set LongHashSet dispatchedSet = loadDispatchLog(dispatchLogSegment); // 根据这个set，将最新的dispatch log segment中未投递的消息add in wheel。 WheelLoadCursor.Cursor loadCursor = facade.loadUnDispatch(setSegment, dispatchedSet, this::refresh); int baseOffset = loadCursor.getBaseOffset(); // 记录cursor loadingCursor.shiftCursor(baseOffset, loadCursor.getOffset()); loadedCursor.shiftCursor(baseOffset); &#125; 恢复基本就是以上的这些内容，接下来看看是如何加载的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344 private void load() &#123; // 提前一定时间加载到下一 delay segment long next = System.currentTimeMillis() + config.getLoadInAdvanceTimesInMillis(); int prepareLoadBaseOffset = resolveSegment(next); try &#123; // 加载到prepareLoadBaseOffset这个delay segment loadUntil(prepareLoadBaseOffset); &#125; catch (InterruptedException ignored) &#123; LOGGER.debug("load segment interrupted"); &#125; &#125;private void loadUntil(int until) throws InterruptedException &#123; // 当前wheel已加载到baseOffset int loadedBaseOffset = loadedCursor.baseOffset(); // 如已加载到until，则break // have loaded if (loadedBaseOffset &gt; until) return; do &#123; // 加载失败，则break // wait next turn when loaded error. if (!loadUntilInternal(until)) break; // 当前并没有until这个delay segment，即loading cursor小于until // load successfully(no error happened) and current wheel loading cursor &lt; until if (loadingCursor.baseOffset() &lt; until) &#123; // 阻塞，直到thresholdTime+blockingExitTime // 即如果提前blockingExitTime还未有until这个delay segment的消息进来，则退出 long thresholdTime = System.currentTimeMillis() + config.getLoadBlockingExitTimesInMillis(); // exit in a few minutes in advance if (resolveSegment(thresholdTime) &gt;= until) &#123; loadingCursor.shiftCursor(until); loadedCursor.shiftCursor(until); break; &#125; &#125; // 避免cpu load过高 Thread.sleep(100); &#125; while (loadedCursor.baseOffset() &lt; until); LOGGER.info("wheel load until &#123;&#125; &lt;= &#123;&#125;", loadedCursor.baseOffset(), until); &#125; 根据配置的提前加载时间，内存中的wheel会提前加载schedule_log，加载是在一个while循环里，直到加载到until delay segment才退出，如果当前没有until 这个delay segment，那么会在配置的blockingExitTime时间退出该循环，而为了避免cpu load过高，这里会在每次循环间隔设置100ms sleep。这里加载为什么是在while循环里？以及为什么sleep 100ms，sleep 500ms 或者1s可不可以？以及为什么要设置个blockingExitTime呢？下面的分析之后，应该就能回答这些问题了。主要考虑两种情况，一种是当之前一直没有delay segment或者delay segment是间隔存在的，比如delay segment刻度为1h，2019031001和2019031004之间的2019031002及2019031003不存在这种之类的delay segment不存在的情况，另一种是当正在加载delay segment的时候，位于该segment的延迟消息正在被加载，这种情况是有可能丢消息的。所以这里加载是在一个循环里，以及设置了两个cursor，即loading cursor，和loaded cursor。一个表示正在加载，一个表示已经加载。此外，上面每次循环sleep 100ms，可不可以sleep 500ms 1s？答案是可以，只是消息是否能容忍500ms 或者1s的延迟。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475private boolean loadUntilInternal(int until) &#123; int index = resolveStartIndex(); if (index &lt; 0) return true; try &#123; while (index &lt;= until) &#123; ScheduleSetSegment segment = facade.loadScheduleLogSegment(index); if (segment == null) &#123; int nextIndex = facade.higherScheduleBaseOffset(index); if (nextIndex &lt; 0) return true; index = nextIndex; continue; &#125; // 具体加载某个segment的地方 loadSegment(segment); int nextIndex = facade.higherScheduleBaseOffset(index); if (nextIndex &lt; 0) return true; index = nextIndex; &#125; &#125; catch (Throwable e) &#123; LOGGER.error("wheel load segment failed,currentSegmentOffset:&#123;&#125; until:&#123;&#125;", loadedCursor.baseOffset(), until, e); QMon.loadSegmentFailed(); return false; &#125; return true;&#125;private void loadSegment(ScheduleSetSegment segment) &#123; final long start = System.currentTimeMillis(); try &#123; int baseOffset = segment.getSegmentBaseOffset(); long offset = segment.getWrotePosition(); if (!loadingCursor.shiftCursor(baseOffset, offset)) &#123; LOGGER.error("doLoadSegment error,shift loadingCursor failed,from &#123;&#125;-&#123;&#125; to &#123;&#125;-&#123;&#125;", loadingCursor.baseOffset(), loadingCursor.offset(), baseOffset, offset); return; &#125; WheelLoadCursor.Cursor loadedCursorEntry = loadedCursor.cursor(); // have loaded // 已经加载 if (baseOffset &lt; loadedCursorEntry.getBaseOffset()) return; long startOffset = 0; // last load action happened error // 如果上次加载失败，则从上一次的位置恢复加载 if (baseOffset == loadedCursorEntry.getBaseOffset() &amp;&amp; loadedCursorEntry.getOffset() &gt; -1) startOffset = loadedCursorEntry.getOffset(); LogVisitor&lt;ScheduleIndex&gt; visitor = segment.newVisitor(startOffset, config.getSingleMessageLimitSize()); try &#123; loadedCursor.shiftCursor(baseOffset, startOffset); long currentOffset = startOffset; // 考虑一种情况，当前delay segment正在append消息，所以是while，而loaded cursor的offset也是没加载一个消息更新的 while (currentOffset &lt; offset) &#123; Optional&lt;ScheduleIndex&gt; recordOptional = visitor.nextRecord(); if (!recordOptional.isPresent()) break; ScheduleIndex index = recordOptional.get(); currentOffset = index.getOffset() + index.getSize(); refresh(index); loadedCursor.shiftOffset(currentOffset); &#125; loadedCursor.shiftCursor(baseOffset); LOGGER.info("loaded segment:&#123;&#125; &#123;&#125;", loadedCursor.baseOffset(), currentOffset); &#125; finally &#123; visitor.close(); &#125; &#125; finally &#123; Metrics.timer("loadSegmentTimer").update(System.currentTimeMillis() - start, TimeUnit.MILLISECONDS); &#125;&#125; 还记得上一篇我们提到过，存储的时候，如果这个消息位于正在被wheel加载segment中，那么这个消息应该是会被加载到wheel中的。 12345678910111213141516171819202122232425262728293031private boolean iterateCallback(final ScheduleIndex index) &#123; long scheduleTime = index.getScheduleTime(); long offset = index.getOffset(); // 主要看一下这个canAdd if (wheelTickManager.canAdd(scheduleTime, offset)) &#123; wheelTickManager.addWHeel(index); return true; &#125; return false;&#125;// 就是cursor起作用的地方了public boolean canAdd(long scheduleTime, long offset) &#123; WheelLoadCursor.Cursor currentCursor = loadingCursor.cursor(); int currentBaseOffset = currentCursor.getBaseOffset(); long currentOffset = currentCursor.getOffset();// 根据延迟时间确定该消息位于哪个segment int baseOffset = resolveSegment(scheduleTime); // 小于当前loading cursor,则put int wheel if (baseOffset &lt; currentBaseOffset) return true;// 正在加载 if (baseOffset == currentBaseOffset) &#123; // 根据cursor的offset判断 return currentOffset &lt;= offset; &#125; return false;&#125; sendersender包里结构如下图：通过brokerGroup做分组，根据组批量发送，发送时是多线程发送，每个组互不影响，发送时也会根据实时broker的weight进行选择考虑broker进行发送。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 @Override public void send(ScheduleIndex index) &#123; if (!BrokerRoleManager.isDelayMaster()) &#123; return; &#125; boolean add; try &#123; long waitTime = Math.abs(sendWaitTime); // 入队 if (waitTime &gt; 0) &#123; add = batchExecutor.addItem(index, waitTime, TimeUnit.MILLISECONDS); &#125; else &#123; add = batchExecutor.addItem(index); &#125; &#125; catch (InterruptedException e) &#123; return; &#125; if (!add) &#123; reject(index); &#125; &#125; @Override public void process(List&lt;ScheduleIndex&gt; indexList) &#123; try &#123; // 发送处理逻辑在senderExecutor里 senderExecutor.execute(indexList, this, brokerService); &#125; catch (Exception e) &#123; LOGGER.error("send message failed,messageSize:&#123;&#125; will retry", indexList.size(), e); retry(indexList); &#125; &#125;// 以下为senderExecutor内容 void execute(final List&lt;ScheduleIndex&gt; indexList, final SenderGroup.ResultHandler handler, final BrokerService brokerService) &#123; // 分组 Map&lt;SenderGroup, List&lt;ScheduleIndex&gt;&gt; groups = groupByBroker(indexList, brokerService); for (Map.Entry&lt;SenderGroup, List&lt;ScheduleIndex&gt;&gt; entry : groups.entrySet()) &#123; doExecute(entry.getKey(), entry.getValue(), handler); &#125; &#125; private void doExecute(final SenderGroup group, final List&lt;ScheduleIndex&gt; list, final SenderGroup.ResultHandler handler) &#123; // 分组发送 group.send(list, sender, handler); &#125; 可以看到，投递时是根据server broker进行分组投递。看一下SenderGroup这个类可以看到，每个组的投递是多线程，互不影响，不会存在某个组的server挂掉，导致其他组无法投递。并且这里如果存在某个组无法投递，重试时会选择其它的server broker进行重试。与此同时，在选择组时，会根据每个server broker的weight进行综合考量，即当前server broker有多少消息量要发送。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 具体发送的地方 private void send(Sender sender, ResultHandler handler, BrokerGroupInfo groupInfo, String groupName, List&lt;ScheduleIndex&gt; list) &#123; try &#123; long start = System.currentTimeMillis(); List&lt;ScheduleSetRecord&gt; records = store.recoverLogRecord(list); QMon.loadMsgTime(System.currentTimeMillis() - start); Datagram response = sendMessages(records, sender); release(records); monitor(list, groupName); if (response == null) &#123; handler.fail(list); &#125; else &#123; final int responseCode = response.getHeader().getCode(); final Map&lt;String, SendResult&gt; resultMap = getSendResult(response); if (resultMap == null || responseCode != CommandCode.SUCCESS) &#123; if (responseCode == CommandCode.BROKER_REJECT || responseCode == CommandCode.BROKER_ERROR) &#123; // 该组熔断 groupInfo.markFailed(); &#125; monitorSendFail(list, groupInfo.getGroupName()); // 重试 handler.fail(list); return; &#125; Set&lt;String&gt; failedMessageIds = new HashSet&lt;&gt;(); boolean brokerRefreshed = false; for (Map.Entry&lt;String, SendResult&gt; entry : resultMap.entrySet()) &#123; int resultCode = entry.getValue().getCode(); if (resultCode != MessageProducerCode.SUCCESS) &#123; failedMessageIds.add(entry.getKey()); &#125; if (!brokerRefreshed &amp;&amp; resultCode == MessageProducerCode.BROKER_READ_ONLY) &#123; groupInfo.markFailed(); brokerRefreshed = true; &#125; &#125; if (!brokerRefreshed) groupInfo.markSuccess(); // dispatch log 记录在这里产生 handler.success(records, failedMessageIds); &#125; &#125; catch (Throwable e) &#123; LOGGER.error("sender group send batch failed,broker:&#123;&#125;,batch size:&#123;&#125;", groupName, list.size(), e); handler.fail(list); &#125; &#125; 就是以上这些，关于QMQ的delay-server源码分析就是这些了，如果以后有机会会分析一下QMQ的其他模块源码，谢谢。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QMQ源码分析之delay-server篇【二】]]></title>
    <url>%2F14f2781.html</url>
    <content type="text"><![CDATA[前言 本来是固定时间周六更博，但是昨天失恋了，心情不好，晚了一天。那么上一篇我们梳理了下QMQ延迟消息的主要功能，这一篇在此基础上，对照着功能分析一下源码。 整体结构要了解delay-server源码的一个整体结构，需要我们跟着源码，从初始化开始简单先过一遍。重试化的工作都在startup这个包里，而这个包只有一个ServerWrapper类。结合上一篇的内容，通过这个类就基本能看到delay的一个源码结构。delay-server基于netty，init方法完成初始化工作（端口默认为20801、心跳、wheel等），register方法是向meta-server发起请求，获取自己自己的角色为delay，并开始和meta-server的心跳。startServer方法是开始HashWheel的转动，从上次结束的位置继续message_log的回放，开启netty server。另外在做准备工作时知道QMQ是基于一主一从一备的方式，关于这个sync方法，是开启监听一个端口回应同步拉取动作，如果是从节点还要开始向主节点发起同步拉取动作。当这一切都完成了，那么online方法就执行，表示delay开始上线提供服务了。总结一下两个要点，QMQ是基于netty进行通信，并且采用一主一从一备的方式。 存储关于存储在之前我们也讨论了，delay-server接收到延迟消息，会顺序append到message_log，之后再对message_log进行回放，以生成schedule_log。所以关于存储我们需要关注两个东西，一个是message_log的存储，另一个是schedule_log的生成。 message_log其实message_log的生成很简单，就是顺序append。主要逻辑在qunar.tc.qmq.delay.receiver.Receiver这个类里，大致流程就是关于QMQ自定义协议的一个反序列化，然后再对序列化的单个消息进行存储。如图：主要逻辑在途中标红方法doInvoke中。 1234567891011private void doInvoke(ReceivedDelayMessage message) &#123; // ... try &#123; // 注：这里是进行append的地方 ReceivedResult result = facade.appendMessageLog(message); offer(message, result); &#125; catch (Throwable t) &#123; error(message, t); &#125;&#125; delay存储层相关逻辑都在facade这个类里，初始化时类似消息的校验等工作也都在这里，而message_log的相关操作都在messageLog里。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990@Override public AppendMessageRecordResult append(RawMessageExtend record) &#123; AppendMessageResult&lt;Long&gt; result; // 注：当前最新的一个segment LogSegment segment = logManager.latestSegment(); if (null == segment) &#123; segment = logManager.allocNextSegment(); &#125; if (null == segment) &#123; return new AppendMessageRecordResult(PutMessageStatus.CREATE_MAPPED_FILE_FAILED, null); &#125; // 注：真正进行append的动作是messageAppender result = segment.append(record, messageAppender); switch (result.getStatus()) &#123; case MESSAGE_SIZE_EXCEEDED: return new AppendMessageRecordResult(PutMessageStatus.MESSAGE_ILLEGAL, null); case END_OF_FILE: if (null == logManager.allocNextSegment()) &#123; return new AppendMessageRecordResult(PutMessageStatus.CREATE_MAPPED_FILE_FAILED, null); &#125; return append(record); case SUCCESS: return new AppendMessageRecordResult(PutMessageStatus.SUCCESS, result); default: return new AppendMessageRecordResult(PutMessageStatus.UNKNOWN_ERROR, result); &#125; &#125; // 看一下这个appender，也可以通过这里能看到QMQ的delay message 格式定义 private class DelayRawMessageAppender implements MessageAppender&lt;RawMessageExtend, Long&gt; &#123; private final ReentrantLock lock = new ReentrantLock(); private final ByteBuffer workingBuffer = ByteBuffer.allocate(1024); @Override public AppendMessageResult&lt;Long&gt; doAppend(long baseOffset, ByteBuffer targetBuffer, int freeSpace, RawMessageExtend message) &#123; // 这个lock这里影响不大 lock.lock(); try &#123; workingBuffer.clear(); final String messageId = message.getHeader().getMessageId(); final byte[] messageIdBytes = messageId.getBytes(StandardCharsets.UTF_8); final String subject = message.getHeader().getSubject(); final byte[] subjectBytes = subject.getBytes(StandardCharsets.UTF_8); final long startWroteOffset = baseOffset + targetBuffer.position(); final int recordSize = recordSizeWithCrc(messageIdBytes.length, subjectBytes.length, message.getBodySize()); if (recordSize &gt; config.getSingleMessageLimitSize()) &#123; return new AppendMessageResult&lt;&gt;(AppendMessageStatus.MESSAGE_SIZE_EXCEEDED, startWroteOffset, freeSpace, null); &#125; workingBuffer.flip(); if (recordSize != freeSpace &amp;&amp; recordSize + MIN_RECORD_BYTES &gt; freeSpace) &#123; // 填充 workingBuffer.limit(freeSpace); workingBuffer.putInt(MESSAGE_LOG_MAGIC_V1); workingBuffer.put(MessageLogAttrEnum.ATTR_EMPTY_RECORD.getCode()); workingBuffer.putLong(System.currentTimeMillis()); targetBuffer.put(workingBuffer.array(), 0, freeSpace); return new AppendMessageResult&lt;&gt;(AppendMessageStatus.END_OF_FILE, startWroteOffset, freeSpace, null); &#125; else &#123; int headerSize = recordSize - message.getBodySize(); workingBuffer.limit(headerSize); workingBuffer.putInt(MESSAGE_LOG_MAGIC_V2); workingBuffer.put(MessageLogAttrEnum.ATTR_MESSAGE_RECORD.getCode()); workingBuffer.putLong(System.currentTimeMillis()); // 注意这里，是schedule_time ，即延迟时间 workingBuffer.putLong(message.getScheduleTime()); // sequence,每个brokerGroup应该是唯一的 workingBuffer.putLong(sequence.incrementAndGet()); workingBuffer.putInt(messageIdBytes.length); workingBuffer.put(messageIdBytes); workingBuffer.putInt(subjectBytes.length); workingBuffer.put(subjectBytes); workingBuffer.putLong(message.getHeader().getBodyCrc()); workingBuffer.putInt(message.getBodySize()); targetBuffer.put(workingBuffer.array(), 0, headerSize); targetBuffer.put(message.getBody().nioBuffer()); final long payloadOffset = startWroteOffset + headerSize; return new AppendMessageResult&lt;&gt;(AppendMessageStatus.SUCCESS, startWroteOffset, recordSize, payloadOffset); &#125; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; 以上基本就是message_log的存储部分，接下来我们来看message_log的回放生成schedule_log。 schedule_logMessageLogReplayer这个类就是控制回放的地方。那么考虑一个问题，下一次重启的时候，我们该从哪里进行回放？QMQ是会有一个回放的offset，这个offset会定时刷盘，下次重启的时候会从这个offset位置开始回放。细节可以看一下下面这段代码块。 123456789101112131415161718192021222324final LogVisitor&lt;LogRecord&gt; visitor = facade.newMessageLogVisitor(iterateFrom.longValue());adjustOffset(visitor);while (true) &#123; final Optional&lt;LogRecord&gt; recordOptional = visitor.nextRecord(); if (recordOptional.isPresent() &amp;&amp; recordOptional.get() == DelayMessageLogVisitor.EMPTY_LOG_RECORD) &#123; break; &#125; recordOptional.ifPresent((record) -&gt; &#123; // post以进行存储 dispatcher.post(record); long checkpoint = record.getStartWroteOffset() + record.getRecordSize(); this.cursor.addAndGet(record.getRecordSize()); facade.updateIterateOffset(checkpoint); &#125;);&#125;iterateFrom.add(visitor.visitedBufferSize());try &#123; TimeUnit.MILLISECONDS.sleep(5);&#125; catch (InterruptedException e) &#123; LOGGER.warn("message log iterate sleep interrupted");&#125; 注意这里除了offset还有个cursor，这是为了防止回放失败，sleep 5ms后再次回放的时候从cursor位置开始，避免重复消息。那么我们看一下dispatcher.post这个方法: 12345678910111213@Overridepublic void post(LogRecord event) &#123; // 这里是schedule_log AppendLogResult&lt;ScheduleIndex&gt; result = facade.appendScheduleLog(event); int code = result.getCode(); if (MessageProducerCode.SUCCESS != code) &#123; LOGGER.error("appendMessageLog schedule log error,log:&#123;&#125; &#123;&#125;,code:&#123;&#125;", event.getSubject(), event.getMessageId(), code); throw new AppendException("appendScheduleLogError"); &#125;// 先看这里 iterateCallback.apply(result.getAdditional());&#125; 如以上代码，我们看略过schedule_log的存储，看一下那个callback是几个意思: 12345678910111213private boolean iterateCallback(final ScheduleIndex index) &#123; // 延迟时间 long scheduleTime = index.getScheduleTime(); // 这个offset是startOffset,即在delay_segment中的这个消息的起始位置 long offset = index.getOffset(); // 是否add到内存中的HashWheel if (wheelTickManager.canAdd(scheduleTime, offset)) &#123; wheelTickManager.addWHeel(index); return true; &#125; return false; &#125; 这里的意思是，delay-server接收到消息，会判断一下这个消息是否需要add到内存中的wheel中，以防止丢消息。大家记着有这个事情，在投递小节中我们回过头来再说这里。那么回到facade.appendScheduleLog这个方法，schedule_log相关操作在scheduleLog里： 12345678910111213141516@Override public RecordResult&lt;T&gt; append(LogRecord record) &#123; long scheduleTime = record.getScheduleTime(); // 这里是根据延迟时间定位对应的delaySegment的 DelaySegment&lt;T&gt; segment = locateSegment(scheduleTime); if (null == segment) &#123; segment = allocNewSegment(scheduleTime); &#125; if (null == segment) &#123; return new NopeRecordResult(PutMessageStatus.CREATE_MAPPED_FILE_FAILED); &#125; // 具体动作在append里 return retResult(segment.append(record, appender)); &#125; 留意locateSegment这个方法，它是根据延迟时间定位DelaySegment，比如如果延迟时间是2019-03-03 16:00:00，那么就会定位到201903031600这个DelaySegment（注：这里贴的代码不是最新的，最新的是DelaySegment的刻度是可以配置，到分钟级别）。同样，具体动作也是appender做的，如下: 123456789101112131415161718192021222324@Override public AppendRecordResult&lt;ScheduleSetSequence&gt; appendLog(LogRecord log) &#123; workingBuffer.clear(); workingBuffer.flip(); final byte[] subjectBytes = log.getSubject().getBytes(StandardCharsets.UTF_8); final byte[] messageIdBytes = log.getMessageId().getBytes(StandardCharsets.UTF_8); int recordSize = getRecordSize(log, subjectBytes.length, messageIdBytes.length); workingBuffer.limit(recordSize); long scheduleTime = log.getScheduleTime(); long sequence = log.getSequence(); workingBuffer.putLong(scheduleTime); // message_log中的sequence workingBuffer.putLong(sequence); workingBuffer.putInt(log.getPayloadSize()); workingBuffer.putInt(messageIdBytes.length); workingBuffer.put(messageIdBytes); workingBuffer.putInt(subjectBytes.length); workingBuffer.put(subjectBytes); workingBuffer.put(log.getRecord()); workingBuffer.flip(); ScheduleSetSequence record = new ScheduleSetSequence(scheduleTime, sequence); return new AppendRecordResult&lt;&gt;(AppendMessageStatus.SUCCESS, 0, recordSize, workingBuffer, record); &#125; 这里也能看到schedule_log的消息格式。 发现就写了个存储篇幅就已经挺大了，投递涉及到的内容可能更多，那么关于投递就开个下一篇吧。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QMQ源码分析之delay-server篇【一】]]></title>
    <url>%2F3159cb59.html</url>
    <content type="text"><![CDATA[前言 QMQ是一款去哪儿网内部使用多年的mq。不久前(大概1-2年前)已在携程投入生产大规模使用，年前这款mq也开源了出来。关于QMQ的相关设计文章可以看这里。在这里，我假设你已经对QMQ前世今生以及其设计和实现等背景知识已经有了一个较为全面的认识。 我的阅读姿势对一款开源产品愈来愈感兴趣，想要了解一款开源产品更多的技术细节的时候，最好的方式自然是去阅读她的源码。那么一个正确阅读开源软件源码的姿势是什么呢？我觉得这完全就像一个相亲过程： 媒婆介绍相亲对象基本信息。这一定是前提。很多人都忽视了这一个步骤。在这个步骤中，要去了解这款开源软件是用来做什么的？解决了什么问题？如何解决这些问题的？所处地位？其实就是what,why,how,where四个问题。要是在阅读源码前能准备下这四个问题的答案，那么接下来阅读源码的工作将更有效果。 见面，喝茶，对媒婆所言一探究竟。这个时候我们要去认识下软件的整体结构，例如，包结构，依赖轻重，主要功能是哪些在哪里等。此外还要去验证下”媒婆所言”是否属实，我们要自己操作运行一下，对这个”姑娘”有一个基础认识。 约会。有以上基础认识之后，就要深入源码一探究竟。针对各功能点(主要是第一个步骤中谈到的解决的什么问题即why)各条线深入下去，最后贯穿起来，形成一个闭环。 自由发挥。这个时候就看缘分了，对上眼就成了contributor，对不上眼也能多个朋友多条路不是。 主要功能对于delay-server，官方已经有了一些介绍。记住，官方通常是最卖力的那个”媒婆”。qmq-delay-server其实主要做的是转发工作。所谓转发，就是delay-server做的就是个存储和投递的工作。怎么理解，就是qmq-client会对消息进行一个路由，即实时消息投递到实时server，延迟消息往delay-server投递，多说一句，这个路由的功能是由qmq-meta-server提供。投递到delay-server的消息会存下来，到时间之后再进行投递。现在我们知道了存储和投递是delay-server主要的两个功能点。那么我们挨个击破: 存储假如让我们来设计实现一个delay-server，存储部分我们需要解决什么问题？我觉得主要是要解决到期投递的到期问题。我们可以用传统db做，但是这个性能肯定是上不去的。我们也可以用基于LSM树的RocksDB。或者，干脆直接用文件存储。QMQ是用文件存储的。而用文件存储是怎么解决到期问题的呢？delay-server接收到延迟消息，就将消息append到message_log中，然后再通过回放这个message_log得到schedule_log，此外还有一个dispatch _log用于记录投递记录。QMQ还有个跟投递相关的存储设计，即两层HashWheel。第一层位于磁盘上，例如，以一个小时一个刻度一个文件，我们叫delay_message_segment，如延迟时间为2019年02月23日 19:00 至2019年02月23日 20:00为延迟消息将被存储在2019022319。并且这个刻度是可以配置调整的。第二层HashWheel位于内存中。也是以一个时间为刻度，比如500ms，加载进内存中的延迟消息文件会根据延迟时间hash到一个HashWheel中，第二层的wheel涉及更多的是下一小节的投递。貌似存储到这里就结束了，然而还有一个问题，目前当投递的时候我们需要将一个delay_message_segment加载进内存中，而假如我们提前一个刻度加载进一个delay_message_segment到内存中的hashwheel，比如在2019年02月23日 18:00加载2019022319这个segment文件，那么一个hashwheel中就会存在两个delay_message_segment，而这个时候所占内存是非常大的，所以这是完全不可接收的。所以，QMQ引入了一个数据结构，叫schedule_index，即消息索引，存储的内容为消息的索引，我们加载到内存的是这个schedule_index，在真正投递的时候再根据索引查到消息体进行投递。 投递解决了存储，那么到期的延迟消息如何投递呢？如在上一小节存储中所提到的，内存中的hashwheel会提前一段时间加载delay_schedule_index，这个时间自然也是可以配置的。而在hashwheel中，默认每500ms会tick一次，这个500ms也是可以根据用户需求配置的。而在投递的时候，QMQ根据实时broker进行分组多线程投递，如果某一broker离线不可用，导致投递失败，delay-server会将延迟消息投递在其他存活的实时broker。其实这里对于实时的broker应该有一个关于投递消息权重的，现在delay-server没有考虑到这一点，不过我看已经有一个pr解决了这个问题，只是官方还没有时间看这个问题。除此之外，QMQ还考虑到了要是当前延迟消息所属的delay_segment已经加载到内存中的hashwheel了，这个时候消息应该是直接投递或也应加载到hashwheel中的。这里需要考虑的情况还是比较多的，比如考虑delay_segment正在加载、已经加载、加载完成等情况，对于这种情况，QMQ用了两个cursor来表示hashwheel加载到哪个delay_segment以及加载到对应segment的什么offset了，这里还是挺复杂的，这里的代码逻辑在WheelTickManager这个类。 我们先来看一看整体结构以功能划分的包结构，算是比较清晰。cleaner是日志清理工作相关，receiver是接收消息相关，sender是投递相关，store是存储相关，sync是同步备份相关，wheel则是hashwheel相关。 关于QMQ源码阅读前的准备工作就先做到这里，下一篇我们就深入源码分析以上提到的各个细节。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>MQ</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何科学上网？]]></title>
    <url>%2F8c46c59b.html</url>
    <content type="text"><![CDATA[为了给我们提供一个安全的网络环境，所以先驱前辈们建立了一堵墙。总有些调皮 好奇的孩子想要翻过墙去看看墙那边的世界。但是存在风险，需谨慎。 共有下面两种方式供选择： 利用VPN 免费免费的vpn有很多，但是速度、稳定性和流量限制是基本不能满足需要的，所以就不推荐了。 收费收费的vpn一般都在每月10元左右，并且足够稳定。另外，建议大家选择国外的vpn，国内的vpn产商说不定哪天就跑路什么的。在这里，推荐ExpressVPN和PureVPN。前者比较知名，也比较稳健，价格大概在每月$7+；后者也相对比较好用，每月大概在$3+，说是有中国用户的专线。详情可参考。 自建代理喜欢掌握主动权的我，倾向于采用自建代理的方案。综合来看自建代理都是最实惠，最可控的方案。 购买VPS目前VPS产商有两家做的最大，分别是BandwagonHost(搬瓦工)和Vultr。有篇文章对比了这两家厂商的产品。购买VPS都是有优惠的，搬瓦工 Vultr。因为搬瓦工比较老牌，老而弥坚，所以我选择的是它。如果你不喜欢老而弥坚的东西，选择了Vultr，那么请移步看搭建SSR。购买时注意是不是支持中国专线，如果没注意，那么购买成功之后也是可以更改线路的。购买完成，你会受到一封邮件，里边有ip port password等信息，连接上vps，安装完一些基础工具(wget等)，就可以开干了。 搭建代理现在用的最多的就是shadowsocks，以及其衍生版本shadowsocks r。我选择的是shadowsockr。这里有个ssr工具网站，客户端，一键安装脚本在这里都能找到。 ssh连接上vps 依次运行下边三条命令： 1wget --no-check-certificate -O shadowsocks-all.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh 1chmod +x shadowsocks-all.sh 1./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log 接下来按照提示，选择参数安装即可，步骤大概为： 选择版本，建议ShadowsocksR 设置SSR密码 选择SSR服务器端口 选择加密方式如这里选择chacha20，输入对应序号12即可。 选择协议如这里选择auth_aes128_md5，输入对应序号5即可 选择混淆方式如这里选择http_simple，输入对应序号2即可 参数设置完成，任意键开始安装，静静等待。 安装完成，你会看到以上信息，记录下来，待会儿会用到。你也可以在下图文件夹下的config.json看到。客户端安装windows 下载mac下载andriod下载 或者在应用商城看一下有没有shadowsocks-r(或者ssr)客户端下载ios 免费的App可以用Potatso Lite，不过应该需要申请一下美国AppleID安装完毕，输入安装完毕让你记下的那些信息(在/etc/shadowsocks-r/config.json，或者在刚才安装的目录下找到shadowsocks-all.log文件里也有相关信息)。另外，ssr客户端支持二维码扫描，剪贴板导入等方式，很方便，如下图：安卓上效果如下：好了，安卓和ios设备现在基本都能上网了。但是pc端还需要一个东西，即chrome的一个插件，swithy omega。下载插件添加到chrome完毕，配置如下图： 其中的规则列表网址：https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 详情可以参考gfwlist 就是这些，你可以科学上网了。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>科学上网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年终总结]]></title>
    <url>%2F59ca7e41.html</url>
    <content type="text"><![CDATA[又到年底，然而今年并不打算再写年终总结。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
</search>
