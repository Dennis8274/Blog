<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[JDK源码阅读之PriorityBlockingQueue]]></title>
    <url>%2F73b74409.html</url>
    <content type="text"><![CDATA[写在前面 An unbounded {@linkpain java.util.concurrent.BlockingQueue blocking queue} that uses the same ordering rules as class {@link PriorityQueue} and supplies blocking retrieval operations. PriorityBlockingQueue,无界优先级阻塞队列。队列中的优先级根据提供的独立的一个 Comparator 接口或者实现 Comparable 接口的队列元素决定。 概述优先级队列是基于堆(小顶堆)是实现的。线程安全性由内部声明的一个ReentrantLock保证，即所有的公共操作都是在锁下完成。 堆堆实际上是一种完全二叉树，分为大顶堆和小顶堆。大顶堆的堆顶的关键字肯定是所有关键字中最大的，小顶堆的堆顶的关键字是所有关键字中最小的。如下图： 一般堆都是由数组实现，记一个节点的索引下标为n，那么它的左右孩子为 2n+1 和 2(n+1) 。 内部变量 private static final int DEFAULT_INITIAL_CAPACITY = 11; // 默认数组大小 private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; // 数组支持最大大小 private transient Object[] queue; // 存储元素底层数组，以堆的形式 private transient int size; // 队列中元素数量 private transient Comparator&lt;? super E&gt; comparator; // 用于区分优先级的比较接口 private final ReentrantLock lock; // 保证线程安全的锁 private final Condition notEmpty; // lock.condition private transient volatile int allocationSpinLock; // 用于数组扩展时的自旋锁 构造方法 public PriorityBlockingQueue(); 12345678/** * Creates a &#123;@code PriorityBlockingQueue&#125; with the default * initial capacity (11) that orders its elements according to * their &#123;@linkplain Comparable natural ordering&#125;. */ public PriorityBlockingQueue() &#123; this(DEFAULT_INITIAL_CAPACITY, null); &#125; 队列默认大小为11** public PriorityBlockingQueue(int initialCapacity); 123456789101112/** * Creates a &#123;@code PriorityBlockingQueue&#125; with the specified * initial capacity that orders its elements according to their * &#123;@linkplain Comparable natural ordering&#125;. * * @param initialCapacity the initial capacity for this priority queue * @throws IllegalArgumentException if &#123;@code initialCapacity&#125; is less * than 1 */ public PriorityBlockingQueue(int initialCapacity) &#123; this(initialCapacity, null); &#125; 未传入comparator，则需要队列元素自身实现ComParable接口 public PriorityBlockingQueue(int initialCapacity,Comparator&lt;? super E&gt; comparator); 123456789101112131415161718192021/** * Creates a &#123;@code PriorityBlockingQueue&#125; with the specified initial * capacity that orders its elements according to the specified * comparator. * * @param initialCapacity the initial capacity for this priority queue * @param comparator the comparator that will be used to order this * priority queue. If &#123;@code null&#125;, the &#123;@linkplain Comparable * natural ordering&#125; of the elements will be used. * @throws IllegalArgumentException if &#123;@code initialCapacity&#125; is less * than 1 */ public PriorityBlockingQueue(int initialCapacity, Comparator&lt;? super E&gt; comparator) &#123; if (initialCapacity &lt; 1) throw new IllegalArgumentException(); this.lock = new ReentrantLock(); this.notEmpty = lock.newCondition(); this.comparator = comparator; this.queue = new Object[initialCapacity]; &#125; public PriorityBlockingQueue(Collection&lt;? extends E&gt; c); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Creates a &#123;@code PriorityBlockingQueue&#125; containing the elements * in the specified collection. If the specified collection is a * &#123;@link SortedSet&#125; or a &#123;@link PriorityQueue&#125;, this * priority queue will be ordered according to the same ordering. * Otherwise, this priority queue will be ordered according to the * &#123;@linkplain Comparable natural ordering&#125; of its elements. * * @param c the collection whose elements are to be placed * into this priority queue * @throws ClassCastException if elements of the specified collection * cannot be compared to one another according to the priority * queue's ordering * @throws NullPointerException if the specified collection or any * of its elements are null */ public PriorityBlockingQueue(Collection&lt;? extends E&gt; c) &#123; this.lock = new ReentrantLock(); this.notEmpty = lock.newCondition(); boolean heapify = true; // true if not known to be in heap order boolean screen = true; // true if must screen for nulls if (c instanceof SortedSet&lt;?&gt;) &#123; SortedSet&lt;? extends E&gt; ss = (SortedSet&lt;? extends E&gt;) c; this.comparator = (Comparator&lt;? super E&gt;) ss.comparator(); heapify = false; &#125; else if (c instanceof PriorityBlockingQueue&lt;?&gt;) &#123; PriorityBlockingQueue&lt;? extends E&gt; pq = (PriorityBlockingQueue&lt;? extends E&gt;) c; this.comparator = (Comparator&lt;? super E&gt;) pq.comparator(); screen = false; if (pq.getClass() == PriorityBlockingQueue.class) // exact match heapify = false; &#125; Object[] a = c.toArray(); int n = a.length; // If c.toArray incorrectly doesn't return Object[], copy it. if (a.getClass() != Object[].class) a = Arrays.copyOf(a, n, Object[].class); if (screen &amp;&amp; (n == 1 || this.comparator != null)) &#123; for (int i = 0; i &lt; n; ++i) if (a[i] == null) throw new NullPointerException(); &#125; this.queue = a; this.size = n; if (heapify) heapify(); &#125; 有必要的话，需要堆化 源码分析PriorityBlockingQueue的最主要的两个动作时，往队列中放入元素，从队列中取出元素。对应的两个核心的方法时 offer 和 poll 。此外还有在上述其中一个构造方法中，设计到一个堆化的操作，对应的方法时 heapify 。 heapify以堆的形式重新组织数组中的元素。 12345678910111213141516171819202122/** * Establishes the heap invariant (described above) in the entire tree, * assuming nothing about the order of the elements prior to the call. */ private void heapify() &#123; Object[] array = queue; int n = size; // 因为堆的特性，所以，只需要从half位置开始往前搜刮 int half = (n &gt;&gt;&gt; 1) - 1; Comparator&lt;? super E&gt; cmp = comparator; if (cmp == null) &#123; for (int i = half; i &gt;= 0; i--) // 位置i往后的孩子们不用担心了，已经拍好队了 // i向下找array[i]的位置 siftDownComparable(i, (E) array[i], array, n); &#125; else &#123; for (int i = half; i &gt;= 0; i--) // 同理 siftDownUsingComparator(i, (E) array[i], array, n, cmp); &#125; &#125; offer队列是无界的，所以理论上offer永远返回true。 1234567891011121314151617181920212223242526272829303132333435363738/** * Inserts the specified element into this priority queue. * As the queue is unbounded, this method will never return &#123;@code false&#125;. * * @param e the element to add * @return &#123;@code true&#125; (as specified by &#123;@link Queue#offer&#125;) * @throws ClassCastException if the specified element cannot be compared * with elements currently in the priority queue according to the * priority queue's ordering * @throws NullPointerException if the specified element is null */ public boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); final ReentrantLock lock = this.lock; lock.lock(); int n, cap; Object[] array; // size超过了数组大小，就需要扩容了 while ((n = size) &gt;= (cap = (array = queue).length)) tryGrow(array, cap); try &#123; Comparator&lt;? super E&gt; cmp = comparator; if (cmp == null) // 从位置n开始往上找到合适e待的位置，被e替换下的元素放在n位置上 siftUpComparable(n, e, array); else // 同理 siftUpUsingComparator(n, e, array, cmp); // 容量 +1 size = n + 1; // signal 阻塞的get线程 notEmpty.signal(); &#125; finally &#123; lock.unlock(); &#125; return true; &#125; 扩容的时候有点讲究，见下 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Tries to grow array to accommodate at least one more element * (but normally expand by about 50%), giving up (allowing retry) * on contention (which we expect to be rare). Call only while * holding lock. * * @param array the heap array * @param oldCap the length of the array */ private void tryGrow(Object[] array, int oldCap) &#123; // 释放锁，用自旋锁保证安全性 lock.unlock(); // must release and then re-acquire main lock Object[] newArray = null; if (allocationSpinLock == 0 &amp;&amp; // 扩容时，用自旋锁锁住 UNSAFE.compareAndSwapInt(this, allocationSpinLockOffset, 0, 1)) &#123; try &#123; // 如果当前容量小于64，则每次扩容2，否则往两倍扩 int newCap = oldCap + ((oldCap &lt; 64) ? (oldCap + 2) : // grow faster if small (oldCap &gt;&gt; 1)); if (newCap - MAX_ARRAY_SIZE &gt; 0) &#123; // possible overflow int minCap = oldCap + 1; if (minCap &lt; 0 || minCap &gt; MAX_ARRAY_SIZE) throw new OutOfMemoryError(); newCap = MAX_ARRAY_SIZE; &#125; if (newCap &gt; oldCap &amp;&amp; queue == array) newArray = new Object[newCap]; &#125; finally &#123; allocationSpinLock = 0; &#125; &#125; if (newArray == null) // back off if another thread is allocating Thread.yield(); // 重新获取锁 lock.lock(); if (newArray != null &amp;&amp; queue == array) &#123; queue = newArray; System.arraycopy(array, 0, newArray, 0, oldCap); &#125; &#125; poll需要在持有锁的前提下，才能从队列中获取元素。获取元素成功后，原则上堆结构是被_破坏_了，所以需要重新调整堆结构。 123456789101112131415161718192021222324252627282930313233343536public E poll() &#123; final ReentrantLock lock = this.lock; // 需要持有锁 lock.lock(); try &#123; // 出队 return dequeue(); &#125; finally &#123; lock.unlock(); &#125; &#125; /** * Mechanics for poll(). Call only while holding lock. */ private E dequeue() &#123; int n = size - 1; if (n &lt; 0) return null; else &#123; Object[] array = queue; // 堆顶元素 E result = (E) array[0]; E x = (E) array[n]; array[n] = null; Comparator&lt;? super E&gt; cmp = comparator; if (cmp == null) // 调整堆结构，即从位置0往下找x的位置 siftDownComparable(0, x, array, n); else siftDownUsingComparator(0, x, array, n, cmp); // size -= 1 size = n; return result; &#125; &#125; 调整堆上文提到的调整堆的操作有 siftDownComparable(siftDownUsingComparator) 和 siftUpComparable(siftUpUsingComparator) 。siftDownComparable 和 siftDownUsingComparator 是同一种操作，只是比较元素大小一个是利用元素实现 Comparable 接口，一个是利用构造时传入的 Comparator 。 siftDownComparable1234567891011121314151617181920212223242526272829303132333435363738/** * Inserts item x at position k, maintaining heap invariant by * demoting x down the tree repeatedly until it is less than or * equal to its children or is a leaf. * * @param k the position to fill * @param x the item to insert * @param array the heap array * @param n heap size */ private static &lt;T&gt; void siftDownComparable(int k, T x, Object[] array, int n) &#123; if (n &gt; 0) &#123; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;)x; int half = n &gt;&gt;&gt; 1; // loop while a non-leaf // k &lt; half 只需数组的前半段找 while (k &lt; half) &#123; // 左孩子 2n+1 int child = (k &lt;&lt; 1) + 1; // assume left child is least Object c = array[child]; // 右孩子 2(n+1) int right = child + 1; if (right &lt; n &amp;&amp; ((Comparable&lt;? super T&gt;) c).compareTo((T) array[right]) &gt; 0) // 找到最小的孩子 c = array[child = right]; if (key.compareTo((T) c) &lt;= 0) // 如果key比最小的孩子还小，那么表示key的位置就是这里,break break; // key应该放在最小的孩子的位置 array[k] = c; // 然后继续找替换掉孩子的位置 k = child; &#125; // 这是k应该是父 而key是比k的孩子都小 array[k] = key; &#125; &#125; siftUpComparable1234567891011121314151617181920212223242526272829303132/** * Inserts item x at position k, maintaining heap invariant by * promoting x up the tree until it is greater than or equal to * its parent, or is the root. * * To simplify and speed up coercions and comparisons. the * Comparable and Comparator versions are separated into different * methods that are otherwise identical. (Similarly for siftDown.) * These methods are static, with heap state as arguments, to * simplify use in light of possible comparator exceptions. * * @param k the position to fill * @param x the item to insert * @param array the heap array */ private static &lt;T&gt; void siftUpComparable(int k, T x, Object[] array) &#123; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;) x; while (k &gt; 0) &#123; // 父节点 int parent = (k - 1) &gt;&gt;&gt; 1; Object e = array[parent]; if (key.compareTo((T) e) &gt;= 0) // 要是比父节点大，则跳出，表示x应该放在k处 break; // 比父节点小，则顶替父节点 array[k] = e; // 接着网上找父节点的位置 k = parent; &#125; // 填充k位置 array[k] = key; &#125; 总结优先级队列内部数据结构是一个数组，这个数组是以堆的形式组织的。线程安全性由内部的一个 ReentrantLock 保证，所有对元素的公共操作是需要在持有锁的前提下才能完成。 出入队后，我们说这个堆结构是被破坏了，所以需要重新调整下这个堆结构。调整堆主要是由两个操作组成：siftUp[Comparable|Comparator] 和 siftDown[Comparable|Comparator] 。分别是从下往上找替换的元素位置，从上往下找替换元素的位置。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK源码阅读之LinkedTransferQueue]]></title>
    <url>%2Ff8a3bd88.html</url>
    <content type="text"><![CDATA[前言 An unbounded {*@link *java.util.concurrent.TransferQueue} based on linked nodes.This queue orders elements FIFO (first-in-first-out) with respect to any given producer.LinkedTransferQueue基于链表实现于TransferQueue接口，是一个遵循FIFO的队列。TransferQueue具有阻塞队列的行为(继承BlockingQueue接口)，并且producer也可以阻塞等待consumer从队列中取出该element消费。 12345678910111213141516171819202122232425262728public interface TransferQueue&lt;E&gt; extends BlockingQueue&lt;E&gt; &#123; /* * 如果有consumer正在队列阻塞等待获取数据，那么tryTransfer成功，否则失败。 * 该方法并不会往队列里put元素，而是直接交给等待着的consumer。 */ boolean tryTransfer(E e); /* * producer阻塞等待直到该element被consumer消费 */ void transfer(E e) throws InterruptedException; /* * 与第一个方法的区别为，会等待上unit.toMillis(timeout)段时间 */ boolean tryTransfer(E e, long timeout, TimeUnit unit) throws InterruptedException; /* * 队列中是否有等待的consumer */ boolean hasWaitingConsumer(); /* * 有多少个等待着的consumer */ int getWaitingConsumerCount();&#125; LinkedTransferQueue除了有BlockingQueue的行为外，还具有以上行为。 概述LinkedTransferQueue实现了TransferQueue接口，而TransferQueue又继承自BlockingQueue。众所周知，阻塞队列是生产者往队列放入元素，消费者往队列取出元素进行消费，如果队列无空闲空间/无可用元素则生产者/消费者会相应阻塞。 一般的阻塞队列，生产者和消费者是互不关心的，即两者完全解耦。正常情况下一般是不会互相阻塞(队列有足够的空间，生产者不会因为队列无空闲空间而阻塞；队列有足够的元素，消费者不会因为队列无元素可取而阻塞)。生产者将元素入队就可以离开了，不关心谁取走了它的元素；消费者将元素取出就可以离开了，不关心谁放的这个元素。但是TransferQueue不是，它的生产者和消费者允许互相阻塞。 LinkedTransferQueue的算法采用的是一种Dual Queue的变种，Dual Queues with Slack。Dual Queue不仅能存放数据节点，还能存放请求节点。一个数据节点想要入队，这个时候队列里正好有请求节点，此时”匹配”，并移除该请求节点。Blocking Dual Queue入队一个未匹配的请求节点时，会阻塞直到匹配节点入队。Dual Synchronous Queue在Blocking Dual Queue基础上，还会在一个未匹配的数据节点入队时也会阻塞。而Dual Transfer Queue支持以上任何一种模式，具体是哪一种取决于调用者，也就是有不同的api支持。 FIFO Dual Queue使用的是名叫M&amp;S的一种lock-free算法&lt;(http://www.cs.rochester.edu/u/scott/papers/1996_PODC_queues.pdf)&gt;的变种。 M&amp;S算法维护一个”head”指针和一个”tail”指针。head指向一个匹配节点M，M指向第一个未匹配节点(如果未空则指向null)，tail指向未匹配节点。如下在Dual Queue内部，由链表组成，对于每个链表节点维护一个match status。在LinkedTransferQueue中为item。对于一个data节点，匹配过程体现在item上就是:non-null -&gt; null，反过来，对于一个request节点，匹配过程就是：null-&gt;non null。Dual Queue的这个match status一经改变，就不会再变更它了。也就是说这个item cas操作过后就不会动了。 基于此，现在的Dual Queue的组成可能就是在一个分割线前全是M节点，分割线后全是U节点。假设我们不关心时间和空间，入队和出队的过程就是遍历这个链表找到第一个U和最后一个U，这样我们就不会有cas竞争的问题了。 基于以上的分析，我们就可以有一个这样的tradeoff来减少head tail的cas竞争。如下：将head和第一个未匹配的节点U之间的距离叫做”slack”，根据一些经验和实验数据发现，slack在1-3之间时工作的更好，所以，LinkedTransferQueue将其定为2。 源码阅读关于BlockingQueue的相关行为不做过多解读，主要看继承自TransferQueue的行为。 内部变量1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374 /** * head of the queue; null until first enqueue */ transient volatile Node head; /** * tail of the queue; null until first append */ private transient volatile Node tail;/* * Possible values for "how" argument in xfer method. */ private static final int NOW = 0; // for untimed poll, tryTransfer private static final int ASYNC = 1; // for offer, put, add private static final int SYNC = 2; // for transfer, take private static final int TIMED = 3; // for timed poll, tryTransfer// Node 类结构 static final class Node &#123; final boolean isData; // false if this is a request node // 作者解释说item用Object，没用泛型的原因是利于垃圾回收 // quoto:Uses Object, not E, for items to allow forgetting them after use. // 没查到为啥利于垃圾回收 volatile Object item; // initially non-null if isData; CASed to match volatile Node next; volatile Thread waiter; // null until waiting /** * Constructs a new node. Uses relaxed write because item can * only be seen after publication via casNext. */ Node(Object item, boolean isData) &#123; // 之所以可以用putObject，是因为item为volatile // 其次Node构造是在pred.casNext之后(casNext之后，构造node才算能够可达) // 所以可见性是能够保证的 UNSAFE.putObject(this, itemOffset, item); // relaxed write this.isData = isData; &#125; /** * Links node to itself to avoid garbage retention. Called * only after CASing head field, so uses relaxed write. */ final void forgetNext() &#123; UNSAFE.putObject(this, nextOffset, this); &#125; /** * Sets item to self and waiter to null, to avoid garbage * retention after matching or cancelling. Uses relaxed writes * because order is already constrained in the only calling * contexts: item is forgotten only after volatile/atomic * mechanics that extract items. Similarly, clearing waiter * follows either CAS or return from park (if ever parked; * else we don't care). */ final void forgetContents() &#123; UNSAFE.putObject(this, itemOffset, this); UNSAFE.putObject(this, waiterOffset, null); &#125; /** * Returns true if this node has been matched, including the * case of artificial matches due to cancellation. */ // 判断是否该Node是否匹配过 final boolean isMatched() &#123; Object x = item; return (x == this) // manual cancel // matched || ((x == null) == isData); &#125; &#125; 由以上代码能看到，内部维护的变量相对来说还是比较少的，主要是链表的head 和tail。最下边的四个静态场景是作为xfer方法how参数，以区分不通方法用于不同场景的调用。 NOW 用于poll()、tryTransfer(E e)方法调用 ASYNC 用于offset()、put()、add()方法调用 SYNC 用于transfer()、take()方法调用 TIMED 用于poll(long timeout,TimeUnit unit)、tryTransfer(E e,long timeout,TimeUnit unit)方法调用 行为分析1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/* * 通过xfer方法实现，同步不同的参数来区分不同的场景。 *//** * producer，要是有等待的consumer成功，否则失败 */public boolean tryTransfer(E e) &#123; return xfer(e, true, NOW, 0) == null; &#125;/** * producer，失败会interrupt,并抛出异常 */ public void transfer(E e) throws InterruptedException &#123; if (xfer(e, true, SYNC, 0) != null) &#123; Thread.interrupted(); // failure possible only due to interrupt throw new InterruptedException(); &#125; &#125;/** * producer,在timeout时间内，成功，否则 */ public boolean tryTransfer(E e, long timeout, TimeUnit unit) throws InterruptedException &#123; if (xfer(e, true, TIMED, unit.toNanos(timeout)) == null) return true; if (!Thread.interrupted()) return false; throw new InterruptedException(); &#125;/** * consumer，在unit.toNanos(timeout)时间内成功，否则会抛出异常 */ public E poll(long timeout, TimeUnit unit) throws InterruptedException &#123; E e = xfer(null, false, TIMED, unit.toNanos(timeout)); if (e != null || !Thread.interrupted()) return e; throw new InterruptedException(); &#125;/** * consumer,队列有即返回，否则null，不会阻塞等待 */ public E poll() &#123; return xfer(null, false, NOW, 0); &#125;/** * consumer,阻塞等待直到队列有元素(producer)可取 */ public E take() throws InterruptedException &#123; E e = xfer(null, false, SYNC, 0); if (e != null) return e; Thread.interrupted(); throw new InterruptedException(); &#125;/** * 队列中是否有等待的consuemr */ public boolean hasWaitingConsumer() &#123; return firstOfMode(false) != null; &#125;/** * 队列中等待的consumer总数 */ public int getWaitingConsumerCount() &#123; return countOfMode(false); &#125; 以上是主要我们要分析的行为，可以看见，出来 hasWaitingConsumer 和 getWaitingConsumerCount ，其他对队列操作元素的方法，都是通过xfer方法实现的。先看 hasWaitingConsumer 和 getWaitingConsumerCount 。 12345678910111213141516171819202122232425262728293031323334 public boolean hasWaitingConsumer() &#123; return firstOfMode(false) != null; &#125;/** * Returns the first unmatched node of the given mode, or null if * none. Used by methods isEmpty, hasWaitingConsumer. */ private Node firstOfMode(boolean isData) &#123; for (Node p = head; p != null; p = succ(p)) &#123; // p.isMatched: // return (item == this) || ((x == null) == isData); // item == this 表示手动取消 // ((x == null) == isData) 表示已匹配 if (!p.isMatched()) // 注意，队列里，未匹配的所有节点一定都是同一种类型的节点 // p是队列中第一个未匹配的节点 // 无非分两种情况分析：isData为true 和false // 1.true 如果p.isData == isData ，那么p为producer，那么返回当前节点，否则返回null。 // 2.false 如果p.isData == isData,那么p为consumer.... return (p.isData == isData) ? p : null; &#125; return null; &#125; /** * Returns the successor of p, or the head node if p.next has been * linked to self, which will only be true if traversing with a * stale pointer that is now off the list. */ final Node succ(Node p) &#123; Node next = p.next; return (p == next) ? head : next; &#125; 1234567891011121314151617181920212223242526272829303132public int getWaitingConsumerCount() &#123; return countOfMode(false);&#125;/** * Traverses and counts unmatched nodes of the given mode. * Used by methods size and getWaitingConsumerCount. */private int countOfMode(boolean data) &#123; int count = 0; for (Node p = head; p != null; ) &#123; // p.isMatched: // return (item == this) || ((x == null) == isData); // item == this 表示手动取消 // ((x == null) == isData) 表示已匹配 if (!p.isMatched()) &#123; // 如果不等，那么后面的节点就不能有p.isData类型的节点了。 if (p.isData != data) return 0; if (++count == Integer.MAX_VALUE) // saturated break; &#125; Node n = p.next; if (n != p) p = n; else &#123; count = 0; p = head; &#125; &#125; return count;&#125; 以上两个方法还是挺清晰的，主要是要注意一个地方，就是第一个未匹配节点的类型，决定了后边未匹配节点的类型。接下来看看 xfer 方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137private E xfer(E e, boolean haveData, int how, long nanos) &#123; // 不允许，说自己有货，口袋确实空的，这谁都顶不住的 if (haveData &amp;&amp; (e == null)) throw new NullPointerException(); Node s = null; // the node to append, if needed retry: for (; ; ) &#123; // restart on append race for (Node h = head, p = h; p != null; ) &#123; // find &amp; match first node boolean isData = p.isData; Object item = p.item; // 未匹配过 // 如果p.item == p || (p.item == null) == p.isData 则已匹配 p.item == p cancel的情况 // match consumer:p.item -&gt; itself producer:p.item-&gt;null if (item != p &amp;&amp; (item != null) == isData) &#123; // unmatched // 同类型,则break，在下一个循环里进行入队操作 if (isData == haveData) // can't match break; // match 过程: // consumer:p.item -&gt; data; producer:p.item -&gt; null // 试图匹配，item-&gt;e if (p.casItem(item, e)) &#123; // match for (Node q = p; q != h; ) &#123; Node n = q.next; // update by 2 unless singleton // 移动head，如果q.next == null 则head-&gt;q，否则head-&gt;n if (head == h &amp;&amp; casHead(h, n == null ? q : n)) &#123; // 更新head成功，将其next指针指向自己，以作清理 h.forgetNext(); break; &#125; // advance and retry // 失败：1.head改变；2.cas竞争失败。总之就是有其他线程已经移动了head if ((h = head) == null || // 即 slack &gt;= 2,需要重试，以调整slack的值在[0,2)范围内 (q = h.next) == null || !q.isMatched()) break; // unless slack &lt; 2 &#125; LockSupport.unpark(p.waiter); return LinkedTransferQueue.&lt;E&gt;cast(item); &#125; // 失败了，已经有其他线程捷足先登，重试 &#125; // 前边都是匹配过的节点，接着往后找 Node n = p.next; p = (p != n) ? n : (h = head); // Use head if p offlist &#125; // 当how == NOW时，匹配失败立即返回，不会入队 if (how != NOW) &#123; // No matches available if (s == null) s = new Node(e, haveData); Node pred = tryAppend(s, haveData); // 当pred == null时，表示当前队列的"分割线"后节点的mode，已经时过境迁了。 if (pred == null) continue retry; // lost race vs opposite mode // how != ASYNC 需要阻塞等待 if (how != ASYNC) return awaitMatch(s, pred, e, (how == TIMED), nanos); &#125; return e; // not waiting &#125;&#125;private Node tryAppend(Node s, boolean haveData) &#123; for (Node t = tail, p = t; ; ) &#123; // move p to last node and append Node n, u; // temps for reads of next &amp; tail if (p == null &amp;&amp; (p = head) == null) &#123; // 队列为空 // head并不需要dummy node if (casHead(null, s)) return s; // initialize &#125; else if (p.cannotPrecede(haveData)) // haveData mode的节点s并不能成为p的后继 return null; // lost race vs opposite mode // tail并不是"tail",继续向后 else if ((n = p.next) != null) // not last; keep traversing p = p != t &amp;&amp; t != (u = tail) ? (t = u) : // stale tail (p != n) ? n : null; // restart if off list // cas失败，已经有其他线程已经先行cas成功 else if (!p.casNext(null, s)) p = p.next; // re-read on CAS failure else &#123; // tail后还有其他节点 if (p != t) &#123; // update if slack now &gt;= 2 while ((tail != t || !casTail(t, s)) &amp;&amp; // tail已经改变或者移动tail失败 (t = tail) != null &amp;&amp; // (s = t.next) != null &amp;&amp; (s = s.next) != null means slack &gt;= 2 (s = t.next) != null &amp;&amp; // advance and retry (s = s.next) != null &amp;&amp; s != t); &#125; return p; &#125; &#125;&#125;private E awaitMatch(Node s, Node pred, E e, boolean timed, long nanos) &#123; // how = timed == false ? SYNC : TIMED final long deadline = timed ? System.nanoTime() + nanos : 0L; Thread w = Thread.currentThread(); int spins = -1; // initialized after first item and cancel checks java.util.concurrent.ThreadLocalRandom randomYields = null; // bound if needed for (; ; ) &#123; Object item = s.item; if (item != e) &#123; // matched // assert item != s; s.forgetContents(); // avoid garbage // 匹配完成 return LinkedTransferQueue.&lt;E&gt;cast(item); &#125; if ((w.isInterrupted() || (timed &amp;&amp; nanos &lt;= 0)) &amp;&amp; s.casItem(e, s)) &#123; // cancel // 这里 mark sweep unsplice(pred, s); return e; &#125; // 自旋 if (spins &lt; 0) &#123; // establish spins at/near front if ((spins = spinsFor(pred, s.isData)) &gt; 0) randomYields = ThreadLocalRandom.current(); &#125; else if (spins &gt; 0) &#123; // spin --spins; if (randomYields.nextInt(CHAINED_SPINS) == 0) Thread.yield(); // occasionally yield &#125; else if (s.waiter == null) &#123; s.waiter = w; // request unpark then recheck // 阻塞等待，等待唤醒 &#125; else if (timed) &#123; nanos = deadline - System.nanoTime(); if (nanos &gt; 0L) LockSupport.parkNanos(this, nanos); &#125; else &#123; LockSupport.park(this); &#125; &#125;&#125; 结合xfer注释，对照着前边的几个操作队列元素的方法，多理解即便，效果更佳。 总结 内部采用了Dual Stack With Slack的结构，slack根据实践数据采用了slack=2 操作队列的元素的各个方法通过xfer方法实现，具体是根据不同的how参数来区分不同场景 对于一个data节点，匹配过程体现在item上就是:non-null -&gt; null，反过来，对于一个request节点，匹配过程就是：null-&gt;non null。 一个可阻塞的节点入队时，如果队列中有等待可匹配的节点，则匹配并移除该节点，否则，入队自旋等待或阻塞等待直至匹配成功或唤醒。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK源码阅读之SynchronousQueue]]></title>
    <url>%2F157f68af.html</url>
    <content type="text"><![CDATA[写在前面 根据源码总的描述，SynchronousQueue是一个阻塞队列，所有的入队出队操作都是阻塞的。根据作者的描述，这个工具的定位是在线程间同步对象，以达到在线程间传递一些信息、事件、或者任务的目的(_They are well suited for handoff designs, in which an object running in one thread must sync up with an object running in another thread in order to hand it some information, event, or )。关于这个工具的描述，作者只介绍了这么多。SynchronousQueue是实现了BlockingQueue，但是有与一般意义上的queue(比如ArrayBlockingQueue)不一样，它内部并没有存放元素的地方。入队阻塞直到出队成功，反之亦然，在线程间同步传递对象，以在线程间同步信息。 实现原理SynchronousQueue内部并没有容纳元素的数据结构，也就是说SynchronousQueue并不存储元素。实现采用了一种无锁算法，扩展的Dual stack and Dual Queue算法，算法的大概实现是采用链表，用头结点(head)和尾结点(tail)记录队列状态，而队列可以根据头尾以及当前节点状态，在不需要锁的情况的执行入出队操作。此外，竞争时，支持公平竞争和非公平竞争。公平竞争实现采用先进先出队列( FIFO queue)；而非公平竞争实现采用先进后出栈(FILO stack)。 内部结构12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 abstract static class Transferer&lt;E&gt; &#123; abstract E transfer(E e, boolean timed, long nanos); &#125; /** The number of CPUs, for spin control */ static final int NCPUS = Runtime.getRuntime().availableProcessors(); static final int maxTimedSpins = (NCPUS &lt; 2) ? 0 : 32 static final int maxUntimedSpins = maxTimedSpins * 16; /** * The number of nanoseconds for which it is faster to spin * rather than to use timed park. A rough estimate suffices. */ static final long spinForTimeoutThreshold = 1000L; static final class TransferStack&lt;E&gt; extends Transferer&lt;E&gt; &#123; /** Node represents an unfulfilled consumer */ // 表示consumer，获取元素的请求 static final int REQUEST = 0; /** Node represents an unfulfilled producer */ // 表示producer，插入元素的请求 static final int DATA = 1; /** Node is fulfilling another unfulfilled DATA or REQUEST */ // 这个状态就是匹配上了队列中的某一个节点 static final int FULFILLING = 2; /** Node class for TransferStacks. */ static final class SNode &#123; volatile SNode next; // next node in stack // 匹配上了某个节点 volatile SNode match; // the node matched to this volatile Thread waiter; // to control park/unpark Object item; // data; or null for REQUESTs int mode; //...省略其他代码 &#125; /** * Puts or takes an item. * E transfer(E e, boolean timed, long nanos)&#123; // ... &#125; //... 省略其他代码 &#125; static final class TransferQueue&lt;E&gt; extends Transferer&lt;E&gt;&#123; static final class QNode &#123; volatile QNode next; // next node in queue // 注意区别SNode的item是volatile的 volatile Object item; // CAS'ed to or from null volatile Thread waiter; // to control park/unpark final boolean isData; //... &#125; /** Head of queue */ transient volatile QNode head; /** Tail of queue */ transient volatile QNode tail; /** * Reference to a cancelled node that might not yet have been * unlinked from queue because it was the last inserted node * when it was cancelled. */ transient volatile QNode cleanMe; /** * Puts or takes an item. * E transfer(E e, boolean timed, long nanos)&#123;//... &#125; &#125; 没有用于存储队列元素内部变量，并且有表示自旋时间的静态变量。内部有个Transferer抽象类，抽象类只有一个transfer方法。分别有TransferStack和TransferQueue实现了这个类，表示非公平和公平两种模式。 transfer主要逻辑都在这个transfer方法中，包括出入队也都是通过这个方法实现的。 offer12345678910111213141516/** * Inserts the specified element into this queue, waiting if necessary * up to the specified wait time for another thread to receive it. * * 插入元素，阻塞给定时间，直到另一线程接收到插入元素 */public boolean offer(E e, long timeout, java.util.concurrent.TimeUnit unit) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); // 方法都是一个方法，区别在于第一个参数是否为null if (transferer.transfer(e, true, unit.toNanos(timeout)) != null) return true; if (!Thread.interrupted()) return false; throw new InterruptedException();&#125; poll1234567891011121314 /** * Retrieves and removes the head of this queue, waiting * if necessary up to the specified wait time, for another thread * to insert it. * * 获取和移除队列头部元素，阻塞给定时间，直到另一线程往队列插入元素 */public E poll(long timeout, TimeUnit unit) throws InterruptedException &#123; // 方法都是一个方法，区别在于第一个参数是否为null E e = transferer.transfer(null, true, unit.toNanos(timeout)); if (e != null || !Thread.interrupted()) return e; throw new InterruptedException(); &#125; TransferStack.transferTransferStack是非公平竞争的实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155 E transfer(E e, boolean timed, long nanos) &#123; /* * Basic algorithm is to loop trying one of three actions: * * 1. If apparently empty or already containing nodes of same * mode, try to push node on stack and wait for a match, * returning it, or null if cancelled. * * 2. If apparently containing node of complementary mode, * try to push a fulfilling node on to stack, match * with corresponding waiting node, pop both from * stack, and return matched item. The matching or * unlinking might not actually be necessary because of * other threads performing action 3: * * 3. If top of stack already holds another fulfilling node, * help it out by doing its match and/or pop * operations, and then continue. The code for helping * is essentially the same as for fulfilling, except * that it doesn't return the item. */ SNode s = null; // constructed/reused as needed // 根据第一个参数决定具体是哪一种请求(poll or offer) int mode = (e == null) ? REQUEST : DATA; for (;;) &#123; SNode h = head; // 队列目前为空 或者 队列中全是同一种类型的节点 if (h == null || h.mode == mode) &#123; // empty or same-mode if (timed &amp;&amp; nanos &lt;= 0) &#123; // can't wait // 这种直接返回null if (h != null &amp;&amp; h.isCancelled()) casHead(h, h.next); // pop cancelled node else return null; // 构造一个节点入队 // s.next=h &#125; else if (casHead(h, s = snode(s, e, h, mode))) &#123; // 自旋或阻塞等待直到fulfill匹配 SNode m = awaitFulfill(s, timed, nanos); // 对于TransferStack，取消(超时)的节点,会赋值match为this // 当match等于自身的时候就是该clean的节点，说明等待足够长时间了 if (m == s) &#123; // wait was cancelled clean(s); return null; &#125; // fulfill，匹配 // h -&gt; fulfill node ; s -&gt; match node; if ((h = head) != null &amp;&amp; h.next == s) // s节点出队(或者该说出栈) casHead(h, s.next); // help s's fulfiller // 如果是consumer的话，那么返回的值该是fulfill节点m的值，否则就是s节点的值 return (E) ((mode == REQUEST) ? m.item : s.item); &#125; // 那么这里就是fulfill操作了 // h节点并不是fulfill &#125; else if (!isFulfilling(h.mode)) &#123; // try to fulfill if (h.isCancelled()) // already cancelled casHead(h, h.next); // pop and retry // 入队一个fulfill节点 // s.next = h; h也应该是s的匹配节点 else if (casHead(h, s=snode(s, e, h, FULFILLING|mode))) &#123; // for循环 以防刚好待匹配节点因为时间到了失效了 for (;;) &#123; // loop until matched or waiters disappear SNode m = s.next; // m is s's match // 队列中已经没有等待节点 if (m == null) &#123; // all waiters are gone // 这个时候就不该插fulfill节点了，所以pop刚插的fulfill casHead(s, null); // pop fulfill node s = null; // use new node next time break; // restart main loop &#125; SNode mn = m.next; // 即 m.match = s; unpark(m.thread); if (m.tryMatch(s)) &#123; casHead(s, mn); // pop both s and m // 同上 return (E) ((mode == REQUEST) ? m.item : s.item); &#125; else // lost match // 匹配节点m已经cancel失效，则移除m节点 s.casNext(m, mn); // help unlink &#125; &#125; // h节点是fulfill节点，总会有这种情况的出现 &#125; else &#123; // help a fulfiller SNode m = h.next; // m is h's match if (m == null) // waiter is gone casHead(h, null); // pop fulfilling node else &#123; SNode mn = m.next; if (m.tryMatch(h)) // help match casHead(h, mn); // pop both h and m else // lost match h.casNext(m, mn); // help unlink &#125; &#125; &#125; &#125;// 构造一个SNode节点// next一般是h static SNode snode(SNode s, Object e, SNode next, int mode) &#123; if (s == null) s = new SNode(e); s.mode = mode; s.next = next; return s; &#125;// Spins/blocks until node s is matched by a fulfill operation.// 自旋，直到fulfill操作匹配节点 SNode awaitFulfill(SNode s, boolean timed, long nanos) &#123; final long deadline = timed ? System.nanoTime() + nanos : 0L; Thread w = Thread.currentThread(); // 如果s在队列头，或者队列中有一有效的fulfill节点，那么将采用自旋 int spins = (shouldSpin(s) ? (timed ? maxTimedSpins : maxUntimedSpins) : 0); for (;;) &#123; if (w.isInterrupted()) // SNode.tryCancel： // s.match = s s.tryCancel(); // 节点s的匹配节点 SNode m = s.match; if (m != null) return m; if (timed) &#123; nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; s.tryCancel(); continue; &#125; &#125; // 自旋 if (spins &gt; 0) spins = shouldSpin(s) ? (spins-1) : 0; else if (s.waiter == null) s.waiter = w; // establish waiter so can park next iter // 阻塞 else if (!timed) LockSupport.park(this); // 给的时间大于阈值才会进入阻塞状态 else if (nanos &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanos); &#125; &#125; /** * Returns true if node s is at head or there is an active * fulfiller. */ boolean shouldSpin(SNode s) &#123; SNode h = head; return (h == s || h == null || isFulfilling(h.mode)); &#125; TransferStack总是先进后出，并不保证公平，甚至在一些极端情况会导致一部分请求总得不到调度。详情： 请求到达，如果栈为空，则入栈相对应状态(consumer-&gt;request、producer-&gt;data)节点 如果栈不为空，并且目前节点状态与栈顶结点状态不一致(即并不是都为consumer or producer)，那么入栈一fulfill节点 匹配过程，匹配栈顶结点为fulfill以及后继节点为头结点的match，成功则出栈两节点 最后根据节点状态返回对应的需要同步的数据对象TransferQueue.transferTransferQueue是公平竞争的实现。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143 E transfer(E e, boolean timed, long nanos) &#123; /* Basic algorithm is to loop trying to take either of * two actions: * * 1. If queue apparently empty or holding same-mode nodes, * try to add node to queue of waiters, wait to be * fulfilled (or cancelled) and return matching item. * * 2. If queue apparently contains waiting items, and this * call is of complementary mode, try to fulfill by CAS'ing * item field of waiting node and dequeuing it, and then * returning matching item. * * In each case, along the way, check for and try to help * advance head and tail on behalf of other stalled/slow * threads. * * The loop starts off with a null check guarding against * seeing uninitialized head or tail values. This never * happens in current SynchronousQueue, but could if * callers held non-volatile/final ref to the * transferer. The check is here anyway because it places * null checks at top of loop, which is usually faster * than having them implicitly interspersed. */ QNode s = null; // constructed/reused as needed boolean isData = (e != null); for (;;) &#123; QNode t = tail; QNode h = head; // 初始化还未完成 if (t == null || h == null) // saw uninitialized value continue; // spin // 队列为空，或当前节点和尾为相同类型节点 if (h == t || t.isData == isData) &#123; // empty or same-mode QNode tn = t.next; // 尾巴变了 if (t != tail) // inconsistent read continue; // 尾巴变了，但未更新，help一下 if (tn != null) &#123; // lagging tail advanceTail(t, tn); continue; &#125; if (timed &amp;&amp; nanos &lt;= 0) // can't wait return null; if (s == null) // 构造节点入队 s = new QNode(e, isData); if (!t.casNext(null, s)) // failed to link in continue; // 入队 advanceTail(t, s); // swing tail and wait // 阻塞等待fulfill Object x = awaitFulfill(s, e, timed, nanos); // QNode 的item： // 等于this-&gt;cancel;null-&gt;consumer;not null -&gt; producer if (x == s) &#123; // wait was cancelled clean(t, s); return null; &#125; // s脱离队列，已不在队列中，这个时候需要重置一下队列 if (!s.isOffList()) &#123; // not already unlinked advanceHead(t, s); // unlink if head if (x != null) // and forget fields s.item = s; s.waiter = null; &#125; // 结合下面的else看，在else中会更改s.item // x != null 表示当前为consumer // x == null 表示当前为producer return (x != null) ? (E)x : e; &#125; else &#123; // complementary-mode // m为fulfill节点 QNode m = h.next; // node to fulfill // 队列改变，或者fulfill为空则需要retry if (t != tail || m == null || h != head) continue; // inconsistent read Object x = m.item; // m的item已经操作过:m.casItem(x,e)，即already fulfilled if (isData == (x != null) || // m already fulfilled // m节点失效 x == m || // m cancelled // 相当于TransferStack的匹配过程，将m.item=e // cas失败，表示others have done，则需要重置头节点retry !m.casItem(x, e)) &#123; // lost CAS advanceHead(h, m); // dequeue and retry continue; &#125; // 成功，h出队，head=m advanceHead(h, m); // successfully fulfilled // 唤醒m节点阻塞线程 LockSupport.unpark(m.waiter); return (x != null) ? (E)x : e; &#125; &#125; &#125;/** * Spins/blocks until node s is fulfilled. */ Object awaitFulfill(QNode s, E e, boolean timed, long nanos) &#123; /* Same idea as TransferStack.awaitFulfill */ final long deadline = timed ? System.nanoTime() + nanos : 0L; Thread w = Thread.currentThread(); int spins = ((head.next == s) ? (timed ? maxTimedSpins : maxUntimedSpins) : 0); for (;;) &#123; if (w.isInterrupted()) // QNode.tryCancel: // s.item = s s.tryCancel(e); Object x = s.item; if (x != e) return x; if (timed) &#123; nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; s.tryCancel(e); continue; &#125; &#125; // 自旋 if (spins &gt; 0) --spins; else if (s.waiter == null) s.waiter = w; // 阻塞 else if (!timed) LockSupport.park(this); // 同样，需要大于阻塞阈值才会真正阻塞，否则就自旋 else if (nanos &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanos); &#125; &#125; TransferQueue总是FIFO，保证了公平。详情： 请求到达，如果队列为空，或者尾节点的类型和当前节点相同（同是consumer或producer），则在队列尾部入队当前节点等待，直到被唤醒 否则头结点出队，唤醒头结点对应线程，返回对应值]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK源码阅读之StampedLock]]></title>
    <url>%2F77e66558.html</url>
    <content type="text"><![CDATA[写在前面 作者Doug Lea_如此描述这个类：_A capability-based lock with three modes for controlling read/write access.这是一个有三种模式的锁。具体哪三种模式，请看源码中的示例。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 三种模式用法 * reading &amp; writing &amp; optimistic read */class Point &#123; private double x, y; private final StampedLock sl = new StampedLock(); // writing mode void move(double deltaX, double deltaY) &#123; // an exclusively locked method long stamp = sl.writeLock(); try &#123; x += deltaX; y += deltaY; &#125; finally &#123; sl.unlockWrite(stamp); &#125; &#125; // Optimistic Reading mode double distanceFromOrigin() &#123; // A read-only method long stamp = sl.tryOptimisticRead(); double currentX = x, currentY = y; // 会有一个验证的步骤 if (!sl.validate(stamp)) &#123; stamp = sl.readLock(); try &#123; currentX = x; currentY = y; &#125; finally &#123; sl.unlockRead(stamp); &#125; &#125; return Math.sqrt(currentX currentX + currentY currentY); &#125; // lock upgrade void moveIfAtOrigin(double newX, double newY) &#123; // upgrade // Could instead start with optimistic, not read mode long stamp = sl.readLock(); try &#123; while (x == 0.0 &amp;&amp; y == 0.0) &#123; long ws = sl.tryConvertToWriteLock(stamp); if (ws != 0L) &#123; stamp = ws; x = newX; y = newY; break; &#125; else &#123; sl.unlockRead(stamp); stamp = sl.writeLock(); &#125; &#125; &#125; finally &#123; sl.unlock(stamp); &#125; &#125;&#125; 在ReentrantReadWriteLock中我们知道，如果在读并发比较高的情况下，那么可能会导致写线程饥饿。而StampedLock并不会发生写饥饿。另外，它也有锁升级的特性（ReentrantReadWriteLock只有锁降级）。那么看看它是怎么实现的吧。 实现原理锁算法是借鉴了序列锁(linux内核，参考文章1、文章2)和顺序读写锁(参考文章)。具体算法细节还可以参考源码中的描述。简单描述一下，用一个long类型（state）的变量表示锁状态（写锁、读锁）,低7位表示读锁状态，其他位表示写锁（第8位是否未1表示是否持有写锁，前面说的sequence lock），如果低7位不够表示读锁位，那么会用一个int值表示”溢出”的读锁。那么写锁可以表示为：state+=2^7；读锁可以表示为state+=1。此外，源码中还用了大量自旋来减少饥饿的概率，头部节点表示的线程不会阻塞，而是会一直自旋等待锁释放。结合内部的变量声明理解一下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 /** The number of bits to use for reader count before overflowing */// 在state上用低7位表示reader count private static final int LG_READERS = 7;// Values for lock state and stamp operations// 表示一个reader private static final long RUNIT = 1L;// 表示一个writer private static final long WBIT = 1L &lt;&lt; LG_READERS;// reader掩码 private static final long RBITS = WBIT - 1L;// state上能表示的最大reader count private static final long RFULL = RBITS - 1L;// state上表示锁的所有位的掩码 private static final long ABITS = RBITS | WBIT;// 反码 private static final long SBITS = ~RBITS; // note overlap with ABITS // Values for node status; order matters// 表示等待 private static final int WAITING = -1;// 表示取消 private static final int CANCELLED = 1; // Modes for nodes (int not boolean to allow arithmetic)// 读 private static final int RMODE = 0;// 写 private static final int WMODE = 1; /** Wait nodes */ static final class WNode &#123; volatile WNode prev; volatile WNode next; // 表示当前节点上的reader volatile WNode cowait; // list of linked readers volatile Thread thread; // non-null while possibly parked // 相当于CLH上的那个locked volatile int status; // 0, WAITING, or CANCELLED final int mode; // RMODE or WMODE WNode(int m, WNode p) &#123; mode = m; prev = p; &#125; &#125; /** Head of CLH queue */ private transient volatile WNode whead; /** Tail (last) of CLH queue */ private transient volatile WNode wtail; /** Lock sequence/state */// 锁状态，或者叫锁序列 private transient volatile long state; /** extra reader count when state read count saturated */// state上只有低7位用来表示reader，超出的部分用这个int值表示// 注意这个变量并不是volatile// 改变这个值时，都是cas操作state，并且改变之后也会对state赋值,可见性和原子性通过这两个操作保证的// 即 s=state; cmp(state,RBITS);++readerOverflow(or readerOverflow -= 1);state=s// 自旋锁保证原子性，内存屏障保证可见性 private transient int readerOverflow; 总结一下，用一个long型变量表示锁的计数，读锁计数用低7位表示，超出部分用一个int值表示。其他位表示写锁状态，第8位如果为1则表示写锁被持有，为0表示未被持有，也就是序列锁，这样即保证了锁（包括读写）的原子性，也能提高效率(操作读或写锁要保证互斥)。用一个CLH队列表示等待的节点，避免饥饿现象，节点首先会采用自旋的方式尝试获取锁。接下来就跟着3个示例解析一下加解锁的流程。 Reading Mode经过上面的分析，state上的低7位是用来表示reader count，而溢出部分，则用readerOverflow表示。 readLock123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262 /** * 非互斥获取读锁，如果暂时不可取，则会阻塞直到可取 */ public long readLock() &#123; long s = state, next; // bypass acquireRead on common uncontended case // 队列为空 return ((whead == wtail // 并且reader count 小于RFULL(2^7 - 2=126) &amp;&amp; (s &amp; ABITS) &lt; RFULL // cas操作state,state += 1 &amp;&amp; U.compareAndSwapLong(this, STATE, s, next = s + RUNIT)) ? // 获取成功返回最新state值,否则，进入acquireRead(false,0L) next : acquireRead(false, 0L)); &#125;/** * 方法有点长，这个方法主要做的是在一定时间内通过自旋去请求锁，如果仍没请求到，则进入阻塞状态 */ private long acquireRead(boolean interruptible, long deadline) &#123; WNode node = null, p; for (int spins = -1;;) &#123; WNode h; // 队列为空(or仅有一个节点) if ((h = whead) == (p = wtail)) &#123; for (long m, s, ns;;) &#123; // 写锁未被占有，并且读锁数量小于RFULL（126），ABITS=255,2^8-1 if ((m = (s = state) &amp; ABITS) &lt; RFULL ? // cas尝试获取读锁，state+=1 U.compareAndSwapLong(this, STATE, s, ns = s + RUNIT) : // 写锁未被占用，读锁数量大于RFULL，执行tryIncReaderOverflow方法 // 关于tryIncReaderOverflow见下该方法解析 (m &lt; WBIT &amp;&amp; (ns = tryIncReaderOverflow(s)) != 0L)) // 更新成功，始终返回当前最新锁状态 return ns; // 到这里那写锁就被占用了 else if (m &gt;= WBIT) &#123; // 自旋等待 if (spins &gt; 0) &#123; // 这个随机是几个意思？分散线程状态聚集在一个状态？避免都在一个时间点阻塞or唤醒？ if (LockSupport.nextSecondarySeed() &gt;= 0) --spins; &#125; else &#123; // 自旋结束 if (spins == 0) &#123; WNode nh = whead, np = wtail; // 头尾未改变 if ((nh == h &amp;&amp; np == p) // 头尾改变且队列不再为空 || (h = nh) != (p = np)) break; &#125; // 因为在头部，上一轮自旋未拿到锁那紧接着下一轮自旋 // SPINS = (NCPU &gt; 1) ? 1 &lt;&lt; 6 : 0; spins = SPINS; &#125; &#125; &#125; &#125; // 尾巴为空，即 队列为空，需要初始化队列 if (p == null) &#123; // initialize queue WNode hd = new WNode(WMODE, null); if (U.compareAndSwapObject(this, WHEAD, null, hd)) wtail = hd; &#125; // 构造节点入队 else if (node == null) node = new WNode(RMODE, p); // 头尾相等或尾节点表示非读锁节点(也就是尾节点是写锁请求线程节点) else if (h == p || p.mode != RMODE) &#123; // 队列尾巴改变 if (node.prev != p) // 重新置尾巴 node.prev = p; // 往队列追加节点 else if (U.compareAndSwapObject(this, WTAIL, p, node)) &#123; p.next = node; // 跳出当前自旋循环，进入下一个for自旋循环 // 唯一跳出本次for循环的地方，也就是只有在往队列新增了个节点的时候出去进入下一个for自旋循环 break; &#125; &#125; // 将当前节点放在尾巴（是读节点或者头节点）cowait(表示读锁等待节点)链表位置头部 else if (!U.compareAndSwapObject(p, WCOWAIT, node.cowait = p.cowait, node)) // 失败 node.cowait = null; // cas成功 else &#123; // 阻塞等待读锁，直到唤醒时，前驱已为头节点 for (;;) &#123; WNode pp, c; Thread w; // 唤醒头结点上的第一个wcowait if ((h = whead) != null &amp;&amp; (c = h.cowait) != null &amp;&amp; U.compareAndSwapObject(h, WCOWAIT, c, c.cowait) &amp;&amp; (w = c.thread) != null) // help release // 唤醒头结点上的cowait读节点线程 U.unpark(w); // 前驱为头结点，或当前节点为头结点，或前驱的前驱为空 // 也就是在队列的头部位置 // 则一直自旋，尝试获取读锁，除非已经有写锁被占有 if (h == (pp = p.prev) || h == p || pp == null) &#123; long m, s, ns; do &#123; // 获取读锁 if ((m = (s = state) &amp; ABITS) &lt; RFULL ? U.compareAndSwapLong(this, STATE, s, ns = s + RUNIT) : (m &lt; WBIT &amp;&amp; (ns = tryIncReaderOverflow(s)) != 0L)) return ns; // 如果是写锁已被占有则跳出 &#125; while (m &lt; WBIT); &#125; // 前驱未改变(未cancel)以及头节点未改变(即未有锁的释放获取) if (whead == h &amp;&amp; p.prev == pp) &#123; long time; // 前驱的前驱为空，或前驱节点为头结点，或节点已经cancel // 当目前节点是头节点，则跳出当前for，进入下一个for循环，自旋获取锁 if (pp == null || h == p || p.status &gt; 0) &#123; node = null; // throw away break; &#125; if (deadline == 0L) time = 0L; else if ((time = deadline - System.nanoTime()) &lt;= 0L) // 将node.status=CANCELLED // 将p的cowait链表上取消的节点剔除 return cancelWaiter(node, p, false); Thread wt = Thread.currentThread(); U.putObject(wt, PARKBLOCKER, this); node.thread = wt; // 前驱不为头结点 if ((h != pp // 或写锁已经被其它线程获取且当前头结点未改变且前驱未改变 || (state &amp; ABITS) == WBIT) &amp;&amp; whead == h &amp;&amp; p.prev == pp) // 阻塞当前线程等待唤醒 U.park(false, time); node.thread = null; U.putObject(wt, PARKBLOCKER, null); if (interruptible &amp;&amp; Thread.interrupted()) return cancelWaiter(node, p, true); &#125; &#125; &#125; &#125; // 这个循环就是头节点"进一步的深度"自旋，"其他"节点则阻塞 for (int spins = -1;;) &#123; WNode h, np, pp; int ps; // 队列为空，即当前节点在头部位置 if ((h = whead) == p) &#123; if (spins &lt; 0) // HEAD_SPINS = (NCPU &gt; 1) ? 1 &lt;&lt; 10 : 0 // 首次自旋2^10 spins = HEAD_SPINS; // (NCPU &gt; 1) ? 1 &lt;&lt; 16 : 0 else if (spins &lt; MAX_HEAD_SPINS) // 指数型递增 spins &lt;&lt;= 1; // 自旋等待锁，注意这里是在头部自旋 for (int k = spins;;) &#123; // spin at head long m, s, ns; // 下面是获取读锁 if ((m = (s = state) &amp; ABITS) &lt; RFULL ? U.compareAndSwapLong(this, STATE, s, ns = s + RUNIT) : (m &lt; WBIT &amp;&amp; (ns = tryIncReaderOverflow(s)) != 0L)) &#123; // 获得读锁 WNode c; Thread w; whead = node; node.prev = null; // 唤醒所有在node上的wcowait读锁等待线程 while ((c = node.cowait) != null) &#123; if (U.compareAndSwapObject(node, WCOWAIT, c, c.cowait) &amp;&amp; (w = c.thread) != null) U.unpark(w); &#125; return ns; &#125; // 已经有写锁被占用了，且自旋结束，结束此次自旋 else if (m &gt;= WBIT &amp;&amp; LockSupport.nextSecondarySeed() &gt;= 0 &amp;&amp; --k &lt;= 0) break; &#125; &#125; // 队列不为空，且头节点部位空 else if (h != null) &#123; WNode c; Thread w; // 依次唤醒头结点上的读锁等待线程，如果有cowait的话 while ((c = h.cowait) != null) &#123; if (U.compareAndSwapObject(h, WCOWAIT, c, c.cowait) &amp;&amp; (w = c.thread) != null) U.unpark(w); &#125; &#125; // 头节点未改变 if (whead == h) &#123; // 即前驱变了，如cancel了 if ((np = node.prev) != p) &#123; if (np != null) (p = np).next = node; // stale &#125; else if ((ps = p.status) == 0) // 更改前驱节点状态为WAITING U.compareAndSwapInt(p, WSTATUS, 0, WAITING); // 取消的节点要剔除掉 else if (ps == CANCELLED) &#123; if ((pp = p.prev) != null) &#123; node.prev = pp; pp.next = node; &#125; &#125; else &#123; long time; if (deadline == 0L) time = 0L; else if ((time = deadline - System.nanoTime()) &lt;= 0L) // 这个cancelWaiter跟前边的参数不一样 // 这里是除了前边的cancelWaiter的功能，还需要做唤醒node上的cowait // 并且将cowait链表的第一个节点唤醒，且用这个cowait替换掉node的位置 return cancelWaiter(node, node, false); Thread wt = Thread.currentThread(); U.putObject(wt, PARKBLOCKER, this); node.thread = wt; // 前驱节点状态为WAITING if (p.status &lt; 0 &amp;&amp; // 前驱不为头结点，或当前写锁被其它线程持有 (p != h || (state &amp; ABITS) == WBIT) &amp;&amp; whead == h &amp;&amp; node.prev == p) // 阻塞在这里 U.park(false, time); node.thread = null; U.putObject(wt, PARKBLOCKER, null); if (interruptible &amp;&amp; Thread.interrupted()) return cancelWaiter(node, node, true); &#125; &#125; &#125; &#125;/** * 相当于是把自旋锁对state更新 * 此外，readerOverflow并不是volatile，但是它的可见性和原子性是怎么保证的？ * 可见性：state是volatile，可以借助它的内存屏障，因为readerOverflow是在state前操作的 * 原子性：读锁在state上最大只有126个，当超过126是将state低位置为127，操作结束后将其置回126 */ private long tryIncReaderOverflow(long s) &#123; // assert (s &amp; ABITS) &gt;= RFULL; if ((s &amp; ABITS) == RFULL) &#123; // RBITS = 2^7 - 1 if (U.compareAndSwapLong(this, STATE, s, s | RBITS)) &#123; ++readerOverflow; state = s; return s; &#125; &#125; else if ((LockSupport.nextSecondarySeed() &amp; OVERFLOW_YIELD_RATE) == 0) Thread.yield(); return 0L; &#125; 首先明确获取读锁并不是互斥的，而是通过cas操作去尝试获取。其次是通过自旋去尝试获取锁，对于头部位置的节点总是在自旋等待锁。 unlockRead接下来看一下时如何释放读锁的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * If the lock state matches the given stamp, releases the * non-exclusive lock. * * @param stamp a stamp returned by a read-lock operation * @throws IllegalMonitorStateException if the stamp does * not match the current state of this lock */public void unlockRead(long stamp) &#123; long s, m; WNode h; for (;;) &#123; // SBITS=~(2^7-1),低8位的掩码 // 如果写锁的状态发生过变更 if (((s = state) &amp; SBITS) != (stamp &amp; SBITS) || // 或stamp目前表示没有锁，或state表示目前没有锁 (stamp &amp; ABITS) == 0L || (m = s &amp; ABITS) == 0L || m == WBIT) throw new IllegalMonitorStateException(); // 读锁小于RFULL=126 if (m &lt; RFULL) &#123; // cas state-=1 if (U.compareAndSwapLong(this, STATE, s, s - RUNIT)) &#123; // 最后一把锁，且队列头节点不为空，头节点状态也不是初始状态(WAITING状态) if (m == RUNIT &amp;&amp; (h = whead) != null &amp;&amp; h.status != 0) // 则释放h.next等待的线程节点 release(h); break; &#125; &#125; // 目前的写锁状态时溢出的，需要对readerOverflow进行处理，看下面该方法的解析 else if (tryDecReaderOverflow(s) != 0L) break; &#125;&#125;/** * Tries to decrement readerOverflow. * * @param s a reader overflow stamp: (s &amp; ABITS) &gt;= RFULL * @return new stamp on success, else zero */private long tryDecReaderOverflow(long s) &#123; // assert (s &amp; ABITS) &gt;= RFULL; // state读锁状态满了 if ((s &amp; ABITS) == RFULL) &#123; // 获取spinlock，即"自旋锁"，正常应该时state=127 if (U.compareAndSwapLong(this, STATE, s, s | RBITS)) &#123; int r; long next; if ((r = readerOverflow) &gt; 0) &#123; readerOverflow = r - 1; next = s; &#125; else next = s - RUNIT; // state为volatile，有barrier，加上上边那个127 //所以readerOverflow的可见性原子性得以保证 // state=126 state = next; return next; &#125; &#125; // 可能会放弃CPU时钟 else if ((LockSupport.nextSecondarySeed() &amp; OVERFLOW_YIELD_RATE) == 0) Thread.yield(); return 0L;&#125; Writing Mode writeLock读锁状态也是也是在state上表示，除了表示读锁状态的低7位，剩下的高25位都表示写锁状态。第8位为1表示写锁被占用，否则表示未被占用，写锁状态时往上递增的，也就是说获取读锁state需要+WBIT，释放也是+WBIT。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144 public long writeLock() &#123; long s, next; // bypass acquireWrite in fully unlocked case only // ABITS=2^8-1=255 return ((((s = state) &amp; ABITS) == 0L &amp;&amp; // WBIT=2^7=128 U.compareAndSwapLong(this, STATE, s, next = s + WBIT)) ? // 如果低8位为0，且cas state+=WBIT成功，即表示获取读锁成功 // 否则进入acquireWrite(false,0L),这个方法也是在无锁状态下才会获取写锁成功 next : acquireWrite(false, 0L)); &#125;// 这个方法时跟acquireRead一样，较长，也比较复杂,不过做的事情比较简单// 就是自旋等待写锁，或者入队阻塞等待唤醒 private long acquireWrite(boolean interruptible, long deadline) &#123; WNode node = null, p; for (int spins = -1;;) &#123; // spin while enqueuing long m, s, ns; // 无锁状态,则尝试获取写锁 if ((m = (s = state) &amp; ABITS) == 0L) &#123; // state += WBIT（2^7=128） if (U.compareAndSwapLong(this, STATE, s, ns = s + WBIT)) return ns; &#125; // 自旋等待 else if (spins &lt; 0) // 如果是写锁且当前队列仅有一个写锁节点(也就是这个节点持有锁)，则自旋尝试获取锁 // 否则就入队囖 spins = (m == WBIT &amp;&amp; wtail == whead) ? SPINS : 0; // 自旋 else if (spins &gt; 0) &#123; if (LockSupport.nextSecondarySeed() &gt;= 0) // 随机，原因应该是上边提过的那个 --spins; &#125; // 自旋结束，如队列为空则初始化队列 else if ((p = wtail) == null) &#123; // initialize queue WNode hd = new WNode(WMODE, null); if (U.compareAndSwapObject(this, WHEAD, null, hd)) wtail = hd; &#125; // 构造当前节点入队 else if (node == null) node = new WNode(WMODE, p); else if (node.prev != p) // CLH队列，需要指定一下prev node.prev = p; // 入队 else if (U.compareAndSwapObject(this, WTAIL, p, node)) &#123; p.next = node; // 成功入队，就需要进入下面的for循环，进入"深度自旋" break; &#125; &#125; for (int spins = -1;;) &#123; WNode h, np, pp; int ps; // 前驱就是头结点，那么就一直在这儿自旋尝试获取锁 if ((h = whead) == p) &#123; // 自旋 if (spins &lt; 0) // HEAD_SPINS = 2^10，先自旋少一点意思意思 spins = HEAD_SPINS; // MAX_HEAD_SPINS = 2^16 else if (spins &lt; MAX_HEAD_SPINS) // 如果一直没有获取到锁，自旋的时间是指数型增长，直到MAX_HEAD_SPINS spins &lt;&lt;= 1; // 一轮自旋 for (int k = spins;;) &#123; // spin at head long s, ns; // state上并无锁状态 if (((s = state) &amp; ABITS) == 0L) &#123; // 则去尝试获取，cas state+=WBIT if (U.compareAndSwapLong(this, STATE, s, ns = s + WBIT)) &#123; // 获取成功，将当前节点node置为带头大哥 whead = node; // GC node.prev = null; return ns; &#125; &#125; // 随机递减 else if (LockSupport.nextSecondarySeed() &gt;= 0 &amp;&amp; --k &lt;= 0) break; &#125; &#125; // 当前节点并不是太子，登不了基，前边排着队呢 // 头节点上的cowait，有些可能过时了，可以尝试唤醒 else if (h != null) &#123; // help release stale waiters WNode c; Thread w; // 挨个唤醒在head上的reader链表，看看当前能不能获取读锁 while ((c = h.cowait) != null) &#123; if (U.compareAndSwapObject(h, WCOWAIT, c, c.cowait) &amp;&amp; (w = c.thread) != null) U.unpark(w); &#125; &#125; // 如果当前头结点未改变 if (whead == h) &#123; // 前驱改变，清空已经失效的节点 if ((np = node.prev) != p) &#123; if (np != null) (p = np).next = node; // stale &#125; // 更新前驱节点状态为WAITING，0-&gt;WAITING else if ((ps = p.status) == 0) U.compareAndSwapInt(p, WSTATUS, 0, WAITING); // 清空已经取消的节点 else if (ps == CANCELLED) &#123; if ((pp = p.prev) != null) &#123; node.prev = pp; pp.next = node; &#125; &#125; else &#123; long time; // 0 argument to park means no timeout if (deadline == 0L) time = 0L; else if ((time = deadline - System.nanoTime()) &lt;= 0L) // 同acquireRead里的解析 return cancelWaiter(node, node, false); Thread wt = Thread.currentThread(); U.putObject(wt, PARKBLOCKER, this); node.thread = wt; // 未被cancel if (p.status &lt; 0 // 前驱节点不为头结点，或锁状态为仍然被持有,当锁置为可获取状态时，如果不是队列头部的节点，那么也可以进入park &amp;&amp; (p != h || (state &amp; ABITS) != 0L) // 头结点未改变 &amp;&amp; whead == h // 前驱仍是p &amp;&amp; node.prev == p) // 则park阻塞 U.park(false, time); // emulate LockSupport.park // 从阻塞中唤醒 node.thread = null; U.putObject(wt, PARKBLOCKER, null); if (interruptible &amp;&amp; Thread.interrupted()) return cancelWaiter(node, node, true); &#125; &#125; &#125; &#125; 由此可见，写锁的获取也是通过自旋，如果队列里有排队的节点，那么入队，保证个公平公正，这也是CLH队列的作用的地方。并不会有开始说的写锁饥饿的现象。 unlockWrite1234567891011121314/** * 如果stamp是当前锁状态，并且表示持有写锁，那么释放写锁 */public void unlockWrite(long stamp) &#123; WNode h; if (state != stamp || (stamp &amp; WBIT) == 0L) throw new IllegalMonitorStateException(); // ORIGIN = 128 &lt;&lt; 1 = 256 state = (stamp += WBIT) == 0L ? ORIGIN : stamp; if ((h = whead) != null &amp;&amp; h.status != 0) // status由WATING-&gt;0，剔除队列中cancel的节点 // 并且唤醒下一等待节点线程，如果有的话 release(h);&#125; upgrade文章开头的示例中，第三个示例就是读锁升级写锁的例子。在ReentranReadWriteLock中，写锁可以降级为读锁，而StampedLock可以由读锁能直接升级为写锁。首先是需要持有读锁(readLock)，接着会尝试升级写锁(tryConvertToWriteLock)，如果升级成功，则直接操作业务并在最后释放锁(unlock)，否则需要释放读锁(unlockRead)获取写锁(writeLock)。我们主要来看_tryConvertToWriteLock_和unlock，其它的逻辑都已经在上边讨论过了。来看看具体是怎么实现的吧。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102 /** * If the lock state matches the given stamp, performs one of * the following actions. If the stamp represents holding a write * lock, returns it. Or, if a read lock, if the write lock is * available, releases the read lock and returns a write stamp. * Or, if an optimistic read, returns a write stamp only if * immediately available. This method returns zero in all other * cases. * * @param stamp a stamp * @return a valid write stamp, or zero on failure *//** * 上边说的很明白了： * 1.如果stamp已经表示写锁，则直接返回 * 2.如果是读锁，会看看写锁是不是可获取，如果可以则会释放读锁返回写锁的stamp * 3.如果是乐观读锁，会立即返回写锁当且仅当目前时间点写锁可取 * 4.其他情况就都只会返回0 */ public long tryConvertToWriteLock(long stamp) &#123; long a = stamp &amp; ABITS, m, s, next; // stamp和state表示写锁状态相同 while (((s = state) &amp; SBITS) == (stamp &amp; SBITS)) &#123; // 锁状态（低8位）目前并不持有任何锁 if ((m = s &amp; ABITS) == 0L) &#123; // stamp表示有读锁，与state不一致，则整个方法返回0 if (a != 0L) break; // 尝试获取写锁(目前state并无锁) if (U.compareAndSwapLong(this, STATE, s, next = s + WBIT)) return next; &#125; // 写锁位已被占用，即写锁已被占用 else if (m == WBIT) &#123; // 并不是当前stamp获取的当前写锁 // 即写锁被其他线程占有 if (a != m) break; // 如果已持有写锁，则直接返回 return stamp; &#125; // 当前读锁升级为写锁 // 目前state上仅有一个读锁状态位，且stamp上是有锁额 else if (m == RUNIT &amp;&amp; a != 0L) &#123; // 则尝试释放读锁，加写锁 if (U.compareAndSwapLong(this, STATE, s, next = s - RUNIT + WBIT)) return next; &#125; else // 其他情况一概返回0 break; &#125; return 0L; &#125; /** * If the lock state matches the given stamp, releases the * corresponding mode of the lock. * * @param stamp a stamp returned by a lock operation * @throws IllegalMonitorStateException if the stamp does * not match the current state of this lock *//** * 如果stamp确实是当前的"邮戳"(相当于邮票上的邮戳，与state上对比),则释放对应的锁 */ public void unlock(long stamp) &#123; long a = stamp &amp; ABITS, m, s; WNode h; // 写锁状态未改变 while (((s = state) &amp; SBITS) == (stamp &amp; SBITS)) &#123; // 低8位，等于0表示未获取任何锁 if ((m = s &amp; ABITS) == 0L) break; // 写锁被获取 else if (m == WBIT) &#123; // 改变 if (a != m) break; state = (s += WBIT) == 0L ? ORIGIN : s; // 唤醒队列 if ((h = whead) != null &amp;&amp; h.status != 0) // unpark h.next release(h); return; &#125; // 未持有任何锁，a&gt;=WBIT表示stamp表示持有写锁，但是目前m!=WBIT，表示写锁已经被释放 else if (a == 0L || a &gt;= WBIT) break; // 接下来就是读锁的释放了 else if (m &lt; RFULL) &#123; if (U.compareAndSwapLong(this, STATE, s, s - RUNIT)) &#123; if (m == RUNIT &amp;&amp; (h = whead) != null &amp;&amp; h.status != 0) release(h); return; &#125; &#125; else if (tryDecReaderOverflow(s) != 0L) return; &#125; throw new IllegalMonitorStateException(); &#125; Optimistic Reading Mode示例中的第二个例子就是乐观读。先尝试获取读锁(tryOptimisticRead)，接着校验(validate)下stamp是否变更，如果校验通过未发生变更则直接进行下一步，否则需要获取读锁(readLock)并操作完成之后释放(unlockRead)读锁。主要来看看_tryOptimisticRead和_validate，其它的流程逻辑参考前边的讨论。 12345678910111213141516171819202122232425262728293031323334353637 /** * Returns a stamp that can later be validated, or zero * if exclusively locked. * * @return a stamp, or zero if exclusively locked *//** * 如果当前并无任何写锁被持有，则返回写锁状态(高25位)，否则返回0 */ public long tryOptimisticRead() &#123; long s; // SBITS = ~(2^7 - 1) WBIT = 2^8 return (((s = state) &amp; WBIT) == 0L) ? (s &amp; SBITS) : 0L; &#125; /** * Returns true if the lock has not been exclusively acquired * since issuance of the given stamp. Always returns false if the * stamp is zero. Always returns true if the stamp represents a * currently held lock. Invoking this method with a value not * obtained from &#123;@link #tryOptimisticRead&#125; or a locking method * for this lock has no defined effect or result. * * @param stamp a stamp * @return &#123;@code true&#125; if the lock has not been exclusively acquired * since issuance of the given stamp; else false *//** * 方法体很小，但是方法注释倒是挺长的嘛 * 其实就是验证stamp和当前state的锁状态是否一致 */ public boolean validate(long stamp) &#123; // load barrier，保证state的可见性 U.loadFence(); // SBITS = ~(2^7 - 1) return (stamp &amp; SBITS) == (state &amp; SBITS); &#125; 如上，tryOptimisticRead并不会去尝试获取读锁(即不会更改state)，而是通过validate验证写锁状态是否在这期间改变过，如果未改变，则可以认为可以共享读锁，否则(即写线程操作过数据)需要实际去获取读锁。 总结有以下几点值得我们注意一下： 读写锁状态在一个volatile long型变量上表示，低7位表示读锁状态，溢出的读锁状态用readerOverflow表示，高25位表示写锁，并且写锁是序列递增的； 申请锁时，使用了大量自旋操作。如果是在队列头部位置的等待线程节点，会一致自旋下去。否则当前线程节点会入队进行阻塞等待，虽然读和写具体的入队方式可能有点差别。通过这样的方式，不仅避免了饥饿现象，一定程度行还体现了公平性； 读锁是共享的，写锁是互斥的。此外读锁还有种乐观读的方式，即尝试获取锁时，并不会更改当前读锁状态，而是通过验证期间写锁状态是否被更改的方式保证数据一致。此外还有一个读锁升级的功能，这是跟ReentrantReadWriteLock的区别。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK源码阅读之ReentrantReadWriteLock]]></title>
    <url>%2F4ba3385f.html</url>
    <content type="text"><![CDATA[写在前面 作者Doug Lea_如此描述这个类：An implementation of {@link java.util.concurrent.locks.ReadWriteLock} supporting similar semantics to {@link java.util.concurrent.locks.ReentrantLock}.ReentratReadWriteLock是一个可重入的读写锁，实现了ReadWriteLock接口，具有与ReentrantLock同样的语义。此外，ReentrantReadWriteLock只支持_65535个可重入写锁和65535个读锁，以及其他一些特性，如下示例中的特性。 接着看一下使用示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// 锁降级，write -&gt; readclass CachedData &#123; Object data; volatile boolean cacheValid; final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); void processCachedData() &#123; rwl.readLock().lock(); if (!cacheValid) &#123; // Must release read lock before acquiring write lock rwl.readLock().unlock(); rwl.writeLock().lock(); try &#123; // Recheck state because another thread might have // acquired write lock and changed state before we did. if (!cacheValid) &#123; data = ... cacheValid = true; &#125; // Downgrade by acquiring read lock before releasing write lock rwl.readLock().lock(); &#125; finally &#123; rwl.writeLock().unlock(); // Unlock write, still hold read &#125; &#125; try &#123; use(data); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125;&#125;// for a large collection,and concurrently accessed.class RWDictionary &#123; private final Map&lt;String, Data&gt; m = new TreeMap&lt;String, Data&gt;(); private final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); private final Lock r = rwl.readLock(); private final Lock w = rwl.writeLock(); public Data get(String key) &#123; r.lock(); try &#123; return m.get(key); &#125; finally &#123; r.unlock(); &#125; &#125; public String[] allKeys() &#123; r.lock(); try &#123; return m.keySet().toArray(); &#125; finally &#123; r.unlock(); &#125; &#125; public Data put(String key, Data value) &#123; w.lock(); try &#123; return m.put(key, value); &#125; finally &#123; w.unlock(); &#125; &#125; public void clear() &#123; w.lock(); try &#123; m.clear(); &#125; finally &#123; w.unlock(); &#125; &#125;&#125; 以上示例就是ReentrantReadWriteLock的基础用法了。另外的一些用法即特性，如WriteLock也有newCondition的api，写锁降级，可重入，写锁线程能获取读锁但反过来却不行，等等特性。 内部变量内部共有三个变量，实现java.util.concurrent.locks.Lock接口的ReadLock和WriteLock；以及继承自AQS的抽象类Sync实例，FairSync和NonfairSync继承自Sync，表示该锁具备公平/非公平语义，有一个带boolean参数的构造函数，根据这个boolean参数决定sync为哪个子类实例。在展开加解锁流程前，先看一下上述提起的内部类。 Sync内部类Sync继承自AQS，主要功能是通过这个类提供： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319abstract static class Sync extends AbstractQueuedSynchronizer &#123; // 高16位才表示共享锁个数，即读锁个数，低位表示写锁 static final int SHARED_SHIFT = 16; // 一个SHARED_UNIT表示一个读锁，加读锁即：state+=SHARED_UNIT static final int SHARED_UNIT = (1 &lt;&lt; SHARED_SHIFT); // 最大的读锁个数 static final int MAX_COUNT = (1 &lt;&lt; SHARED_SHIFT) - 1; // 互斥锁掩码，即写锁掩码 static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1; /** Returns the number of shared holds represented in count */ // 读锁个数，无符号右移，高位值 static int sharedCount(int c) &#123; return c &gt;&gt;&gt; SHARED_SHIFT; &#125; /** Returns the number of exclusive holds represented in count */ // 写锁个数,低位值 static int exclusiveCount(int c) &#123; return c &amp; EXCLUSIVE_MASK; &#125; /** * A counter for per-thread read hold counts. * Maintained as a ThreadLocal; cached in cachedHoldCounter */ // 线程持有锁个数 static final class HoldCounter &#123; int count = 0; // Use id, not reference, to avoid garbage retention final long tid = getThreadId(Thread.currentThread()); &#125; /** * ThreadLocal subclass. Easiest to explicitly define for sake * of deserialization mechanics. */ // 注意是继承自TheadLocal,各个线程维护各自的holdCounter static final class ThreadLocalHoldCounter extends ThreadLocal&lt;HoldCounter&gt; &#123; public HoldCounter initialValue() &#123; return new HoldCounter(); &#125; &#125; private transient ThreadLocalHoldCounter readHolds; // 缓存上一线程的holder，便于release,大概率会节省ThreadLocal的检索次数 // 因为一般情况下是，上一线程release，下一线程acquire private transient HoldCounter cachedHoldCounter; // private transient Thread firstReader = null; private transient int firstReaderHoldCount; Sync() &#123; readHolds = new ThreadLocalHoldCounter(); setState(getState()); // ensures visibility of readHolds &#125; abstract boolean readerShouldBlock(); abstract boolean writerShouldBlock(); protected final boolean tryRelease(int releases) &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; boolean free = exclusiveCount(nextc) == 0; if (free) // 所有写锁已经完全释放 setExclusiveOwnerThread(null); setState(nextc); return free; &#125; protected final boolean tryAcquire(int acquires) &#123; /* * Walkthrough: * 1. If read count nonzero or write count nonzero * and owner is a different thread, fail. * 2. If count would saturate, fail. (This can only * happen if count is already nonzero.) * 3. Otherwise, this thread is eligible for lock if * it is either a reentrant acquire or * queue policy allows it. If so, update state * and set owner. */ Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) &#123; // (Note: if c != 0 and w == 0 then shared count != 0) if (w == 0 || current != getExclusiveOwnerThread()) // w == 0说明已经有读锁存在了 // current != getExclusiveOwnerThread() 说明写锁已经被其他线程持有 // 以上两种情况都会失败 return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) // 超过了最大锁限制 throw new Error("Maximum lock count exceeded"); // Reentrant acquire // 可重入 setState(c + acquires); return true; &#125; // 子类(FairSync or NonfairSync)实现writerShouldBlock if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; // 设置当前线程为owner setExclusiveOwnerThread(current); return true; &#125; protected final boolean tryReleaseShared(int unused) &#123; Thread current = Thread.currentThread(); if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; if (firstReaderHoldCount == 1) // 已经全部释放 firstReader = null; else firstReaderHoldCount--; &#125; else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; if (count &lt;= 1) &#123; // 该线程不再持有读锁，那么需要清理ThreadLocal readHolds.remove(); if (count &lt;= 0) throw unmatchedUnlockException(); &#125; // count 减1 --rh.count; &#125; for (;;) &#123; int c = getState(); int nextc = c - SHARED_UNIT; // cas state -= SHARED_UNIT if (compareAndSetState(c, nextc)) // Releasing the read lock has no effect on readers, // but it may allow waiting writers to proceed if // both read and write locks are now free. // 如果读并发过大，这里会始终return false,最终造成写线程饥饿 return nextc == 0; &#125; &#125; protected final int tryAcquireShared(int unused) &#123; /* * Walkthrough: * 1. If write lock held by another thread, fail. * 2. Otherwise, this thread is eligible for * lock wrt state, so ask if it should block * because of queue policy. If not, try * to grant by CASing state and updating count. * Note that step does not check for reentrant * acquires, which is postponed to full version * to avoid having to check hold count in * the more typical non-reentrant case. * 3. If step 2 fails either because thread * apparently not eligible or CAS fails or count * saturated, chain to version with full retry loop. */ Thread current = Thread.currentThread(); int c = getState(); if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) // 写锁已经被其他线程持有 // 如果被当前线程持有，则往下接着执行 return -1; int r = sharedCount(c); // readerShouldBlock有Sync子类(FairSync or NonfairSync)实现 // 注意这里，公平锁的时候，读写锁都是公平的，先来后到 if (!readerShouldBlock() &amp;&amp; r &lt; MAX_COUNT &amp;&amp; // cas state += SHARED_UNIT compareAndSetState(c, c + SHARED_UNIT)) &#123; if (r == 0) &#123; // 初始获取读锁 firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; &#125; return 1; &#125; // 如果cas失败 return fullTryAcquireShared(current); &#125; final int fullTryAcquireShared(Thread current) &#123; /* * This code is in part redundant with that in * tryAcquireShared but is simpler overall by not * complicating tryAcquireShared with interactions between * retries and lazily reading hold counts. */ HoldCounter rh = null; for (;;) &#123; int c = getState(); if (exclusiveCount(c) != 0) &#123; // 写锁已经被持有 if (getExclusiveOwnerThread() != current) // 并不是当前线程持有写锁 return -1; // 否则那么是当前线程持有写锁，往下接着执行 // else we hold the exclusive lock; blocking here // would cause deadlock. &#125; else if (readerShouldBlock()) &#123; // 队列里已经有其他线程节点，具体情况，取决于是Fair 还是 NonFair // Make sure we're not acquiring read lock reentrantly if (firstReader == current) &#123; // 相当于重入 // assert firstReaderHoldCount &gt; 0; &#125; else &#123; if (rh == null) &#123; rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) &#123; rh = readHolds.get(); // 不再持有读锁的线程节点，需要清理ThreadLocal if (rh.count == 0) readHolds.remove(); &#125; &#125; // 已经清理了，那么需要重新入队 // 否则向下执行(重入) if (rh.count == 0) return -1; &#125; &#125; if (sharedCount(c) == MAX_COUNT) // 超过了最大读锁限制 throw new Error("Maximum lock count exceeded"); // cas state += SHARED_UNIT if (compareAndSetState(c, c + SHARED_UNIT)) &#123; // 未有线程持有读锁 if (sharedCount(c) == 0) &#123; firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; cachedHoldCounter = rh; // cache for release &#125; return 1; &#125; &#125; &#125; // ... 省略其他代码&#125;static final class NonfairSync extends Sync &#123; private static final long serialVersionUID = -8159625535654395037L; final boolean writerShouldBlock() &#123; // 抢占式 return false; // writers can always barge &#125; final boolean readerShouldBlock() &#123; /* As a heuristic to avoid indefinite writer starvation, * block if the thread that momentarily appears to be head * of queue, if one exists, is a waiting writer. This is * only a probabilistic effect since a new reader will not * block if there is a waiting writer behind other enabled * readers that have not yet drained from the queue. */ // from AQS return apparentlyFirstQueuedIsExclusive(); &#125;&#125;// from AQS 当前queue下一节点是否互斥节点final boolean apparentlyFirstQueuedIsExclusive() &#123; Node h, s; return (h = head) != null &amp;&amp; (s = h.next) != null &amp;&amp; !s.isShared() &amp;&amp; s.thread != null;&#125;static final class FairSync extends Sync &#123; private static final long serialVersionUID = -2274990926593161451L; final boolean writerShouldBlock() &#123; // from AQS，解析见下 return hasQueuedPredecessors(); &#125; final boolean readerShouldBlock() &#123; return hasQueuedPredecessors(); &#125;&#125; // from AQS // 是否有前驱节点 public final boolean hasQueuedPredecessors() &#123; // The correctness of this depends on head being initialized // before tail and on head.next being accurate if the current // thread is first in queue. Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t &amp;&amp; // h.next 为null，可以理解为有前驱，因为head为上一刚刚释放锁的最后节点 // 其实也可以理解为目前队列里没有节点 ((s = h.next) == null || s.thread != Thread.currentThread()); &#125; 同样，也是利用AQS的state变量来标识锁的个数，不同的是，ReentrantReadWriteLock利用高16位表示读锁，低16位表示写锁。两个抽象方法writerShouldBlock和ReaderShouldBlock，子类FairSync和NonfairSync通过实现这两个方法来提供公平/非公平锁的功能。 ReadLock实现java.util.concurrent.locks.Lock接口： 12345678910111213141516171819202122232425262728293031323334353637383940public static class ReadLock implements java.util.concurrent.locks.Lock, java.io.Serializable &#123; private final Sync sync; protected ReadLock(ReentrantReadWriteLock lock) &#123; sync = lock.sync; &#125; /** * Acquires the read lock. * * &lt;p&gt;Acquires the read lock if the write lock is not held by * another thread and returns immediately. * * &lt;p&gt;If the write lock is held by another thread then * the current thread becomes disabled for thread scheduling * purposes and lies dormant until the read lock has been acquired. */ public void lock() &#123; // 见上Sync的该方法的解析 sync.acquireShared(1); &#125; /** * Attempts to release this lock. * * &lt;p&gt;If the number of readers is now zero then the lock * is made available for write lock attempts. */ public void unlock() &#123; // 见上Sync的该方法的解析 sync.releaseShared(1); &#125; public java.util.concurrent.locks.Condition newCondition() &#123; throw new UnsupportedOperationException(); &#125; // 省略其他代码，有兴趣可以自行研读源码 // like lockInterruptibly tryLock tryLock(long timeout, TimeUnit unit)&#125; ReadLock是不支持newCondition这一api的。 WriteLock同样，WriteLock也是实现了java.util.concurrent.locks.Lock接口: 1234567891011121314151617181920212223public static class WriteLock implements java.util.concurrent.locks.Lock, java.io.Serializable &#123; private final Sync sync; protected WriteLock(ReentrantReadWriteLock lock) &#123; sync = lock.sync; &#125; public void lock() &#123; // 见上Sync解析 sync.acquire(1); &#125; public void unlock() &#123; // 见上Sync解析 sync.release(1); &#125; public java.util.concurrent.locks.Condition newCondition() &#123; return sync.newCondition(); &#125; // 省略其他代码&#125; WriteLock是支持newCondition api的，这一api是构造一个AQS的内部类实例，具体可以看我之前的文章。 WriteLock.lock先看一下写锁的加锁流程。WriteLock.lock()调用内部类Sync的acquire(1)方法，acquire方法AQS提供的一个方法： 1234567891011121314151617181920212223242526272829303132333435 // from AQSpublic final void acquire(int arg) &#123; // tryAcquire from Sync 如果已经有读锁或者写锁已经被其他线程持有，则返回false if (!tryAcquire(arg) &amp;&amp; // 尝试获取写锁失败，则入队一个EXCLUSIVE互斥节点 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125;// from AQS final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); // tryAcquire from Sync,解析见上 if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 设置当前节点为头结点 setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; // 更改node的status为SIGNAL if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // park的地方 parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; 可见写锁加锁过程就是对state变量进行操作的过程，公平/非公平锁主要是通过writerShouldBlock这个方法，非公平这个方法直接返回false，也就是抢占式地去获取锁，而公平则是会查看队列里是否有前驱，如有则失败。 WriteLock.unlock接着是写锁的解锁过程。同样，也是调用Sync内部方法release(1),release也为AQS的一个方法： 123456789101112public final boolean release(int arg) &#123; // tryRelease from Sync,见上解析 if (tryRelease(arg)) &#123; Node h = head; // 在acquire的阻塞位置已经将waitStatus更新为SIGNAL了 if (h != null &amp;&amp; h.waitStatus != 0) // 这个方法其实真正唤醒的是h.next unparkSuccessor(h); return true; &#125; return false;&#125; ReadLock.lock方法内部调用的是AQS的acquireShared(1)方法： 1234567891011121314151617181920212223242526272829303132333435363738394041 public final void acquireShared(int arg) &#123; // tryAcquireShared见上Sync解析 if (tryAcquireShared(arg) &lt; 0) // 见下 doAcquireShared(arg); &#125;// from AQS private void doAcquireShared(int arg) &#123; // 在队列尾部添加一SHARED节点 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; // 见上Sync解析 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 设置当前节点为头头结点 // 并唤醒下一节点 setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; // 更新前驱节点waitStatus为SIGNAL if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // park parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; 结合Sync的解析，读锁的获取过程就是对state这一变量的操作过程。解析还是很清晰的，具体过程可以看一下解析。 ReadLock.unlockunlock调用的仍是AQS内部方法，releaseShared(1): 12345678910111213141516171819202122232425262728293031323334353637383940414243 public final boolean releaseShared(int arg) &#123; // tryReleaseShared from Sync,见上解析 // 如果读线程并发大，那么tryReleaseShared总会返回false，则始终不会执行到doReleaseShared，则造成写线程饥饿 if (tryReleaseShared(arg)) &#123; // 见下 doReleaseShared(); return true; &#125; return false; &#125;// from AQS private void doReleaseShared() &#123; /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; // 阻塞前已经将waitStatus更新为SIGNAL if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 唤醒h.next unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125; &#125; 可见，解锁读锁，是对state操作，state-=_SHARED_UNIT，_接着再查看队列是否有等待节点，如有则需唤醒。 总结 state内部变量的高16位表示所有线程持有读锁个数，低16位表示持有写锁个数 读写锁都为可重入的 写锁为互斥锁，同时持有线程持有读锁。加解锁过程均为cas操作state变量。加锁过程state+=1,解锁过程state-=1。加锁时如果其它线程持有锁，则往队列里添加一个EXCLUSIVE节点表示当前线程等待写锁，并在当前位置park住等待唤醒，解锁时，如果队列里有等待节点则需要唤醒节点对应线程 读锁为共享锁，多个线程能同时获得读锁。加锁过程即为state+=SHARED_UNIT(2^16)。加解锁过程是cas操作state变量高16位。加锁过程为state+=2^16,解锁过程state-=2^16。加锁时如果其它线程持有写锁，则往队列里添加一个SHARED节点标识当前线程等待读锁，并在当前位置park住等待唤醒，解锁时如果队列里有等待节点则需唤醒对应节点线程 能看到，无论Fari还是Nonfair，写锁的获取都有可能因为读锁阻塞，在一定情况下，造成了获取写锁的线程饥饿的现象]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK源码阅读之ReentrantLok]]></title>
    <url>%2Fb484f60f.html</url>
    <content type="text"><![CDATA[写在前面作者Doug Lea_如此描述这个类：_A reentrant mutual exclusion {@link **java.util.concurrent.locks.Lock} with the same basic behavior and semantics as the implicit monitor lock accessed using {@code **synchronized} methods and statements, but with extended capabilities.ReentrantLock是继承_java.util.concurrent.locks.Lock_的可重入互斥锁，它具有跟隐式监视器锁(Synchronized)同样语义，用于锁定一个方法或代码块，除此之外，它还有一些额外的功能。ReentrantLock锁，只能被一个线程持有，如果该线程持有的同时尝试去获取该ReentrantLock，会立即返回。当一个线程获取ReentrantLock，即调用lock时，只有当该锁未被其它线程持有时才能成功。看一下源码中的示例： 12345678910111213class X &#123; private final ReentrantLock lock = new ReentrantLock(); // ... public void m() &#123; lock.lock(); // block until condition holds try &#123; // ... method body &#125; finally &#123; lock.unlock() &#125; &#125;&#125; 另外，还有个lock.newCondition api可以使用，看下面用法： 12345678910111213141516171819202122232425public class Y &#123; private final ReentrantLock lock = new ReentrantLock(); private final Condition condition = lock.newCondition(); public void waitOn() &#123; lock.lock(); try &#123; condition.await(); &#125; catch (InterruptedException e) &#123; // ... &#125; finally &#123; lock.unlock(); &#125; &#125; public void signal() &#123; lock.lock(); try&#123; condition.signal(); // or condition.signalAll() &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 注意，condition使用需在持有lock时。 ReentrantLock.lock还是结合示例，看看整个流程是怎么串起来的。有如下两个构造函数： 1234567public ReentrantLock() &#123; sync = new NonfairSync();&#125;public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 构造时通过传入一个boolean参数，可以实例化一个公平Lock。而这里的FairSync和NonfairSync是继承自内部类Sync，而Sync则继承自AQS。如下代码：Sync类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495abstract static class Sync extends AbstractQueuedSynchronizer &#123; private static final long serialVersionUID = -5179523762034025860L; /** * Performs &#123;@link java.util.concurrent.locks.Lock#lock&#125;. The main reason for subclassing * is to allow fast path for nonfair version. */ // 子类实现这个方法，以提供公平/非公平锁的功能 abstract void lock(); /** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ // 非公平地尝试获取锁 final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); // 利用AQS中的state变量表示锁的获取次数 int c = getState(); if (c == 0) &#123; // 目前所无持有者 if (compareAndSetState(0, acquires)) &#123; // 设置当前owner线程 // setExclusiveOwnerThread方法为AbstractOwnableSynchronizer类的方法 // 该类就一个私有变量Thread exclusiveOwnerThread // 用于表示互斥资源的互斥持有线程 setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; // 当前线程为锁的持有线程，即可重入 int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); // 直接设置state setState(nextc); return true; &#125; // 否则获取失败 return false; &#125; // 尝试释放 protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; // 只有state==0时，才能认为锁可以被释放 free = true; // 设置owner线程为null setExclusiveOwnerThread(null); &#125; // 设置state setState(c); return free; &#125; protected final boolean isHeldExclusively() &#123; // While we must in general read state before owner, // we don't need to do so to check if current thread is owner return getExclusiveOwnerThread() == Thread.currentThread(); &#125; // for api ReentranLock.newCondition final ConditionObject newCondition() &#123; return new ConditionObject(); &#125; // Methods relayed from outer class final Thread getOwner() &#123; return getState() == 0 ? null : getExclusiveOwnerThread(); &#125; final int getHoldCount() &#123; return isHeldExclusively() ? getState() : 0; &#125; final boolean isLocked() &#123; // 锁是否被持有，即需判断state是否等于0 return getState() != 0; &#125; /** * Reconstitutes the instance from a stream (that is, deserializes it). */ private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; s.defaultReadObject(); setState(0); // reset to unlocked state &#125;&#125; 内部抽象类Sync继承Sync，实现了一些基础功能，内部有个抽象方法lock。子类实现这个方法可提供公平/非公平锁的功能。NonFairSync类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778 static final class NonfairSync extends Sync &#123; private static final long serialVersionUID = 7316153563782823691L; /** * Performs lock. Try immediate barge, backing up to normal * acquire on failure. */ final void lock() &#123; // cas 操作state if (compareAndSetState(0, 1)) // 获取成功，设置当前线程为owner线程 setExclusiveOwnerThread(Thread.currentThread()); else // state 不等于0， 锁已经被其他线程持有 // acquire为AQS内方法，如下解析 acquire(1); &#125; protected final boolean tryAcquire(int acquires) &#123; // non fair模式，是直接调用Sync类的nonfairTryAcquire方法，如上Sync类的解析 return nonfairTryAcquire(acquires); &#125; &#125;// from AQS public final void acquire(int arg) &#123; // tryAcquire为FairSync/NonfairSync重写方法 if (!tryAcquire(arg) &amp;&amp; // 构造一个node入链表 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125;// from AQS// 给定mode构建node节点，入链表，返回当前节点 private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node; &#125;// from AQS// 获取锁 final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); // 前驱为头结点 // tryAcquire为FairSync/NonfairSync重写方法 if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 已获取，设置当前node为head // 唤醒总是唤醒头结点的下一节点线程 setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // 这里阻塞 LockSupport.park parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; FairSync类 123456789101112131415161718192021222324252627282930313233343536static final class FairSync extends Sync &#123; private static final long serialVersionUID = -3000897897090466540L; final void lock() &#123; // 与Non fair的区别 // 调用AQS的acquire // 即先判断锁是否被持有，有没有前驱，如果持有是不是当前线程 // 然后再根据判断情况构造节点加入链表尾部，并阻塞。 acquire(1); &#125; /** * Fair version of tryAcquire. Don't grant access unless * recursive call or no waiters or is first. */ protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 与non fair 区别：是否有前驱，如有则获取失败 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125;&#125; 以上代码分析，非公平锁，通过cas操作state去获取锁，即0-&gt;1，如果获取失败，则再次尝试(cas操作state，持有者是否为当前线程)，如果仍然获取失败，则构造一个node并接入链表尾部，并阻塞当前线程直到被唤醒。公平锁则不会直接去获取锁，而是在非公平锁基础上，会先查看链表是否有前驱，有则阻塞并构造新节点加入链表尾部。辅助下面的图看源码可能会有帮助。非公平锁：公平锁： ReentrantLock.unlock释放锁的逻辑相对获取锁要简短很多。unlock方法就是调用AQS的release方法： 1234567891011121314151617public final boolean release(int arg) &#123; // 尝试释放锁 Sync实现tryRelease,见如上Sync类解析 // 即state减arg,如果state减为0，则释放锁（owner线程置null） // 另外，这里AQS的state表示持有锁的次数 if (tryRelease(arg)) &#123; // 释放成功 Node h = head; // 头结点不为空，说明当前线程入过链表，所以并且头结点的状态应该是SIGNAL // shouldParkAfterFailedAcquire这里修改node的状态 if (h != null &amp;&amp; h.waitStatus != 0) // 唤醒下一节点线程，还记得获取锁的时候，线程节点入链表之后阻塞的位置是哪里吗？ // 在AQS里的acquireQueued这个方法 unparkSuccessor(h); return true; &#125; return false;&#125; ReentrantLock.newCondition接下来看一下newCondition这个api的内部流程是什么样的。ReentrantLock.newCondition方法内部调用Sync类实现的newCondition方法，而这个方法是实例化一个AQS的ConditionObject类对象: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143final ConditionObject newCondition() &#123; return new ConditionObject();&#125; public class ConditionObject implements Condition, java.io.Serializable &#123; /** First node of condition queue. */ private transient Node firstWaiter; /** Last node of condition queue. */ private transient Node lastWaiter; public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 添加一个condition节点，见下面该方法解析 Node node = addConditionWaiter(); // 释放所有获取次数记录，置state为0，见下面该方法解析 // 注意这里，调用condition.await时，是在lock块内 int savedState = fullyRelease(node); int interruptMode = 0; // isOnSyncQueue判断是否在Sync队列里，见下面该方法解析 while (!isOnSyncQueue(node)) &#123; // 阻塞在这里，直到signal唤醒 // signal唤醒后，isOnSyncQueue返回true，调出循环 LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // acquireQueued见上面Sync对应方法解析，方法里的这里的queue指Sync的队列 // 这个队列会在signal唤醒时构造 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); &#125; private Node addConditionWaiter() &#123; Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; unlinkCancelledWaiters(); t = lastWaiter; &#125; Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node; &#125; // 删除一些取消的节点 private void unlinkCancelledWaiters() &#123; Node t = firstWaiter; Node trail = null; while (t != null) &#123; Node next = t.nextWaiter; if (t.waitStatus != Node.CONDITION) &#123; t.nextWaiter = null; if (trail == null) firstWaiter = next; else trail.nextWaiter = next; if (next == null) lastWaiter = trail; &#125; else // 上一节点 trail = t; t = next; &#125; &#125; public final void signal() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first); &#125; public final void signalAll() &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignalAll(first); &#125; // ... 省略其他代码 &#125;// from AQS final int fullyRelease(Node node) &#123; boolean failed = true; try &#123; int savedState = getState(); // 置state为0 if (release(savedState)) &#123; failed = false; return savedState; &#125; else &#123; throw new IllegalMonitorStateException(); &#125; &#125; finally &#123; if (failed) node.waitStatus = Node.CANCELLED; &#125; &#125; public final boolean release(int arg) &#123; // 尝试释放，置state -= arg，结合上下文，也就是state=0 if (tryRelease(arg)) &#123; Node h = head; // 这里h为null，waitState为CONDITION // 因为condition的api使用要求都在lock块内，所以h必定为null if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false; &#125;// node是否在Sync等待队列里(也就是调用了condition.signal，将waitStatus置为SIGNAL)// 根据waitStatus以及队列元素比较 final boolean isOnSyncQueue(Node node) &#123; if (node.waitStatus == Node.CONDITION || node.prev == null) return false; if (node.next != null) // If has successor, it must be on queue return true; /* * node.prev can be non-null, but not yet on queue because * the CAS to place it on queue can fail. So we have to * traverse from tail to make sure it actually made it. It * will always be near the tail in calls to this method, and * unless the CAS failed (which is unlikely), it will be * there, so we hardly ever traverse much. */ return findNodeFromTail(node); &#125; await方法的比较复杂，需要仔细梳理下，并且结合signal的流程才能清晰起来。以下为signal的流程： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class ConditionObject implements Condition, java.io.Serializable &#123; public final void signal() &#123; // 如果本线程不是持有锁线程，则throw if (!isHeldExclusively()) throw new IllegalMonitorStateException(); // 还记得吗，firstWaiter为condition队列里的头结点 Node first = firstWaiter; if (first != null) // 唤醒，结合上边await的流程看 // 就是改变waitStatus，以及入Sync队列 doSignal(first); &#125; private void doSignal(Node first) &#123; do &#123; if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; // transferForSIgnal改变first状态，并入Sync队列 &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null); &#125;&#125;// from AQS final boolean transferForSignal(Node node) &#123; /* * If cannot change waitStatus, the node has been cancelled. */ if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; /* * Splice onto queue and try to set waitStatus of predecessor to * indicate that thread is (probably) waiting. If cancelled or * attempt to set waitStatus fails, wake up to resync (in which * case the waitStatus can be transiently and harmlessly wrong). */ // 入队，Sync的队列 Node p = enq(node); int ws = p.waitStatus; // 将waitStatus置为SIGNAL if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) LockSupport.unpark(node.thread); return true; &#125; signal流程大致就是这样，signalAll其实就是从Condition头结点firstWaiter开始依次调用transferForSignal方法，大同小异。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK源码阅读之Semaphore]]></title>
    <url>%2F61e94ea2.html</url>
    <content type="text"><![CDATA[写在前面作者_Doug Lea_如此描述这个类：A counting semaphore. Conceptually, a semaphore maintains a set of permits.顾名思义。计数信号量，它维护许可数量。acquire一个许可阻塞至池里有可用许可，release一个许可即往池里添加一个许可。如下为源码中示例代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243class Pool &#123; private static final int MAX_AVAILABLE = 100; private final Semaphore available = new Semaphore(MAX_AVAILABLE, true); public Object getItem() throws InterruptedException &#123; available.acquire(); return getNextAvailableItem(); &#125; public void putItem(Object x) &#123; if (markAsUnused(x)) available.release(); &#125; // Not a particularly efficient data structure; just for demo protected Object[] items = ... whatever kinds of items being managed protected boolean[] used = new boolean[MAX_AVAILABLE]; protected synchronized Object getNextAvailableItem() &#123; for (int i = 0; i &lt; MAX_AVAILABLE; ++i) &#123; if (!used[i]) &#123; used[i] = true; return items[i]; &#125; &#125; return null; // not reached &#125; protected synchronized boolean markAsUnused(Object item) &#123; for (int i = 0; i &lt; MAX_AVAILABLE; ++i) &#123; if (item == items[i]) &#123; if (used[i]) &#123; used[i] = false; return true; &#125; else return false; &#125; &#125; return false; &#125; &#125;&#125; 如上，示例中用到的api就两个，即acquire和release，意为获取一个许可及释放一个许可。 Sync变量内部抽象类Sync继承自AQS，用AQS中的volatile int state变量表示许可数量。Sync的子类有两个版本，fair和nonfair。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990abstract static class Sync extends AbstractQueuedSynchronizer &#123; private static final long serialVersionUID = 1192457210091910933L; Sync(int permits) &#123; // state 表示当前许可数量 setState(permits); &#125; final int getPermits() &#123; return getState(); &#125; // 非公平式获取许可，cas操作，state减去acquires final int nonfairTryAcquireShared(int acquires) &#123; for (;;) &#123; int available = getState(); int remaining = available - acquires; // 注意这里remaining &lt; 0 if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125; &#125; // 释放许可就是state + releases protected final boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int current = getState(); int next = current + releases; if (next &lt; current) // overflow throw new Error("Maximum permit count exceeded"); if (compareAndSetState(current, next)) return true; &#125; &#125; final void reducePermits(int reductions) &#123; for (;;) &#123; int current = getState(); int next = current - reductions; if (next &gt; current) // underflow throw new Error("Permit count underflow"); if (compareAndSetState(current, next)) return; &#125; &#125; final int drainPermits() &#123; for (;;) &#123; int current = getState(); if (current == 0 || compareAndSetState(current, 0)) return current; &#125; &#125;&#125;static final class NonfairSync extends Sync &#123; private static final long serialVersionUID = -2694183684443567898L; NonfairSync(int permits) &#123; super(permits); &#125; protected int tryAcquireShared(int acquires) &#123; // 非公平式获取许可，调用父类（Sync）的nonfairTryAcquireShared return nonfairTryAcquireShared(acquires); &#125;&#125;static final class FairSync extends Sync &#123; private static final long serialVersionUID = 2014338818796000944L; FairSync(int permits) &#123; super(permits); &#125; protected int tryAcquireShared(int acquires) &#123; for (;;) &#123; // 先查看有没有前驱在阻塞等着获取许可，如果有，当前线程获取失败 // 这就是跟非公平的区别 if (hasQueuedPredecessors()) return -1; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125; &#125;&#125; Sync的两个子类，NonfairSync和FairSync，分别表示在获取许可时是非公平式（抢占式）和公平式。 Semaphore.acquire获取许可，调用Sync.acquireSharedInterruptibly(1)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 调用内部实现类tryAcquireShared if (tryAcquireShared(arg) &lt; 0) // 池中许可数量小于0，即state&lt;0 doAcquireSharedInterruptibly(arg);&#125;private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; // 在链表尾部添加一个node表示当前阻塞的节点 // 注意头结点为一个标识节点，如下addWaiter方法 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; // 前驱 final Node p = node.predecessor(); if (p == head) &#123; // 调用内部类（fair or nonfair）实现tryAcquireShared，获取许可 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 许可数量恢复&gt;0，设置当前节点为头节点并且唤醒下一节点 setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; &#125; &#125; // 没有许可可获取，阻塞在这里，等待唤醒 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125;private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; // 链表中没有前驱 enq(node); return node;&#125;private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) // 先要设置一个"空"头结点 tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 需要注意，当没有阻塞的节点时，即链表为空，这时往链表添加节点时是往一个”空”头节点后添加。唤醒时，在阻塞位置恢复再次循环，如果前驱是头结点且当前池中有许可，那么设置当前节点为头结点，并唤醒下一节点，否则再次阻塞。 Semaphore.release调用Sync.releaseShared(1)。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public final boolean releaseShared(int arg) &#123; // sync中的tryReleaseShared, state += arg if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125;private void doReleaseShared() &#123; /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) &#123; Node h = head; // 如果有阻塞的节点 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // 阻塞时，在shouldParkAfterFailedAcquire这个方法里，将node的前驱已经设置为SIGNAL if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 唤醒h的下一节点，如下unparkSuccessor分析 unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125;private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; // 从后往前找，去掉已经cancel的节点，见AQS类waitStatus的可取类型 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) // 唤醒node的下一可用节点 LockSupport.unpark(s.thread);&#125; 唤醒时总是唤醒头结点的下一节点。注意waitStatus这个状态，在阻塞时，会在shouldParkAfterFailedAcquire这个方法里，将当前阻塞节点的前缀设置为SIGNAL。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK源码阅读之CyclicBarrier]]></title>
    <url>%2F97e68def.html</url>
    <content type="text"><![CDATA[写在前面_作者Doug Lea_如此描述这个类：A synchronization aid that allows a set of threads to all wait for each other to reach a common barrier point.这也是一个多线程协调的辅助工具类。barrier可翻译为栅栏，顾名思义，这个类控制先到的线程则在”栅栏”处等待其他线程，直到所有线程都到达，再接着往下执行。此外，CyclicBarrier如其名，是循环可复用的。如下是源码中给出的示例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solver &#123; final int N; final float[][] data; final CyclicBarrier barrier; class Worker implements Runnable &#123; int myRow; Worker(int row) &#123; myRow = row; &#125; public void run() &#123; while (!done()) &#123; processRow(myRow); try &#123; // 等待一行处理结束 barrier.await(); &#125; catch (InterruptedException ex) &#123; return; &#125; catch (BrokenBarrierException ex) &#123; return; &#125; &#125; &#125; &#125; public Solver(float[][] matrix) &#123; data = matrix; N = matrix.length; // 所有行处理完，再merge Runnable barrierAction = new Runnable() &#123; public void run() &#123; mergeRows(...); &#125;&#125;; barrier = new CyclicBarrier(N, barrierAction); List&lt;Thread&gt; threads = new ArrayList&lt;Thread&gt;(N); for (int i = 0; i &lt; N; i++) &#123; Thread thread = new Thread(new Worker(i)); threads.add(thread); thread.start(); &#125; // wait until done for (Thread thread : threads) thread.join(); &#125; &#125;&#125; 如以上示例，并行处理每行矩阵元素，待所有行处理结束再对每行处理结果进行合并。 内部变量// 可重入锁控制对barrier的访问private final ReentrantLock lock = new ReentrantLock();// 控制线程阻塞，直到所有线程”到达”private final Condition trip = lock.newCondition();// 多少个参与方(线程)private final int parties;// 到达栅栏后执行的线程private final Runnable barrierCommand;// 内部类表示目前是哪一代private Generation generation = new Generation()// 还有几个参与方(线程)在未到达private int count; Generation为内部类，当触发栅栏或者重置，generation就会改变。 1234private static class Generation &#123; // 标识栅栏有没有被"踢翻" boolean broken = false;&#125; # CyclicBarrier.await 由源码中的示例代码，await是CyclicBarrier的主要起作用的方法。首先先看一下构造方法中对内部变量的初始化： 1234567public CyclicBarrier(int parties, Runnable barrierAction) &#123; if (parties &lt;= 0) throw new IllegalArgumentException(); this.parties = parties; // 初始parties个线程 this.count = parties; this.barrierCommand = barrierAction; &#125; 构造方法里就初始化了三个变量，分别是表示多少个线程的parties、还有多少个线程未到达的count、后置线程barrierCommand。await方法是调用内部私有方法dowait: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException &#123; final ReentrantLock lock = this.lock; // 加锁 lock.lock(); try &#123; // 当前代 final Generation g = generation; if (g.broken) // 抱歉，栅栏已经被踢翻了 throw new BrokenBarrierException(); if (Thread.interrupted()) &#123; // 线程中断了，需要唤醒所有线程 breakBarrier(); throw new InterruptedException(); &#125; int index = --count; if (index == 0) &#123; // tripped // 所有线程已到达 boolean ranAction = false; try &#123; final Runnable command = barrierCommand; if (command != null) // 后置线程 command.run(); ranAction = true; // 重置状态并唤醒所有线程 nextGeneration(); return 0; &#125; finally &#123; if (!ranAction) breakBarrier(); &#125; &#125; // loop until tripped, broken, interrupted, or timed out for (;;) &#123; try &#123; if (!timed) // condition.await 调用AQS里的Condition实现类 trip.await(); else if (nanos &gt; 0L) // 允许只等待一定时间 nanos = trip.awaitNanos(nanos); &#125; catch (InterruptedException ie) &#123; if (g == generation &amp;&amp; ! g.broken) &#123; breakBarrier(); throw ie; &#125; else &#123; // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // "belong" to subsequent execution. Thread.currentThread().interrupt(); &#125; &#125; if (g.broken) throw new BrokenBarrierException(); if (g != generation) // 物是人非，已经不是睡之前的时代了 return index; if (timed &amp;&amp; nanos &lt;= 0L) &#123; breakBarrier(); throw new TimeoutException(); &#125; &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; CyclicBarrier利用ReentrantLock控制对barrier的加锁访问，ReentrantLock.condition控制线程的阻塞唤醒。内部类Generation表示栅栏的一次生命周期，而每次栅栏被踢翻，generation要换代，即CyclicBarrier是可循环复用的。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK源码阅读之CountDownLatch]]></title>
    <url>%2Fbdd3aaa7.html</url>
    <content type="text"><![CDATA[写在前面_作者Doug Lea_如此描述这个类：A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes.这是一个多线程协调的辅助类。源码中给出的示例代码： 1234567891011121314151617181920212223242526272829303132class Driver &#123; // ... void main() throws InterruptedException &#123; CountDownLatch startSignal = new CountDownLatch(1); CountDownLatch doneSignal = new CountDownLatch(N); for (int i = 0; i &lt; N; ++i) // create and start threads new Thread(new Worker(startSignal, doneSignal)).start(); doSomethingElse(); // don't let run yet startSignal.countDown(); // let all threads proceed doSomethingElse(); doneSignal.await(); // wait for all to finish &#125;&#125;class Worker implements Runnable &#123; private final CountDownLatch startSignal; private final CountDownLatch doneSignal; Worker(CountDownLatch startSignal, CountDownLatch doneSignal) &#123; this.startSignal = startSignal; this.doneSignal = doneSignal; &#125; public void run() &#123; try &#123; startSignal.await(); doWork(); doneSignal.countDown(); &#125; catch (InterruptedException ex) &#123;&#125; // return; &#125; void doWork() &#123; ... &#125;&#125;&#125; 通过示例对CountDownLatch的使用场景应该有个清晰的认识。即当有需要线程等待，直到在其他线程的一系列操作完成之后，再接着往下执行。 Sync变量Sync类是CountDownLatch的一个内部类，继承自AbstractQueuedSynchronizer，也就是常说的AQS。内部类重写了AQS的tryAcquireShared和tryReleaseShared两个方法。此外Sync的构造函数带一个int参数，在构造函数内调用了AQS的setState方法，这个方法是对AQS的内部一个int volatile变量赋值。 1234567891011121314151617181920212223242526private static final class Sync extends AbstractQueuedSynchronizer &#123; private static final long serialVersionUID = 4982264981922014374L; Sync(int count) &#123; setState(count); &#125; int getCount() &#123; return getState(); &#125; protected int tryAcquireShared(int acquires) &#123; return (getState() == 0) ? 1 : -1; &#125; protected boolean tryReleaseShared(int releases) &#123; // Decrement count; signal when transition to zero for (;;) &#123; int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; &#125; &#125; 在上边的示例中，我们用到了CountDownLatch的await方法和countDown方法，CountDownLatch的功能也就是通过这两个方法实现。这两个方法其实也就是调用sync。 CountDownLatch.await功能：当前线程等待，直到state等于0，或该线程interrupt。该方法就是调用sync.acquireSharedInterruptibly(1)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 调用实现类Sync的tryAcquireShared if (tryAcquireShared(arg) &lt; 0) // state 不等于 0 则执行 doAcquireSharedInterruptibly(arg);&#125;private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; // 用一个链表来表示需要"wait"的线程，在链表尾部加入一个node // 注意 如果head == null ,则初始化一个head，令head.next = node // 所以能理解state==0,一一唤醒所有等待的线程时，是唤醒头结点的下一节点所表示的线程 // 这样就达到了唤醒所有线程的目的 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; // 前驱 final Node p = node.predecessor(); // 如果当前节点为头节点 if (p == head) &#123; // 调用CountDownLatch实现类里的tryAcquireShared int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 当state == 0时，设置node为头结点 // 并且唤醒(LockSupport.unpark)node下一节点的所表示的线程 setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; &#125; &#125; // wait，直到state == 0 时被唤醒（LockSupport.unpark） if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; 总结：CountDownLatch.await就是调用LockSupport.park阻塞当前线程，并用一链表表示所有阻塞的线程，方便唤醒时一一唤醒。 CountDownLatch.countDown功能：令state减1，但state为0时，唤醒所有等待线程。该方法是调用sync.releaseShared(1)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677 public final boolean releaseShared(int arg) &#123; // 调用实现类Sync的tryReleaseShared // state减1 if (tryReleaseShared(arg)) &#123; // 当state减为0时 doReleaseShared(); return true; &#125; return false; &#125;// 唤醒等待链表的头结点的下一节点// 下一节点唤醒后在doAcquireSharedInterruptibly这个方法中继续循环// 直到执行setHeadAndPropagate，在此方法中又会调用doReleaseShared,唤醒接下来的节点// 依次一一唤醒，直到唤醒所有等待线程节点 private void doReleaseShared() &#123; /* * Ensure that a release propagates, even if there are other * in-progress acquires/releases. This proceeds in the usual * way of trying to unparkSuccessor of head if it needs * signal. But if it does not, status is set to PROPAGATE to * ensure that upon release, propagation continues. * Additionally, we must loop in case a new node is added * while we are doing this. Also, unlike other uses of * unparkSuccessor, we need to know if CAS to reset status * fails, if so rechecking. */ for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; // cas头结点状态为初始状态 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 此方法中是唤醒头结点的下一线程节点 unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; // 如果线程节点改变重新循环 if (h == head) // loop if head changed break; &#125; &#125;// 唤醒node下一线程节点 private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; // node后继节点可能为null或cancel for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) // 唤醒节点s表示线程 LockSupport.unpark(s.thread); &#125; CountDownLatch源码分析的整个流程就是这样。CountDownLatch的功能是基于AQS展开，在后续的JUC的分析文章中还可以看到AQS的身影。接下来仍会对JDK中的源码做一些分析工作，重心会在JUC上。谢谢。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QMQ源码分析之Actor]]></title>
    <url>%2F93d6d117.html</url>
    <content type="text"><![CDATA[前言 QMQ有关actor的一篇文章阐述了actor的应用场景。即client消费消息的请求会先进入一个RequestQueue，在client消费消息时，往往存在多个主题、多个消费组共享一个RequestQueue消费消息。在这个Queue中，存在不同主题的有不同消费组数量，以及不同消费组有不同consumer数量，那么就会存在抢占资源的情况。举个文章中的例子，一个主题下有两个消费组A和B，A有100个consumer，B有200个consumer，那么在RequestQueue中来自B的请求可能会多于A，这个时候就存在消费unfair的情况，所以需要隔离不同主题不同消费组以保证fair。除此之外，当consumer消费能力不足，造成broker消息堆积，这个时候就会导致consumer所在消费组总在消费”老消息”，影响全局整体的一个消费能力。因为”老消息”不会存在page cache中，这个时候很可能就会从磁盘load，那么表现是RequestQueue中来自消费”老消息”消费组的请求处理时间过长，影响到其他主题消费组的消费，因此这个时候也需要做策略来避免不同消费组的相互影响。所以QMQ就有了actor机制，以消除各个消费组之间因消费能力不同、consumer数量不同而造成的相互影响各自的消费能力。 PullMessageWorker要了解QMQ的actor模式是如何起作用的，就要先来看看Broker是如何处理消息拉取请求的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293class PullMessageWorker implements ActorSystem.Processor&lt;PullMessageProcessor.PullEntry&gt; &#123; // 消息存储层 private final MessageStoreWrapper store; // actor private final ActorSystem actorSystem; private final ConcurrentMap&lt;String, ConcurrentMap&lt;String, Object&gt;&gt; subscribers; PullMessageWorker(MessageStoreWrapper store, ActorSystem actorSystem) &#123; this.store = store; this.actorSystem = actorSystem; this.subscribers = new ConcurrentHashMap&lt;&gt;(); &#125; void pull(PullMessageProcessor.PullEntry pullEntry) &#123; // subject+group作actor调度粒度 final String actorPath = ConsumerGroupUtils.buildConsumerGroupKey(pullEntry.subject, pullEntry.group); // actor调度 actorSystem.dispatch(actorPath, pullEntry, this); &#125; @Override public boolean process(PullMessageProcessor.PullEntry entry , ActorSystem.Actor&lt;PullMessageProcessor.PullEntry&gt; self) &#123; QMon.pullQueueTime(entry.subject, entry.group, entry.pullBegin); //开始处理请求的时候就过期了，那么就直接不处理了，也不返回任何东西给客户端，客户端等待超时 //因为出现这种情况一般是server端排队严重，暂时挂起客户端可以避免情况恶化 // deadline机制，如果QMQ认为这个消费请求来不及处理，那么就直接返回，避免雪崩 if (entry.expired()) &#123; QMon.pullExpiredCountInc(entry.subject, entry.group); return true; &#125; if (entry.isInValid()) &#123; QMon.pullInValidCountInc(entry.subject, entry.group); return true; &#125; // 存储层find消息 final PullMessageResult pullMessageResult = store.findMessages(entry.pullRequest); if (pullMessageResult == PullMessageResult.FILTER_EMPTY || pullMessageResult.getMessageNum() &gt; 0 || entry.isPullOnce() || entry.isTimeout()) &#123; entry.processMessageResult(pullMessageResult); return true; &#125; // 没有拉取到消息，那么挂起该actor self.suspend(); // timer task，在超时前唤醒actor if (entry.setTimerOnDemand()) &#123; QMon.suspendRequestCountInc(entry.subject, entry.group); // 订阅消息，一有消息来就唤醒该actor subscribe(entry.subject, entry.group); return false; &#125; // 已经超时，那么即刻唤醒调度 self.resume(); entry.processNoMessageResult(); return true; &#125; // 订阅 private void subscribe(String subject, String group) &#123; ConcurrentMap&lt;String, Object&gt; map = subscribers.get(subject); if (map == null) &#123; map = new ConcurrentHashMap&lt;&gt;(); map = ObjectUtils.defaultIfNull(subscribers.putIfAbsent(subject, map), map); &#125; map.putIfAbsent(group, HOLDER); &#125; // 有消息来就唤醒订阅的subscriber void remindNewMessages(final String subject) &#123; final ConcurrentMap&lt;String, Object&gt; map = this.subscribers.get(subject); if (map == null) return; for (String group : map.keySet()) &#123; map.remove(group); this.actorSystem.resume(ConsumerGroupUtils.buildConsumerGroupKey(subject, group)); QMon.resumeActorCountInc(subject, group); &#125; &#125;&#125;// ActorSystem内定义的处理接口public interface ActorSystem.Processor&lt;T&gt; &#123; boolean process(T message, Actor&lt;T&gt; self);&#125; 能看除在这里起作用的是这个actorSystem。PullMessageWorker继承了ActorSystem.Processor，所以真正处理拉取请求的是这个接口里的process方法。请求到达pullMessageWorker，worker将该次请求交给actorSystem调度，调度到这次请求时，worker还有个根据拉取结果做反应的策略，即如果暂时没有消息，那么suspend，以一个timer task定时resume；如果在timer task执行之前有消息进来，那么也会即时resume。 ActorSystem接下来就看看ActorSystem里边是如何做的公平调度。 123456789101112131415161718192021222324public class ActorSystem &#123; // 内部维护的是一个ConcurrentMap，key即PullMessageWorker里的subject+group private final ConcurrentMap&lt;String, Actor&gt; actors; // 执行actor的executor private final ThreadPoolExecutor executor; private final AtomicInteger actorsCount; private final String name; public ActorSystem(String name) &#123; this(name, Runtime.getRuntime().availableProcessors() * 4, true); &#125; public ActorSystem(String name, int threads, boolean fair) &#123; this.name = name; this.actorsCount = new AtomicInteger(); // 这里根据fair参数初始化一个优先级队列作为executor的参数，处理关于前言里说的"老消息"的情况 BlockingQueue&lt;Runnable&gt; queue = fair ? new PriorityBlockingQueue&lt;&gt;() : new LinkedBlockingQueue&lt;&gt;(); this.executor = new ThreadPoolExecutor(threads, threads, 60, TimeUnit.MINUTES, queue, new NamedThreadFactory("actor-sys-" + name)); this.actors = Maps.newConcurrentMap(); QMon.dispatchersGauge(name, actorsCount::doubleValue); QMon.actorSystemQueueGauge(name, () -&gt; (double) executor.getQueue().size()); &#125;&#125; 可以看到，用一个线程池处理actor的调度执行，这个线程池里的队列是一个优先级队列。优先级队列存储的元素是Actor。关于Actor我们稍后来看，先来看一下ActorSystem的处理调度流程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// PullMessageWorker调用的就是这个方法 public &lt;E&gt; void dispatch(String actorPath, E msg, Processor&lt;E&gt; processor) &#123; // 取得actor Actor&lt;E&gt; actor = createOrGet(actorPath, processor); // 在后文Actor定义里能看到，actor内部维护一个queue，这里actor仅仅是offer(msg) actor.dispatch(msg); // 执行调度 schedule(actor, true); &#125;// 无消息时，则会挂起 public void suspend(String actorPath) &#123; Actor actor = actors.get(actorPath); if (actor == null) return; actor.suspend(); &#125;// 有消息则恢复，可以理解成线程的"就绪状态" public void resume(String actorPath) &#123; Actor actor = actors.get(actorPath); if (actor == null) return; actor.resume(); // 立即调度，可以留意一下那个false // 当actor是"可调度状态"时，这个actor是否能调度是取决于actor的queue是否有消息 schedule(actor, false); &#125; private &lt;E&gt; Actor&lt;E&gt; createOrGet(String actorPath, Processor&lt;E&gt; processor) &#123; Actor&lt;E&gt; actor = actors.get(actorPath); if (actor != null) return actor; Actor&lt;E&gt; add = new Actor&lt;&gt;(this.name, actorPath, this, processor, DEFAULT_QUEUE_SIZE); Actor&lt;E&gt; old = actors.putIfAbsent(actorPath, add); if (old == null) &#123; LOG.info("create actorSystem: &#123;&#125;", actorPath); actorsCount.incrementAndGet(); return add; &#125; return old; &#125;// 将actor入队的地方 private &lt;E&gt; boolean schedule(Actor&lt;E&gt; actor, boolean hasMessageHint) &#123; // 如果actor不能调度，则ret false if (!actor.canBeSchedule(hasMessageHint)) return false; // 设置actor为"可调度状态" if (actor.setAsScheduled()) &#123; // 提交时间，和actor执行总耗时共同决定在队列里的优先级 actor.submitTs = System.currentTimeMillis(); // 入队，入的是线程池里的优先级队列 this.executor.execute(actor); return true; &#125; // actor.setAsScheduled()里，这里是actor已经是可调度状态，那么没必要再次入队 return false; &#125; actorSystem维护一个线程池，线程池队列具有优先级，队列存储元素是actor。actor的粒度是subject+group。Actor是一个Runnable，且因为是优先级队列的存储元素所以需继承Comparable接口（队列并没有传_Comparator参数_），并且actor有四种状态，初始状态、可调度状态、挂起状态、调度状态（这个状态其实不存在，但是暂且这么叫以帮助理解）。接下来看看Actor这个类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175public static class Actor&lt;E&gt; implements Runnable, Comparable&lt;Actor&gt; &#123; // 初始状态 private static final int Open = 0; // 可调度状态 private static final int Scheduled = 2; // 掩码，二进制表示:11 与Open和Scheduled作&amp;运算 // shouldScheduleMask&amp;currentStatus != Open 则为不可置为调度状态（当currentStatus为挂起状态或调度状态） private static final int shouldScheduleMask = 3; private static final int shouldNotProcessMask = ~2; // 挂起状态 private static final int suspendUnit = 4; //每个actor至少执行的时间片 private static final int QUOTA = 5; // status属性内存偏移量，用Unsafe操作 private static long statusOffset; static &#123; try &#123; statusOffset = Unsafe.instance.objectFieldOffset(Actor.class.getDeclaredField("status")); &#125; catch (Throwable t) &#123; throw new ExceptionInInitializerError(t); &#125; &#125; final String systemName; final ActorSystem actorSystem; // actor内部维护的queue，后文简单分析下 final BoundedNodeQueue&lt;E&gt; queue; // ActorSystem内部定义接口，PullMessageWorker实现的就是这个接口，用于真正业务逻辑处理的地方 final Processor&lt;E&gt; processor; private final String name; // 一个actor执行总耗时 private long total; // actor执行提交时间，即actor入队时间 private volatile long submitTs; //通过Unsafe操作 private volatile int status; Actor(String systemName, String name, ActorSystem actorSystem, Processor&lt;E&gt; processor, final int queueSize) &#123; this.systemName = systemName; this.name = name; this.actorSystem = actorSystem; this.processor = processor; this.queue = new BoundedNodeQueue&lt;&gt;(queueSize); QMon.actorQueueGauge(systemName, name, () -&gt; (double) queue.count()); &#125; // 入队，是actor内部的队列 boolean dispatch(E message) &#123; return queue.add(message); &#125; // actor执行的地方 @Override public void run() &#123; long start = System.currentTimeMillis(); String old = Thread.currentThread().getName(); try &#123; Thread.currentThread().setName(systemName + "-" + name); if (shouldProcessMessage()) &#123; processMessages(); &#125; &#125; finally &#123; long duration = System.currentTimeMillis() - start; // 每次actor执行的耗时累加到total total += duration; QMon.actorProcessTime(name, duration); Thread.currentThread().setName(old); // 设置为"空闲状态"，即初始状态 (currentStatus &amp; ~Scheduled) setAsIdle(); // 进行下一次调度 this.actorSystem.schedule(this, false); &#125; &#125; void processMessages() &#123; long deadline = System.currentTimeMillis() + QUOTA; while (true) &#123; E message = queue.peek(); if (message == null) return; // 处理业务逻辑 boolean process = processor.process(message, this); // 失败，该message不会出队，等待下一次调度 // 如pullMessageWorker中没有消息时将actor挂起 if (!process) return; // 出队 queue.pollNode(); // 每个actor只有QUOTA个时间片的执行时间 if (System.currentTimeMillis() &gt;= deadline) return; &#125; &#125; final boolean shouldProcessMessage() &#123; // 能够真正执行业务逻辑的判断 // 一种场景是，针对挂起状态，由于没有拉取到消息该actor置为挂起状态 // 自然就没有抢占时间片的必要了 return (currentStatus() &amp; shouldNotProcessMask) == 0; &#125; // 能否调度 private boolean canBeSchedule(boolean hasMessageHint) &#123; int s = currentStatus(); if (s == Open || s == Scheduled) return hasMessageHint || !queue.isEmpty(); return false; &#125; public final boolean resume() &#123; while (true) &#123; int s = currentStatus(); int next = s &lt; suspendUnit ? s : s - suspendUnit; if (updateStatus(s, next)) return next &lt; suspendUnit; &#125; &#125; public final void suspend() &#123; while (true) &#123; int s = currentStatus(); if (updateStatus(s, s + suspendUnit)) return; &#125; &#125; final boolean setAsScheduled() &#123; while (true) &#123; int s = currentStatus(); // currentStatus为非Open状态，则ret false if ((s &amp; shouldScheduleMask) != Open) return false; // 更新actor状态为调度状态 if (updateStatus(s, s | Scheduled)) return true; &#125; &#125; final void setAsIdle() &#123; while (true) &#123; int s = currentStatus(); // 更新actor状态位不可调度状态，(这里可以理解为更新为初始状态Open) if (updateStatus(s, s &amp; ~Scheduled)) return; &#125; &#125; final int currentStatus() &#123; // 根据status在内存中的偏移量取得status return Unsafe.instance.getIntVolatile(this, statusOffset); &#125; private boolean updateStatus(int oldStatus, int newStatus) &#123; // Unsafe 原子操作，处理status的轮转变更 return Unsafe.instance.compareAndSwapInt(this, statusOffset, oldStatus, newStatus); &#125; // 决定actor在优先级队列里的优先级的地方 // 先看总耗时，以达到动态限速，保证执行"慢"的请求（已经堆积的消息拉取请求）在后执行 // 其次看提交时间，先提交的actor先执行 @Override public int compareTo(Actor o) &#123; int result = Long.compare(total, o.total); return result == 0 ? Long.compare(submitTs, o.submitTs) : result; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Actor&lt;?&gt; actor = (Actor&lt;?&gt;) o; return Objects.equals(systemName, actor.systemName) &amp;&amp; Objects.equals(name, actor.name); &#125; @Override public int hashCode() &#123; return Objects.hash(systemName, name); &#125;&#125; Actor实现了Comparable，在优先级队列里优先级是Actor里的total和submitTs共同决定的。total是actor执行总耗时，submitTs是调度时间。那么对于处理较慢的actor自然就会在队列里相对”尾部”位置，这时就做到了根据actor的执行耗时的一个动态限速。Actor利用Unsafe机制来控制各个状态的轮转原子性更新的，且每个actor执行时间可以简单理解为5个时间片（暂且这么理解，其实并不是时间片）。其实工作进行到这里就可以结束了，但是抱着对于编程的热爱（周末闲的慌），还可以往下接着看看。Actor内部维护一个Queue，这个Queue是自定义的，是一个Lock-free bounded non-blocking multiple-producer single-consumer queue。JDK里的QUEUE多数都是用锁控制，不用锁，猜测也应该是用Unsafe 原子操作实现。那么来看看吧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176private static class BoundedNodeQueue&lt;T&gt; &#123;// 头结点、尾节点在内存中的偏移量 private final static long enqOffset, deqOffset; static &#123; try &#123; enqOffset = Unsafe.instance.objectFieldOffset(BoundedNodeQueue.class.getDeclaredField("_enqDoNotCallMeDirectly")); deqOffset = Unsafe.instance.objectFieldOffset(BoundedNodeQueue.class.getDeclaredField("_deqDoNotCallMeDirectly")); &#125; catch (Throwable t) &#123; throw new ExceptionInInitializerError(t); &#125; &#125; private final int capacity; // 尾节点，通过enqOffset操作 private volatile Node&lt;T&gt; _enqDoNotCallMeDirectly; // 头结点，通过deqOffset操作 private volatile Node&lt;T&gt; _deqDoNotCallMeDirectly; protected BoundedNodeQueue(final int capacity) &#123; if (capacity &lt; 0) throw new IllegalArgumentException("AbstractBoundedNodeQueue.capacity must be &gt;= 0"); this.capacity = capacity; final Node&lt;T&gt; n = new Node&lt;T&gt;(); setDeq(n); setEnq(n); &#125;// 获取尾节点 private Node&lt;T&gt; getEnq() &#123; // getObjectVolatile这种方式保证拿到的都是最新数据 return (Node&lt;T&gt;) Unsafe.instance.getObjectVolatile(this, enqOffset); &#125;// 设置尾节点，仅在初始化时用 private void setEnq(Node&lt;T&gt; n) &#123; Unsafe.instance.putObjectVolatile(this, enqOffset, n); &#125; private boolean casEnq(Node&lt;T&gt; old, Node&lt;T&gt; nju) &#123; // cas，循环设置，直到成功 return Unsafe.instance.compareAndSwapObject(this, enqOffset, old, nju); &#125;// 获取头结点 private Node&lt;T&gt; getDeq() &#123; return (Node&lt;T&gt;) Unsafe.instance.getObjectVolatile(this, deqOffset); &#125;// 仅在初始化时用 private void setDeq(Node&lt;T&gt; n) &#123; Unsafe.instance.putObjectVolatile(this, deqOffset, n); &#125;// cas设置头结点 private boolean casDeq(Node&lt;T&gt; old, Node&lt;T&gt; nju) &#123; return Unsafe.instance.compareAndSwapObject(this, deqOffset, old, nju); &#125;// 与其叫count，不如唤作index，但是是否应该考虑溢出的情况？ public final int count() &#123; final Node&lt;T&gt; lastNode = getEnq(); final int lastNodeCount = lastNode.count; return lastNodeCount - getDeq().count; &#125; /** * @return the maximum capacity of this queue */ public final int capacity() &#123; return capacity; &#125; public final boolean add(final T value) &#123; for (Node&lt;T&gt; n = null; ; ) &#123; final Node&lt;T&gt; lastNode = getEnq(); final int lastNodeCount = lastNode.count; if (lastNodeCount - getDeq().count &lt; capacity) &#123; // Trade a branch for avoiding to create a new node if full, // and to avoid creating multiple nodes on write conflict á la Be Kind to Your GC if (n == null) &#123; n = new Node&lt;T&gt;(); n.value = value; &#125; n.count = lastNodeCount + 1; // Piggyback on the HB-edge between getEnq() and casEnq() // Try to putPullLogs the node to the end, if we fail we continue loopin' // 相当于 // enq -&gt; next = new Node(value); enq = neq -&gt; next; if (casEnq(lastNode, n)) &#123; // 注意一下这个Node.setNext方法 lastNode.setNext(n); return true; &#125; &#125; else return false; // Over capacity—couldn't add the node &#125; &#125; public final boolean isEmpty() &#123; // enq == deq 即为empty return getEnq() == getDeq(); &#125; /** * Removes the first element of this queue if any * * @return the value of the first element of the queue, null if empty */ public final T poll() &#123; final Node&lt;T&gt; n = pollNode(); return (n != null) ? n.value : null; &#125; public final T peek() &#123; Node&lt;T&gt; n = peekNode(); return (n != null) ? n.value : null; &#125; protected final Node&lt;T&gt; peekNode() &#123; for (; ; ) &#123; final Node&lt;T&gt; deq = getDeq(); final Node&lt;T&gt; next = deq.next(); if (next != null || getEnq() == deq) return next; &#125; &#125; /** * Removes the first element of this queue if any * * @return the `Node` of the first element of the queue, null if empty */ public final Node&lt;T&gt; pollNode() &#123; for (; ; ) &#123; final Node&lt;T&gt; deq = getDeq(); final Node&lt;T&gt; next = deq.next(); if (next != null) &#123; if (casDeq(deq, next)) &#123; deq.value = next.value; deq.setNext(null); next.value = null; return deq; &#125; // else we retry (concurrent consumers) // 比较套路的cas操作，就不多说了 &#125; else if (getEnq() == deq) return null; // If we got a null and head meets tail, we are empty &#125; &#125; public static class Node&lt;T&gt; &#123; private final static long nextOffset; static &#123; try &#123; nextOffset = Unsafe.instance.objectFieldOffset(Node.class.getDeclaredField("_nextDoNotCallMeDirectly")); &#125; catch (Throwable t) &#123; throw new ExceptionInInitializerError(t); &#125; &#125; protected T value; protected int count; // 也是利用偏移量操作 private volatile Node&lt;T&gt; _nextDoNotCallMeDirectly; public final Node&lt;T&gt; next() &#123; return (Node&lt;T&gt;) Unsafe.instance.getObjectVolatile(this, nextOffset); &#125; protected final void setNext(final Node&lt;T&gt; newNext) &#123; // 这里有点讲究，下面分析下 Unsafe.instance.putOrderedObject(this, nextOffset, newNext); &#125; &#125;&#125; 如上代码，是通过属性在内存的偏移量，结合cas原子操作来进行更新赋值等操作，以此来实现lock-free，这是比较常规的套路。值得一说的是Node里的setNext方法，这个方法的调用是在cas节点后，对”上一位置”的next节点进行赋值。而这个方法使用的是Unsafe.instance.putOrderedObject，要说这个putOrderedObject，就不得不说MESI，缓存一致性协议。如volatile，当进行写操作时，它是依靠storeload barrier来实现其他线程对此的可见性。而putOrderedObject也是依靠内存屏障，只不过是storestore barrier。storestore是比storeload快速的一种内存屏障。在硬件层面，内存屏障分两种：Load-Barrier和Store-Barrier。Load-Barrier是让高速缓存中的数据失效，强制重新从主内存加载数据；Store-Barrier是让写入高速缓存的数据更新写入主内存，对其他线程可见。而java层面的四种内存屏障无非是硬件层面的两种内存屏障的组合而已。那么可见，storestore barrier自然比storeload barrier快速。那么有一个问题，我们可不可以在这里也用cas操作呢？答案是可以，但没必要。你可以想想这里为什么没必要。谢谢。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QMQ源码分析之delay-server篇【三】]]></title>
    <url>%2F4c5184d3.html</url>
    <content type="text"><![CDATA[前言 上篇我们分析了QMQ delay-server关于存储的部分，这一篇我们会对投递的源码进行分析。 投递投递的相关内容在WheelTickManager这个类。提前加载schedule_log、wheel根据延迟时间到时进行投递等相关工作都在这里完成。而关于真正进行投递的相关类是在sender这个包里。 wheelwheel包里一共就三个类文件， HashWheelTimer 、 WheelLoadCursor 、 WheelTickManager ， WheelTickManager 就应该是wheel加载文件，wheel中的消息到时投递的管理器； WheelLoadCursor 应该就是上一篇中提到的schedule_log文件加载到哪里的cursor标识；那么 HashWheelTimer 就是一个辅助工具类，简单理解成Java中的 ScheduledExecutorService ，可理解成是根据延迟消息的延迟时间进行投递的timer，所以这里不对这个工具类做更多解读，我们更关心MQ逻辑。首先来看提前一定时间加载schedule_log，这里的提前一定时间是多长时间呢？这个是根据需要配置的，比如schedule_log的刻度自定义配置为1h，提前加载时间配置为30min，那么在2019-02-10 17:30就应该加载2019021018这个schedule_log。 1234567891011121314@Override public void start() &#123; if (!isStarted()) &#123; sender.init(); // hash wheel timer,内存中的wheel timer.start(); started.set(true); // 根据dispatch log,从上次投递结束的地方恢复开始投递 recover(); // 加载线程，用于加载schedule_log loadScheduler.scheduleWithFixedDelay(this::load, 0, config.getLoadSegmentDelayMinutes(), TimeUnit.MINUTES); LOGGER.info("wheel started."); &#125; &#125; recover 这个方法，会根据dispatch log中的投递记录，找到上一次最后投递的位置，在delay-server重启的时候，wheel会根据这个位置恢复投递。 1234567891011121314151617181920212223242526272829303132333435private void recover() &#123; LOGGER.info("wheel recover..."); // 最新的dispatch log segment DispatchLogSegment currentDispatchedSegment = facade.latestDispatchSegment(); if (currentDispatchedSegment == null) &#123; LOGGER.warn("load latest dispatch segment null"); return; &#125; int latestOffset = currentDispatchedSegment.getSegmentBaseOffset(); DispatchLogSegment lastSegment = facade.lowerDispatchSegment(latestOffset); if (null != lastSegment) doRecover(lastSegment); // 根据最新的dispatch log segment进行恢复投递 doRecover(currentDispatchedSegment); LOGGER.info("wheel recover done. currentOffset:&#123;&#125;", latestOffset); &#125;private void doRecover(DispatchLogSegment dispatchLogSegment) &#123; int segmentBaseOffset = dispatchLogSegment.getSegmentBaseOffset(); ScheduleSetSegment setSegment = facade.loadScheduleLogSegment(segmentBaseOffset); if (setSegment == null) &#123; LOGGER.error("load schedule index error,dispatch segment:&#123;&#125;", segmentBaseOffset); return; &#125; // 得到一个关于已投递记录的set LongHashSet dispatchedSet = loadDispatchLog(dispatchLogSegment); // 根据这个set，将最新的dispatch log segment中未投递的消息add in wheel。 WheelLoadCursor.Cursor loadCursor = facade.loadUnDispatch(setSegment, dispatchedSet, this::refresh); int baseOffset = loadCursor.getBaseOffset(); // 记录cursor loadingCursor.shiftCursor(baseOffset, loadCursor.getOffset()); loadedCursor.shiftCursor(baseOffset); &#125; 恢复基本就是以上的这些内容，接下来看看是如何加载的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344 private void load() &#123; // 提前一定时间加载到下一 delay segment long next = System.currentTimeMillis() + config.getLoadInAdvanceTimesInMillis(); int prepareLoadBaseOffset = resolveSegment(next); try &#123; // 加载到prepareLoadBaseOffset这个delay segment loadUntil(prepareLoadBaseOffset); &#125; catch (InterruptedException ignored) &#123; LOGGER.debug("load segment interrupted"); &#125; &#125;private void loadUntil(int until) throws InterruptedException &#123; // 当前wheel已加载到baseOffset int loadedBaseOffset = loadedCursor.baseOffset(); // 如已加载到until，则break // have loaded if (loadedBaseOffset &gt; until) return; do &#123; // 加载失败，则break // wait next turn when loaded error. if (!loadUntilInternal(until)) break; // 当前并没有until这个delay segment，即loading cursor小于until // load successfully(no error happened) and current wheel loading cursor &lt; until if (loadingCursor.baseOffset() &lt; until) &#123; // 阻塞，直到thresholdTime+blockingExitTime // 即如果提前blockingExitTime还未有until这个delay segment的消息进来，则退出 long thresholdTime = System.currentTimeMillis() + config.getLoadBlockingExitTimesInMillis(); // exit in a few minutes in advance if (resolveSegment(thresholdTime) &gt;= until) &#123; loadingCursor.shiftCursor(until); loadedCursor.shiftCursor(until); break; &#125; &#125; // 避免cpu load过高 Thread.sleep(100); &#125; while (loadedCursor.baseOffset() &lt; until); LOGGER.info("wheel load until &#123;&#125; &lt;= &#123;&#125;", loadedCursor.baseOffset(), until); &#125; 根据配置的提前加载时间，内存中的wheel会提前加载schedule_log，加载是在一个while循环里，直到加载到until delay segment才退出，如果当前没有until 这个delay segment，那么会在配置的 blockingExitTime 时间退出该循环，而为了避免cpu load过高，这里会在每次循环间隔设置100ms sleep。这里加载为什么是在while循环里？以及为什么sleep 100ms，sleep 500ms 或者1s可不可以？以及为什么要设置个blockingExitTime呢？下面的分析之后，应该就能回答这些问题了。主要考虑两种情况，一种是当之前一直没有delay segment或者delay segment是间隔存在的，比如delay segment刻度为1h，2019031001和2019031004之间的2019031002及2019031003不存在这种之类的delay segment不存在的情况，另一种是当正在加载delay segment的时候，位于该segment的延迟消息正在被加载，这种情况是有可能丢消息的。所以这里加载是在一个循环里，以及设置了两个cursor，即loading cursor，和loaded cursor。一个表示正在加载，一个表示已经加载。此外，上面每次循环sleep 100ms，可不可以sleep 500ms or 1s？答案是可以，只是消息是否能容忍500ms 或者1s的延迟。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374private boolean loadUntilInternal(int until) &#123; int index = resolveStartIndex(); if (index &lt; 0) return true; try &#123; while (index &lt;= until) &#123; ScheduleSetSegment segment = facade.loadScheduleLogSegment(index); if (segment == null) &#123; int nextIndex = facade.higherScheduleBaseOffset(index); if (nextIndex &lt; 0) return true; index = nextIndex; continue; &#125;// 具体加载某个segment的地方 loadSegment(segment); int nextIndex = facade.higherScheduleBaseOffset(index); if (nextIndex &lt; 0) return true; index = nextIndex; &#125; &#125; catch (Throwable e) &#123; LOGGER.error("wheel load segment failed,currentSegmentOffset:&#123;&#125; until:&#123;&#125;", loadedCursor.baseOffset(), until, e); QMon.loadSegmentFailed(); return false; &#125; return true;&#125;private void loadSegment(ScheduleSetSegment segment) &#123; final long start = System.currentTimeMillis(); try &#123; int baseOffset = segment.getSegmentBaseOffset(); long offset = segment.getWrotePosition(); if (!loadingCursor.shiftCursor(baseOffset, offset)) &#123; LOGGER.error("doLoadSegment error,shift loadingCursor failed,from &#123;&#125;-&#123;&#125; to &#123;&#125;-&#123;&#125;", loadingCursor.baseOffset(), loadingCursor.offset(), baseOffset, offset); return; &#125; WheelLoadCursor.Cursor loadedCursorEntry = loadedCursor.cursor(); // have loaded // 已经加载 if (baseOffset &lt; loadedCursorEntry.getBaseOffset()) return; long startOffset = 0; // last load action happened error // 如果上次加载失败，则从上一次的位置恢复加载 if (baseOffset == loadedCursorEntry.getBaseOffset() &amp;&amp; loadedCursorEntry.getOffset() &gt; -1) startOffset = loadedCursorEntry.getOffset(); LogVisitor&lt;ScheduleIndex&gt; visitor = segment.newVisitor(startOffset, config.getSingleMessageLimitSize()); try &#123; loadedCursor.shiftCursor(baseOffset, startOffset); long currentOffset = startOffset; // 考虑一种情况，当前delay segment正在append消息，所以是while，而loaded cursor的offset也是没加载一个消息更新的 while (currentOffset &lt; offset) &#123; Optional&lt;ScheduleIndex&gt; recordOptional = visitor.nextRecord(); if (!recordOptional.isPresent()) break; ScheduleIndex index = recordOptional.get(); currentOffset = index.getOffset() + index.getSize(); refresh(index); loadedCursor.shiftOffset(currentOffset); &#125; loadedCursor.shiftCursor(baseOffset); LOGGER.info("loaded segment:&#123;&#125; &#123;&#125;", loadedCursor.baseOffset(), currentOffset); &#125; finally &#123; visitor.close(); &#125; &#125; finally &#123; Metrics.timer("loadSegmentTimer").update(System.currentTimeMillis() - start, TimeUnit.MILLISECONDS); &#125;&#125; 还记得上一篇我们提到过，存储的时候，如果这个消息位于正在被wheel加载segment中，那么这个消息应该是会被加载到wheel中的。 12345678910111213141516171819202122232425262728293031 private boolean iterateCallback(final ScheduleIndex index) &#123; long scheduleTime = index.getScheduleTime(); long offset = index.getOffset(); // 主要看一下这个canAdd if (wheelTickManager.canAdd(scheduleTime, offset)) &#123; wheelTickManager.addWHeel(index); return true; &#125; return false; &#125; // 就是cursor起作用的地方了 public boolean canAdd(long scheduleTime, long offset) &#123; WheelLoadCursor.Cursor currentCursor = loadingCursor.cursor(); int currentBaseOffset = currentCursor.getBaseOffset(); long currentOffset = currentCursor.getOffset();// 根据延迟时间确定该消息位于哪个segment int baseOffset = resolveSegment(scheduleTime); // 小于当前loading cursor,则put int wheel if (baseOffset &lt; currentBaseOffset) return true;// 正在加载 if (baseOffset == currentBaseOffset) &#123; // 根据cursor的offset判断 return currentOffset &lt;= offset; &#125; return false; &#125; sendersender包里结构如下图：通过brokerGroup做分组，根据组批量发送，发送时是多线程发送，每个组互不影响，发送时也会根据实时broker的weight进行选择考虑broker进行发送。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 @Override public void send(ScheduleIndex index) &#123; if (!BrokerRoleManager.isDelayMaster()) &#123; return; &#125; boolean add; try &#123; long waitTime = Math.abs(sendWaitTime); // 入队 if (waitTime &gt; 0) &#123; add = batchExecutor.addItem(index, waitTime, TimeUnit.MILLISECONDS); &#125; else &#123; add = batchExecutor.addItem(index); &#125; &#125; catch (InterruptedException e) &#123; return; &#125; if (!add) &#123; reject(index); &#125; &#125; @Override public void process(List&lt;ScheduleIndex&gt; indexList) &#123; try &#123; // 发送处理逻辑在senderExecutor里 senderExecutor.execute(indexList, this, brokerService); &#125; catch (Exception e) &#123; LOGGER.error("send message failed,messageSize:&#123;&#125; will retry", indexList.size(), e); retry(indexList); &#125; &#125;// 以下为senderExecutor内容 void execute(final List&lt;ScheduleIndex&gt; indexList, final SenderGroup.ResultHandler handler, final BrokerService brokerService) &#123; // 分组 Map&lt;SenderGroup, List&lt;ScheduleIndex&gt;&gt; groups = groupByBroker(indexList, brokerService); for (Map.Entry&lt;SenderGroup, List&lt;ScheduleIndex&gt;&gt; entry : groups.entrySet()) &#123; doExecute(entry.getKey(), entry.getValue(), handler); &#125; &#125; private void doExecute(final SenderGroup group, final List&lt;ScheduleIndex&gt; list, final SenderGroup.ResultHandler handler) &#123; // 分组发送 group.send(list, sender, handler); &#125; 可以看到，投递时是根据server broker进行分组投递。看一下 SenderGroup 这个类可以看到，每个组的投递是多线程，互不影响，不会存在某个组的server挂掉，导致其他组无法投递。并且这里如果存在某个组无法投递，重试时会选择其它的server broker进行重试。与此同时，在选择组时，会根据每个server broker的weight进行综合考量，即当前server broker有多少消息量要发送。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 具体发送的地方 private void send(Sender sender, ResultHandler handler, BrokerGroupInfo groupInfo, String groupName, List&lt;ScheduleIndex&gt; list) &#123; try &#123; long start = System.currentTimeMillis(); List&lt;ScheduleSetRecord&gt; records = store.recoverLogRecord(list); QMon.loadMsgTime(System.currentTimeMillis() - start); Datagram response = sendMessages(records, sender); release(records); monitor(list, groupName); if (response == null) &#123; handler.fail(list); &#125; else &#123; final int responseCode = response.getHeader().getCode(); final Map&lt;String, SendResult&gt; resultMap = getSendResult(response); if (resultMap == null || responseCode != CommandCode.SUCCESS) &#123; if (responseCode == CommandCode.BROKER_REJECT || responseCode == CommandCode.BROKER_ERROR) &#123; // 该组熔断 groupInfo.markFailed(); &#125; monitorSendFail(list, groupInfo.getGroupName()); // 重试 handler.fail(list); return; &#125; Set&lt;String&gt; failedMessageIds = new HashSet&lt;&gt;(); boolean brokerRefreshed = false; for (Map.Entry&lt;String, SendResult&gt; entry : resultMap.entrySet()) &#123; int resultCode = entry.getValue().getCode(); if (resultCode != MessageProducerCode.SUCCESS) &#123; failedMessageIds.add(entry.getKey()); &#125; if (!brokerRefreshed &amp;&amp; resultCode == MessageProducerCode.BROKER_READ_ONLY) &#123; groupInfo.markFailed(); brokerRefreshed = true; &#125; &#125; if (!brokerRefreshed) groupInfo.markSuccess(); // dispatch log 记录在这里产生 handler.success(records, failedMessageIds); &#125; &#125; catch (Throwable e) &#123; LOGGER.error("sender group send batch failed,broker:&#123;&#125;,batch size:&#123;&#125;", groupName, list.size(), e); handler.fail(list); &#125; &#125; 就是以上这些，关于QMQ的delay-server源码分析就是这些了，如果以后有机会会分析一下QMQ的其他模块源码，谢谢。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QMQ源码分析之delay-server篇【二】]]></title>
    <url>%2F14f2781.html</url>
    <content type="text"><![CDATA[前言 本来是固定时间周六更博，但是昨天失恋了，心情不好，晚了一天。那么上一篇我们梳理了下QMQ延迟消息的主要功能，这一篇在此基础上，对照着功能分析一下源码。 整体结构要了解delay-server源码的一个整体结构，需要我们跟着源码，从初始化开始简单先过一遍。重试工作都在startup这个包里，而这个包只有一个ServerWrapper类。结合上一篇的内容，通过这个类就基本能看到delay的一个源码结构。delay-server基于netty，init方法完成初始化工作（端口默认为20801、心跳、wheel等），register方法是向meta-server发起请求，获取自己自己的角色为 delay ，并开始和meta-server的心跳。startServer方法是开始HashWheel的转动，从上次结束的位置继续message_log的回放，开启netty server。另外在做准备工作时知道QMQ是基于一主一从一备的方式，关于这个sync方法，是开启监听一个端口回应同步拉取动作，如果是从节点还要开始向主节点发起同步拉取动作。当这一切都完成了，那么online方法就执行，表示delay开始上线提供服务了。总结一下两个要点，QMQ是基于netty进行通信，并且采用一主一从一备的方式。 存储关于存储在之前我们也讨论了，delay-server接收到延迟消息，会顺序append到message_log，之后再对message_log进行回放，以生成schedule_log。所以关于存储我们需要关注两个东西，一个是message_log的存储，另一个是schedule_log的生成。 message_log其实 message_log 的生成很简单，就是顺序append。主要逻辑在 qunar.tc.qmq.delay.receiver.Receiver 这个类里，大致流程就是关于QMQ自定义协议的一个反序列化，然后再对序列化的单个消息进行存储。如图：主要逻辑在途中标红方法 doInvoke 中。 1234567891011private void doInvoke(ReceivedDelayMessage message) &#123; // ... try &#123; // 注：这里是进行append的地方 ReceivedResult result = facade.appendMessageLog(message); offer(message, result); &#125; catch (Throwable t) &#123; error(message, t); &#125;&#125; delay存储层相关逻辑都在facade这个类里，初始化时类似消息的校验等工作也都在这里，而message_log的相关操作都在 messageLog 里。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990@Override public AppendMessageRecordResult append(RawMessageExtend record) &#123; AppendMessageResult&lt;Long&gt; result; // 注：当前最新的一个segment LogSegment segment = logManager.latestSegment(); if (null == segment) &#123; segment = logManager.allocNextSegment(); &#125; if (null == segment) &#123; return new AppendMessageRecordResult(PutMessageStatus.CREATE_MAPPED_FILE_FAILED, null); &#125; // 注：真正进行append的动作是messageAppender result = segment.append(record, messageAppender); switch (result.getStatus()) &#123; case MESSAGE_SIZE_EXCEEDED: return new AppendMessageRecordResult(PutMessageStatus.MESSAGE_ILLEGAL, null); case END_OF_FILE: if (null == logManager.allocNextSegment()) &#123; return new AppendMessageRecordResult(PutMessageStatus.CREATE_MAPPED_FILE_FAILED, null); &#125; return append(record); case SUCCESS: return new AppendMessageRecordResult(PutMessageStatus.SUCCESS, result); default: return new AppendMessageRecordResult(PutMessageStatus.UNKNOWN_ERROR, result); &#125; &#125; // 看一下这个appender，也可以通过这里能看到QMQ的delay message 格式定义 private class DelayRawMessageAppender implements MessageAppender&lt;RawMessageExtend, Long&gt; &#123; private final ReentrantLock lock = new ReentrantLock(); private final ByteBuffer workingBuffer = ByteBuffer.allocate(1024); @Override public AppendMessageResult&lt;Long&gt; doAppend(long baseOffset, ByteBuffer targetBuffer, int freeSpace, RawMessageExtend message) &#123; // 这个lock这里影响不大 lock.lock(); try &#123; workingBuffer.clear(); final String messageId = message.getHeader().getMessageId(); final byte[] messageIdBytes = messageId.getBytes(StandardCharsets.UTF_8); final String subject = message.getHeader().getSubject(); final byte[] subjectBytes = subject.getBytes(StandardCharsets.UTF_8); final long startWroteOffset = baseOffset + targetBuffer.position(); final int recordSize = recordSizeWithCrc(messageIdBytes.length, subjectBytes.length, message.getBodySize()); if (recordSize &gt; config.getSingleMessageLimitSize()) &#123; return new AppendMessageResult&lt;&gt;(AppendMessageStatus.MESSAGE_SIZE_EXCEEDED, startWroteOffset, freeSpace, null); &#125; workingBuffer.flip(); if (recordSize != freeSpace &amp;&amp; recordSize + MIN_RECORD_BYTES &gt; freeSpace) &#123; // 填充 workingBuffer.limit(freeSpace); workingBuffer.putInt(MESSAGE_LOG_MAGIC_V1); workingBuffer.put(MessageLogAttrEnum.ATTR_EMPTY_RECORD.getCode()); workingBuffer.putLong(System.currentTimeMillis()); targetBuffer.put(workingBuffer.array(), 0, freeSpace); return new AppendMessageResult&lt;&gt;(AppendMessageStatus.END_OF_FILE, startWroteOffset, freeSpace, null); &#125; else &#123; int headerSize = recordSize - message.getBodySize(); workingBuffer.limit(headerSize); workingBuffer.putInt(MESSAGE_LOG_MAGIC_V2); workingBuffer.put(MessageLogAttrEnum.ATTR_MESSAGE_RECORD.getCode()); workingBuffer.putLong(System.currentTimeMillis()); // 注意这里，是schedule_time ，即延迟时间 workingBuffer.putLong(message.getScheduleTime()); // sequence,每个brokerGroup应该是唯一的 workingBuffer.putLong(sequence.incrementAndGet()); workingBuffer.putInt(messageIdBytes.length); workingBuffer.put(messageIdBytes); workingBuffer.putInt(subjectBytes.length); workingBuffer.put(subjectBytes); workingBuffer.putLong(message.getHeader().getBodyCrc()); workingBuffer.putInt(message.getBodySize()); targetBuffer.put(workingBuffer.array(), 0, headerSize); targetBuffer.put(message.getBody().nioBuffer()); final long payloadOffset = startWroteOffset + headerSize; return new AppendMessageResult&lt;&gt;(AppendMessageStatus.SUCCESS, startWroteOffset, recordSize, payloadOffset); &#125; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; 以上基本就是message_log的存储部分，接下来我们来看message_log的回放生成schedule_log。 schedule_logMessageLogReplayer这个类就是控制回放的地方。那么考虑一个问题，下一次重启的时候，我们该从哪里进行回放？QMQ是会有一个回放的offset，这个offset会定时刷盘，下次重启的时候会从这个offset位置开始回放。细节可以看一下下面这段代码块。 123456789101112131415161718192021222324final LogVisitor&lt;LogRecord&gt; visitor = facade.newMessageLogVisitor(iterateFrom.longValue());adjustOffset(visitor);while (true) &#123; final Optional&lt;LogRecord&gt; recordOptional = visitor.nextRecord(); if (recordOptional.isPresent() &amp;&amp; recordOptional.get() == DelayMessageLogVisitor.EMPTY_LOG_RECORD) &#123; break; &#125; recordOptional.ifPresent((record) -&gt; &#123; // post以进行存储 dispatcher.post(record); long checkpoint = record.getStartWroteOffset() + record.getRecordSize(); this.cursor.addAndGet(record.getRecordSize()); facade.updateIterateOffset(checkpoint); &#125;);&#125;iterateFrom.add(visitor.visitedBufferSize());try &#123; TimeUnit.MILLISECONDS.sleep(5);&#125; catch (InterruptedException e) &#123; LOGGER.warn("message log iterate sleep interrupted");&#125; 注意这里除了offset还有个cursor，这是为了防止回放失败，sleep 5ms后再次回放的时候从cursor位置开始，避免重复消息。那么我们看一下dispatcher.post这个方法: 12345678910111213 @Override public void post(LogRecord event) &#123; // 这里是schedule_log AppendLogResult&lt;ScheduleIndex&gt; result = facade.appendScheduleLog(event); int code = result.getCode(); if (MessageProducerCode.SUCCESS != code) &#123; LOGGER.error("appendMessageLog schedule log error,log:&#123;&#125; &#123;&#125;,code:&#123;&#125;", event.getSubject(), event.getMessageId(), code); throw new AppendException("appendScheduleLogError"); &#125;// 先看这里 iterateCallback.apply(result.getAdditional()); &#125; 如以上代码，我们看略过schedule_log的存储，看一下那个callback是几个意思: 12345678910111213private boolean iterateCallback(final ScheduleIndex index) &#123; // 延迟时间 long scheduleTime = index.getScheduleTime(); // 这个offset是startOffset,即在delay_segment中的这个消息的起始位置 long offset = index.getOffset(); // 是否add到内存中的HashWheel if (wheelTickManager.canAdd(scheduleTime, offset)) &#123; wheelTickManager.addWHeel(index); return true; &#125; return false; &#125; 这里的意思是，delay-server接收到消息，会判断一下这个消息是否需要add到内存中的wheel中，以防止丢消息。大家记着有这个事情，在投递小节中我们回过头来再说这里。那么回到 facade.appendScheduleLog 这个方法，schedule_log相关操作在scheduleLog里： 12345678910111213141516@Override public RecordResult&lt;T&gt; append(LogRecord record) &#123; long scheduleTime = record.getScheduleTime(); // 这里是根据延迟时间定位对应的delaySegment的 DelaySegment&lt;T&gt; segment = locateSegment(scheduleTime); if (null == segment) &#123; segment = allocNewSegment(scheduleTime); &#125; if (null == segment) &#123; return new NopeRecordResult(PutMessageStatus.CREATE_MAPPED_FILE_FAILED); &#125;// 具体动作在append里 return retResult(segment.append(record, appender)); &#125; 留意 locateSegment 这个方法，它是根据延迟时间定位 DelaySegment ，比如如果延迟时间是2019-03-03 16:00:00，那么就会定位到201903031600这个DelaySegment（注：这里贴的代码不是最新的，最新的是 DelaySegment 的刻度是可以配置，到分钟级别）。同样，具体动作也是 appender 做的，如下: 123456789101112131415161718192021222324@Override public AppendRecordResult&lt;ScheduleSetSequence&gt; appendLog(LogRecord log) &#123; workingBuffer.clear(); workingBuffer.flip(); final byte[] subjectBytes = log.getSubject().getBytes(StandardCharsets.UTF_8); final byte[] messageIdBytes = log.getMessageId().getBytes(StandardCharsets.UTF_8); int recordSize = getRecordSize(log, subjectBytes.length, messageIdBytes.length); workingBuffer.limit(recordSize); long scheduleTime = log.getScheduleTime(); long sequence = log.getSequence(); workingBuffer.putLong(scheduleTime); // message_log中的sequence workingBuffer.putLong(sequence); workingBuffer.putInt(log.getPayloadSize()); workingBuffer.putInt(messageIdBytes.length); workingBuffer.put(messageIdBytes); workingBuffer.putInt(subjectBytes.length); workingBuffer.put(subjectBytes); workingBuffer.put(log.getRecord()); workingBuffer.flip(); ScheduleSetSequence record = new ScheduleSetSequence(scheduleTime, sequence); return new AppendRecordResult&lt;&gt;(AppendMessageStatus.SUCCESS, 0, recordSize, workingBuffer, record); &#125; 这里也能看到schedule_log的消息格式。 发现就写了个存储篇幅就已经挺大了，投递涉及到的内容可能更多，那么关于投递就开个下一篇吧。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QMQ源码分析之delay-server篇【一】]]></title>
    <url>%2F3159cb59.html</url>
    <content type="text"><![CDATA[前言 QMQ是一款去哪儿网内部使用多年的mq。不久前(大概1-2年前)已在携程投入生产大规模使用，年前这款mq也开源了出来。关于QMQ的相关设计文章可以看这里。在这里，我假设你已经对QMQ前世今生以及其设计和实现等背景知识已经有了一个较为全面的认识。 我的阅读姿势对一款开源产品愈来愈感兴趣，想要了解一款开源产品更多的技术细节的时候，最好的方式自然是去阅读她的源码。那么一个正确阅读开源软件源码的姿势是什么呢？我觉得这完全就像一个相亲过程： 媒婆介绍相亲对象基本信息。这一定是前提。很多人都忽视了这一个步骤。在这个步骤中，要去了解这款开源软件是用来做什么的？解决了什么问题？如何解决这些问题的？所处地位？其实就是what,why,how,where四个问题。要是在阅读源码前能准备下这四个问题的答案，那么接下来阅读源码的工作将更有效果。 见面，喝茶，对媒婆所言一探究竟。这个时候我们要去认识下软件的整体结构，例如，包结构，依赖轻重，主要功能是哪些在哪里等。此外还要去验证下”媒婆所言”是否属实，我们要自己操作运行一下，对这个”姑娘”有一个基础认识。 约会。有以上基础认识之后，就要深入源码一探究竟。针对各功能点(主要是第一个步骤中谈到的解决的什么问题即why)各条线深入下去，最后贯穿起来，形成一个闭环。 自由发挥。这个时候就看缘分了，对上眼就成了contributor，对不上眼也能多个朋友多条路不是。 主要功能对于delay-server，官方已经有了一些介绍。记住，官方通常是最卖力的那个”媒婆”。qmq-delay-server其实主要做的是转发工作。所谓转发，就是delay-server做的就是个存储和投递的工作。怎么理解，就是qmq-client会对消息进行一个路由，即实时消息投递到实时server，延迟消息往delay-server投递，多说一句，这个路由的功能是由qmq-meta-server提供。投递到delay-server的消息会存下来，到时间之后再进行投递。现在我们知道了存储和投递是delay-server主要的两个功能点。那么我们挨个击破: 存储假如让我们来设计实现一个delay-server，存储部分我们需要解决什么问题？我觉得主要是要解决到期投递的到期问题。我们可以用传统db做，但是这个性能肯定是上不去的。我们也可以用基于LSM树的RocksDB。或者，干脆直接用文件存储。QMQ是用文件存储的。而用文件存储是怎么解决到期问题的呢？delay-server接收到延迟消息，就将消息append到message_log中，然后再通过回放这个message_log得到schedule_log，此外还有一个dispatch _log用于记录投递记录。QMQ还有个跟投递相关的存储设计，即两层HashWheel。第一层位于磁盘上，例如，以一个小时一个刻度一个文件，我们叫delay_message_segment，如延迟时间为2019年02月23日 19:00 至2019年02月23日 20:00为延迟消息将被存储在2019022319。并且这个刻度是可以配置调整的。第二层HashWheel位于内存中。也是以一个时间为刻度，比如500ms，加载进内存中的延迟消息文件会根据延迟时间hash到一个HashWheel中，第二层的wheel涉及更多的是下一小节的投递。貌似存储到这里就结束了，然而还有一个问题，目前当投递的时候我们需要将一个delay_message_segment加载进内存中，而假如我们提前一个刻度加载进一个delay_message_segment到内存中的hashwheel，比如在2019年02月23日 18:00加载2019022319这个segment文件，那么一个hashwheel中就会存在两个delay_message_segment，而这个时候所占内存是非常大的，所以这是完全不可接收的。所以，QMQ引入了一个数据结构，叫schedule_index，即消息索引，存储的内容为消息的索引，我们加载到内存的是这个schedule_index，在真正投递的时候再根据索引查到消息体进行投递。 投递解决了存储，那么到期的延迟消息如何投递呢？如在上一小节存储中所提到的，内存中的hashwheel会提前一段时间加载delay_schedule_index，这个时间自然也是可以配置的。而在hashwheel中，默认每500ms会tick一次，这个500ms也是可以根据用户需求配置的。而在投递的时候，QMQ根据实时broker进行分组多线程投递，如果某一broker离线不可用，导致投递失败，delay-server会将延迟消息投递在其他存活的实时broker。其实这里对于实时的broker应该有一个关于投递消息权重的，现在delay-server没有考虑到这一点，不过我看已经有一个pr解决了这个问题，只是官方还没有时间看这个问题。除此之外，QMQ还考虑到了要是当前延迟消息所属的delay_segment已经加载到内存中的hashwheel了，这个时候消息应该是直接投递或也应加载到hashwheel中的。这里需要考虑的情况还是比较多的，比如考虑delay_segment正在加载、已经加载、加载完成等情况，对于这种情况，QMQ用了两个cursor来表示hashwheel加载到哪个delay_segment以及加载到对应segment的什么offset了，这里还是挺复杂的，这里的代码逻辑在WheelTickManager这个类。 我们先来看一看整体结构以功能划分的包结构，算是比较清晰。cleaner是日志清理工作相关，receiver是接收消息相关，sender是投递相关，store是存储相关，sync是同步备份相关，wheel则是hashwheel相关。 关于QMQ源码阅读前的准备工作就先做到这里，下一篇我们就深入源码分析以上提到的各个细节。]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何科学上网？]]></title>
    <url>%2F8c46c59b.html</url>
    <content type="text"><![CDATA[为了给我们提供一个安全的网络环境，所以先驱前辈们建立了一堵墙。总有些调皮 好奇的孩子想要翻过墙去看看墙那边的世界。但是存在风险，需谨慎。 共有下面两种方式供选择： 利用VPN 免费免费的vpn有很多，但是速度、稳定性和流量限制是基本不能满足需要的，所以就不推荐了。 收费收费的vpn一般都在每月10元左右，并且足够稳定。另外，建议大家选择国外的vpn，国内的vpn产商说不定哪天就跑路什么的。在这里，推荐ExpressVPN和PureVPN。前者比较知名，也比较稳健，价格大概在每月$7+；后者也相对比较好用，每月大概在$3+，说是有中国用户的专线。详情可参考。 自建代理喜欢掌握主动权的我，倾向于采用自建代理的方案。综合来看自建代理都是最实惠，最可控的方案。 购买VPS目前VPS产商有两家做的最大，分别是BandwagonHost(搬瓦工)和Vultr。有篇文章对比了这两家厂商的产品。购买VPS都是有优惠的，搬瓦工 Vultr。因为搬瓦工比较老牌，老而弥坚，所以我选择的是它。如果你不喜欢老而弥坚的东西，选择了Vultr，那么请移步看搭建SSR。购买时注意是不是支持中国专线，如果没注意，那么购买成功之后也是可以更改线路的。购买完成，你会受到一封邮件，里边有ip port password等信息，连接上vps，安装完一些基础工具(wget等)，就可以开干了。 搭建代理现在用的最多的就是shadowsocks，以及其衍生版本shadowsocks r。我选择的是shadowsockr。这里有个ssr工具网站，客户端，一键安装脚本在这里都能找到。 ssh连接上vps 依次运行下边三条命令： 1wget --no-check-certificate -O shadowsocks-all.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh 1chmod +x shadowsocks-all.sh 1./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log 接下来按照提示，选择参数安装即可，步骤大概为： 选择版本，建议ShadowsocksR 设置SSR密码 选择SSR服务器端口 选择加密方式如这里选择chacha20，输入对应序号12即可。 选择协议如这里选择auth_aes128_md5，输入对应序号5即可 选择混淆方式如这里选择http_simple，输入对应序号2即可 参数设置完成，任意键开始安装，静静等待。 安装完成，你会看到以上信息，记录下来，待会儿会用到。你也可以在下图文件夹下的config.json看到。客户端安装windows 下载mac下载andriod下载 或者在应用商城看一下有没有shadowsocks-r(或者ssr)客户端下载ios 免费的App可以用Potatso Lite，不过应该需要申请一下美国AppleID安装完毕，输入安装完毕让你记下的那些信息(在/etc/shadowsocks-r/config.json，或者在刚才安装的目录下找到shadowsocks-all.log文件里也有相关信息)。另外，ssr客户端支持二维码扫描，剪贴板导入等方式，很方便，如下图：安卓上效果如下：好了，安卓和ios设备现在基本都能上网了。但是pc端还需要一个东西，即chrome的一个插件，swithy omega。下载插件添加到chrome完毕，配置如下图： 其中的规则列表网址：https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 详情可以参考gfwlist 就是这些，你可以科学上网了。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>科学上网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年终总结]]></title>
    <url>%2F59ca7e41.html</url>
    <content type="text"><![CDATA[__又到年底，然而今年并不打算再写年终总结。__]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
</search>
